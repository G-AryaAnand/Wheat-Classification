{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b605089",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cc1a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras as keras\n",
    "from keras import layers as layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c004aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, timeit\n",
    "from skimage.filters import threshold_otsu\n",
    "import numpy as np\n",
    "from math import inf as inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10614a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9300cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spectral.io import envi as envi\n",
    "from spectral import imshow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee790ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3717a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3c2830",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import set_random_seed\n",
    "set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd26930",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f72da7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import platform\n",
    "DATA_DIRECTORY = \"\"\n",
    "SLASH = \"\"\n",
    "if platform == \"linux\" or platform == \"linux2\":\n",
    "    DATA_DIRECTORY = \"/home/tyagi/Desktop/wheat/data/BULK/\"\n",
    "    SLASH = \"/\"\n",
    "elif platform == \"win32\":\n",
    "    DATA_DIRECTORY = \"D:\\mvl\\wheat\\data\\BULK\\\\\"\n",
    "    SLASH=\"\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5979451f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constants\n",
    "BAND_NUMBER = 60\n",
    "FILLED_AREA_RATIO = 0.9\n",
    "TOTAL_IMAGE_COUNT = 8\n",
    "IMAGE_COUNT = int(TOTAL_IMAGE_COUNT/4)\n",
    "NUM_VARIETIES = 4\n",
    "\n",
    "IMAGE_WIDTH = 30\n",
    "IMAGE_HEIGHT = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066f0249",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIVATION_TYPE =  \"relu\"\n",
    "BATCH_SIZE = 2*NUM_VARIETIES\n",
    "LEARNING_RATE_BASE = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f32e132",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class filter_method(Enum):\n",
    "    none = 0\n",
    "    snv = 1\n",
    "    msc = 2\n",
    "    savgol = 3\n",
    "    \n",
    "FILTER = filter_method(0).name\n",
    "\n",
    "# to be set if filter chosen is savgol\n",
    "WINDOW = 7\n",
    "ORDER = 2\n",
    "DERIVATIVE = \"none\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3a1d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    " \n",
    "class feature_extraction_method(Enum):\n",
    "    none = 0\n",
    "    pca_loading = 1\n",
    "    lda = 2\n",
    "    ipca = 3\n",
    "\n",
    "FEATURE_EXTRACTION = feature_extraction_method(0).name\n",
    "\n",
    "NUM_OF_BANDS = 3\n",
    "if FEATURE_EXTRACTION == \"pca_loading\" or FEATURE_EXTRACTION == \"ipca\":\n",
    "    NUM_OF_BANDS = 8\n",
    "elif FEATURE_EXTRACTION == \"lda\":\n",
    "    NUM_OF_BANDS = 3\n",
    "    assert NUM_OF_BANDS <= min(NUM_VARIETIES-1,168),\"NUM_OF_BANDS is greater.\"\n",
    "\n",
    "\n",
    "REMOVE_NOISY_BANDS = False\n",
    "FIRST_BAND = 15\n",
    "LAST_BAND = 161"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61072a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_timer():\n",
    "    print(\"Testing started\")\n",
    "    return timeit.default_timer()\n",
    "\n",
    "def end_timer():\n",
    "    return timeit.default_timer()\n",
    "\n",
    "def show_time(tic,toc): \n",
    "    test_time = toc - tic\n",
    "    print('Testing time (s) = ' + str(test_time) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16e403c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List for All varieties\n",
    "VARIETIES = []\n",
    "VARIETIES_CODE = {}\n",
    "\n",
    "for name in os.listdir(DATA_DIRECTORY):\n",
    "    if (name.endswith(\".hdr\") or name.endswith(\".bil\")):\n",
    "        continue\n",
    "    VARIETIES_CODE[name] = len(VARIETIES)\n",
    "    VARIETIES.append(name)\n",
    "    if len(VARIETIES)==NUM_VARIETIES:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72409e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_file_name(variety):\n",
    "    name = \"./dataset/V\"+str(variety).zfill(3)+\"_IC_\"+str(TOTAL_IMAGE_COUNT).zfill(5)+\"_FilledArea_\"+str(FILLED_AREA_RATIO)+\"_NumOfBands_\"+str(NUM_OF_BANDS)+\"_FB_\"+str(FIRST_BAND)+\"_LB_\"+str(LAST_BAND)+\"_BandNo_\"+str(BAND_NUMBER)+\"_ImageHeight_\"+str(IMAGE_HEIGHT)+\"_ImageWidth_\"+str(IMAGE_WIDTH)+\"_FILTER_\"+str(FILTER)+\"_FeatureExtraction_\"+str(FEATURE_EXTRACTION)\n",
    "    if REMOVE_NOISY_BANDS:\n",
    "        name+=\"_REMOVE_NOISY_BANDS_\"+str(REMOVE_NOISY_BANDS)\n",
    "    if FILTER == \"savgol\":\n",
    "        name+=\"_WINDOW_\"+str(WINDOW)+\"_ORDER_\"+str(ORDER)\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74afa6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = []\n",
    "train_dataset_label = []\n",
    "test_dataset=[]\n",
    "test_dataset_label = []\n",
    "\n",
    "for idx, v in enumerate(VARIETIES):\n",
    "    print(\"idx: \",idx)\n",
    "    if idx >= NUM_VARIETIES:\n",
    "        break\n",
    "    train_dataset= train_dataset + np.load(dataset_file_name(v)+\"_train_dataset.npy\").tolist()\n",
    "    train_dataset_label = train_dataset_label + np.load(dataset_file_name(v)+\"_train_dataset_label.npy\").tolist()\n",
    "    test_dataset = test_dataset + np.load(dataset_file_name(v)+\"_test_dataset.npy\").tolist()\n",
    "    test_dataset_label = test_dataset_label + np.load(dataset_file_name(v)+\"_test_dataset_label.npy\").tolist()\n",
    "    \n",
    "train_dataset = np.array(train_dataset)\n",
    "train_dataset_label = np.array(train_dataset_label)\n",
    "test_dataset = np.array(test_dataset)\n",
    "test_dataset_label = np.array(test_dataset_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6c0621",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import keras\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Activation, BatchNormalization, Add, Conv2DTranspose, Flatten, Dense, Conv1D, AveragePooling2D, LeakyReLU, PReLU, GlobalAveragePooling2D\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers import concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "import os, pdb, timeit\n",
    "import numpy as np\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f98cd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeDataWholeSeed(data,normalization_type='max'):\n",
    "    \n",
    "    if normalization_type == 'max':\n",
    "        for idx in range(data.shape[0]):\n",
    "            data[idx,:,:,:] = data[idx,:,:,:]/np.max(abs(data[idx,:,:,:]))\n",
    "            \n",
    "    elif normalization_type == 'l2norm':\n",
    "        from numpy import linalg as LA\n",
    "        for idx in range(data.shape[0]):\n",
    "            data[idx,:,:,:] = data[idx,:,:,:]/LA.norm(data[idx,:,:,:]) # L2-norm by default        \n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce22266",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hyperparam_string(USE_DATA_AUG, learning_rate_base, batch_size, kernel_size, dropout_rate, num_training,\n",
    "                           num_nodes_fc, activation_type):\n",
    "    hparam = \"\"\n",
    "\n",
    "    # Hyper-parameters\n",
    "    if USE_DATA_AUG:\n",
    "        hparam += \"AUG_\"\n",
    "\n",
    "    hparam += str(num_nodes_fc) + \"nodes_\" + str(learning_rate_base) + \"lr_\" + str(batch_size) + \"batch_\" + str(\n",
    "        kernel_size) + \"kernel_\" + str(dropout_rate) + \"drop_\" + str(\n",
    "        num_training) + \"train_\" + activation_type\n",
    "\n",
    "    return hparam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd4a22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "\n",
    "    import itertools\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.clim(0,sum(cm[0,:]))\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67d07a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_K_classification_accuracy(y_predicted, y_true, K=1):\n",
    "\n",
    "    num_samples = y_predicted.shape[0]\n",
    "    num_classes = y_predicted.shape[1]\n",
    "\n",
    "    if K > num_classes:\n",
    "        sys.exit(1)\n",
    "\n",
    "    temp = np.zeros((num_samples,))\n",
    "\n",
    "    for idx in range(num_samples):\n",
    "        curr_predicted = np.argsort(y_predicted[idx,:])\n",
    "        curr_predicted = curr_predicted[::-1] # descending\n",
    "\n",
    "        if y_true[idx] in curr_predicted[:K]:\n",
    "            temp[idx] = 1\n",
    "\n",
    "    return 100.0 * np.sum(temp)/num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351725d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2D_ResNet(x, kernel_size, activation_type, dropout_rate, num_filters_first_conv1D):\n",
    "\n",
    "    x_orig = x\n",
    "\n",
    "    # Batch norm\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # 1x1 Conv2D\n",
    "    x = Conv2D(num_filters_first_conv1D, kernel_size=1, activation=None, use_bias=False, padding='same',\n",
    "               kernel_initializer='truncated_normal')(x)\n",
    "\n",
    "    # Activation\n",
    "    if activation_type == 'LeakyReLU':\n",
    "        x = LeakyReLU()(x)\n",
    "    elif activation_type == 'PReLU':\n",
    "        x = PReLU()(x)\n",
    "    else:\n",
    "        x = Activation(activation_type)(x)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # 3x3 Conv2D\n",
    "    x = Conv2D(num_filters_first_conv1D, kernel_size, activation=None, use_bias=True, padding='same',\n",
    "               kernel_initializer='truncated_normal')(x)\n",
    "\n",
    "    # Activation\n",
    "    if activation_type == 'LeakyReLU':\n",
    "        x = LeakyReLU()(x)\n",
    "    elif activation_type == 'PReLU':\n",
    "        x = PReLU()(x)\n",
    "    else:\n",
    "        x = Activation(activation_type)(x)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # 1x1 Conv2D\n",
    "    x = Conv2D(num_filters_first_conv1D*4, kernel_size=1, activation=None, use_bias=False, padding='same',\n",
    "               kernel_initializer='truncated_normal')(x)\n",
    "\n",
    "    # Skip connection\n",
    "    if int(x.shape[3]) != int(x_orig.shape[3]):\n",
    "        x_orig = Conv2D(int(x.shape[3]), kernel_size=1, activation=None, use_bias=False, padding='same',\n",
    "               kernel_initializer='truncated_normal')(x_orig)\n",
    "\n",
    "    # Activation\n",
    "    if activation_type == 'LeakyReLU':\n",
    "        x = LeakyReLU()(x)\n",
    "    elif activation_type == 'PReLU':\n",
    "        x = PReLU()(x)\n",
    "    else:\n",
    "        x = Activation(activation_type)(x)\n",
    "\n",
    "    x = Add()([x, x_orig])\n",
    "\n",
    "    # Dropout\n",
    "    return Dropout(dropout_rate)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0731a5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createBlock_ResNet2D(x, num_layers, kernel_size, activation_type, dropout_rate, num_filters_first_conv1D):\n",
    "\n",
    "    for idx_layer in range(num_layers):\n",
    "\n",
    "        x = conv2D_ResNet(x, kernel_size, activation_type, dropout_rate, num_filters_first_conv1D)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bd16db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# growth_rate: number of filters for each normal convolution ('k' in the paper)\n",
    "def ResNet2D_classifier(data_num_rows, data_num_cols, num_classes, kernel_size=3, num_layers_each_block=[6, 12, 24, 16],\n",
    "                        num_chan_per_block = [64,128,256,512], activation_type='swish', dropout_rate=0.0, num_input_chans=1, num_nodes_fc=64):\n",
    "\n",
    "    input_data = Input(shape=(data_num_rows, data_num_cols, num_input_chans))\n",
    "\n",
    "    # Input layer: Conv2D -> activation\n",
    "    x = Conv2D(num_chan_per_block[0], kernel_size, activation=None, use_bias=True, padding='same',\n",
    "               kernel_initializer='truncated_normal')(input_data)\n",
    "\n",
    "    # Activation\n",
    "    if activation_type == 'LeakyReLU':\n",
    "        x = LeakyReLU()(x)\n",
    "    elif activation_type == 'PReLU':\n",
    "        x = PReLU()(x)\n",
    "    else:\n",
    "        x = Activation(activation_type)(x)\n",
    "\n",
    "    #  Blocks & Downsampling Layers\n",
    "    for idx_block in range(len(num_layers_each_block)):\n",
    "        x = createBlock_ResNet2D(x, num_layers_each_block[idx_block], kernel_size, activation_type, dropout_rate,\n",
    "                                 num_chan_per_block[idx_block])\n",
    "\n",
    "        x = BatchNormalization()(x)\n",
    "\n",
    "        if idx_block != len(num_layers_each_block)-1:\n",
    "            x = Conv2D(num_chan_per_block[idx_block]*2, kernel_size, strides = 2, activation=None, use_bias=True, padding='valid',\n",
    "                   kernel_initializer='truncated_normal')(x)\n",
    "        else:\n",
    "            x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    # Output layer\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(units=num_nodes_fc, activation=None, kernel_initializer='truncated_normal')(x)\n",
    "\n",
    "    # Activation\n",
    "    if activation_type == 'LeakyReLU':\n",
    "        x = LeakyReLU()(x)\n",
    "    elif activation_type == 'PReLU':\n",
    "        x = PReLU()(x)\n",
    "    else:\n",
    "        x = Activation(activation_type)(x)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    output_data = Dense(units=num_classes, activation='softmax', kernel_initializer='truncated_normal')(x)\n",
    "\n",
    "    return Model(inputs=input_data, outputs=output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebae4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a3c3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model,dataset,dataset_label,normalization_type):\n",
    "    print(\"--------------Make Predictions--------------\")    \n",
    "    x = np.array(dataset)\n",
    "    labels = np.array(dataset_label)\n",
    "    \n",
    "    # Normalize the data\n",
    "    x = normalizeDataWholeSeed(x,normalization_type=normalization_type)\n",
    "    \n",
    "    num = x.shape[0]\n",
    "\n",
    "    print(\"Testing started\")\n",
    "    tic = timeit.default_timer()\n",
    "    labels_predicted = model.predict(x)\n",
    "    toc = timeit.default_timer()\n",
    "    test_time = toc - tic\n",
    "    print('Testing time (s) = ' + str(test_time) + '\\n')\n",
    "    \n",
    "    print(\"--------\")\n",
    "    # Classification accuracy\n",
    "    labels_integer_format = labels\n",
    "    labels_predicted_integer_format = np.argmax(labels_predicted, axis=1)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(labels_integer_format, labels_predicted_integer_format)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    \n",
    "    # Confusion matrices\n",
    "    confusion_matrix_results = confusion_matrix(labels_integer_format, labels_predicted_integer_format)\n",
    "    print(\"Confusion matrix = \")\n",
    "    print(confusion_matrix_results)\n",
    "    print(\"------------------------------------------------\")\n",
    "    \n",
    "    # Calculate precision, recall, and F1-score for each class\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(labels_integer_format, labels_predicted_integer_format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4c541b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model,normalization_type):\n",
    "    evaluate(model,train_dataset,train_dataset_label,normalization_type)\n",
    "    \n",
    "    evaluate(model,test_dataset,test_dataset_label,normalization_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6797662b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createAndTrainResNetB(params):\n",
    "                                        \n",
    "    ############ Extract params ############\n",
    "    USE_DATA_AUG = params['USE_DATA_AUG']\n",
    "    learning_rate_base = params['learning_rate_base']\n",
    "    kernel_size = params['kernel_size']\n",
    "    batch_size = params['batch_size']\n",
    "    dropout_rate = params['dropout_rate']\n",
    "    activation_type = params['activation_type']\n",
    "    num_nodes_fc = params['num_nodes_fc']\n",
    "    wheat_types = params['wheat_types']\n",
    "    normalization_type = params['normalization_type']\n",
    "    num_layers_each_block = params['num_layers_each_block']\n",
    "    num_chan_per_block = params['num_chan_per_block']\n",
    "    N_classes = len(wheat_types)\n",
    "    \n",
    "    \n",
    "    ############ Load data ############\n",
    "    print(\"--------------Load Data--------------\")\n",
    "\n",
    "    # Load training data and their corresponding labels\n",
    "    x_training = np.array(train_dataset)\n",
    "    labels_training = np.array(train_dataset_label)\n",
    "    \n",
    "    # Normalize the data\n",
    "    x_training = normalizeDataWholeSeed(x_training,normalization_type=normalization_type)\n",
    "    \n",
    "    # Extract some information\n",
    "    num_training = x_training.shape[0]\n",
    "    N_spatial = x_training.shape[1:3]\n",
    "    N_bands = x_training.shape[3]\n",
    "    num_batch_per_epoch = int(num_training/batch_size)\n",
    "    \n",
    "    print('#training = %d' %(num_training))\n",
    "    print('#batches per epoch = %d' %(num_batch_per_epoch))\n",
    "    \n",
    "    print(\"--------------Done--------------\")\n",
    "    \n",
    "    \n",
    "    ############ Prepare the path for saving the models/stats ############\n",
    "    print(\"--------------Prepare a path for saving the models/stats--------------\")\n",
    "    \n",
    "    hparams = make_hyperparam_string(USE_DATA_AUG, learning_rate_base, batch_size, kernel_size, dropout_rate,\n",
    "                                     num_training, num_nodes_fc, activation_type)\n",
    "    print('Saving the model to...')\n",
    "    \n",
    "    results_dir = os.path.join(params['results_base_directory'],hparams)\n",
    "    \n",
    "    if not os.path.exists(results_dir):\n",
    "        os.makedirs(results_dir)\n",
    "    print(results_dir)\n",
    "\n",
    "    print(\"--------------Done--------------\")\n",
    "\n",
    "    ############ Create a model ############\n",
    "    print(\"--------------Create a model--------------\")\n",
    "    \n",
    "    # Generate a model\n",
    "    model = ResNet2D_classifier(data_num_rows=N_spatial[0], data_num_cols=N_spatial[1], num_classes=N_classes,\n",
    "                                kernel_size=kernel_size, num_layers_each_block=num_layers_each_block,\n",
    "                                num_chan_per_block=num_chan_per_block, activation_type=activation_type,\n",
    "                                dropout_rate=dropout_rate, num_input_chans=N_bands, num_nodes_fc=num_nodes_fc)\n",
    "\n",
    "    # Compile the model\n",
    "    adam_opt = Adam(learning_rate=learning_rate_base, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.01)\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=adam_opt, metrics=['acc'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8da36f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Parameters (mostly determined using validation datasets)\n",
    "params = dict()\n",
    "params['normalization_type'] = 'max'                    # Data normalization type\n",
    "params['wheat_types'] = VARIETIES                       \n",
    "params['kernel_size'] = 3                               # Kernel size\n",
    "params['dropout_rate'] = 0.0                            # Dropout rate\n",
    "params['num_nodes_fc'] = 512                            # Number of  nodes in the fully-connected layers\n",
    "params['num_layers_each_block'] = [8, 8, 12, 8]         # Number of layers per block\n",
    "params['num_chan_per_block'] = [128, 128, 256, 256]     # Number of filters in the conv layers\n",
    "params['results_base_directory'] = './results/'  # Directory of saving results\n",
    "params['activation_type'] = ACTIVATION_TYPE\n",
    "params['batch_size'] = BATCH_SIZE                    # Batch size\n",
    "params['USE_DATA_AUG'] = False              # Use data augmentation (In the paper, we set it to True)\n",
    "params['learning_rate_base'] = LEARNING_RATE_BASE      # Initial learning rate (In the paper, we set it to 0.005)\n",
    "\n",
    "# Add 'swish' activation\n",
    "if params['activation_type'] == 'swish':\n",
    "\n",
    "    from tensorflow.keras.utils import get_custom_objects\n",
    "    import keras.backend as K\n",
    "\n",
    "    # Taken from https://github.com/dataplayer12/swish-activation/blob/master/MNIST/activations.ipynb\n",
    "    def swish(x):\n",
    "        beta = tf.Variable(initial_value=1.0,trainable=True)\n",
    "        return x*tf.nn.sigmoid(beta*x)\n",
    "\n",
    "    get_custom_objects().update({'swish': swish})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ac47e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = createAndTrainResNetB(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74535028",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897dba37",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dataframe = pd.DataFrame() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43cde60",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_training = np.array(train_dataset)\n",
    "labels_training = np.array(train_dataset_label)\n",
    "\n",
    "# Normalize the data\n",
    "x_training = normalizeDataWholeSeed(x_training,normalization_type='max')\n",
    "    \n",
    "# Extract some information\n",
    "num_training = x_training.shape[0]\n",
    "N_spatial = x_training.shape[1:3]\n",
    "N_bands = x_training.shape[3]\n",
    "batch_size = BATCH_SIZE\n",
    "num_batch_per_epoch = int(num_training/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3324c7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Train the model ############\n",
    "print(\"--------------Begin training the model--------------\")\n",
    "\n",
    "tic = timeit.default_timer()\n",
    "\n",
    "# Train the model\n",
    "for x in range(0,20):\n",
    "    print(\"\\nEpoch: \",x+1)\n",
    "    history = model.fit(x_training, labels_training, batch_size=batch_size, steps_per_epoch=num_batch_per_epoch, epochs = 1, validation_split=0.2, verbose=2)\n",
    "    if history_dataframe.size == 0:\n",
    "        history_dataframe = pd.DataFrame.from_dict(history.history)\n",
    "    else:\n",
    "        history_dataframe = pd.concat([history_dataframe,pd.DataFrame.from_dict(history.history)],axis=0,ignore_index=True)\n",
    "\n",
    "toc = timeit.default_timer()\n",
    "training_time = toc-tic\n",
    "print(\"Total training time = \" + str(training_time))\n",
    "\n",
    "print(\"--------------Done--------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d4ef3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(model,params[\"normalization_type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb608fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(df,title,xlabel,ylabel,values=['loss'],legends=[]):\n",
    "    \n",
    "    for value in values:\n",
    "        epoch_count = range(1, len(df.index) + 1)\n",
    "        plt.plot(epoch_count, df[value].tolist())\n",
    "    plt.title(title)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xlabel(xlabel)\n",
    "    if legends==[]:\n",
    "        legends = values\n",
    "    plt.legend(legends, loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca1b8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2320b839",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plot_graph(history_dataframe,'Model Train Accuracy','Epoch','Train Accuracy',['acc'])\n",
    "plot_graph(history_dataframe,'Model Train Loss','Epoch','Train Loss',['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff40a2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graph(history_dataframe,'Model Val Accuracy','Epoch','Validation Accuracy',['val_acc'])\n",
    "plot_graph(history_dataframe,'Model Val Loss','Epoch','Validation Loss',['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90169b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Train the model ############\n",
    "print(\"--------------Begin training the model--------------\")\n",
    "\n",
    "tic = timeit.default_timer()\n",
    "\n",
    "# Train the model\n",
    "for x in range(20,30):\n",
    "    print(\"\\nEpoch: \",x+1)\n",
    "    history = model.fit(x_training, labels_training, batch_size=batch_size, steps_per_epoch=num_batch_per_epoch, epochs = 1, validation_split=0.2, verbose=2)\n",
    "    if history_dataframe.size == 0:\n",
    "        history_dataframe = pd.DataFrame.from_dict(history.history)\n",
    "    else:\n",
    "        history_dataframe = pd.concat([history_dataframe,pd.DataFrame.from_dict(history.history)],axis=0,ignore_index=True)\n",
    "\n",
    "toc = timeit.default_timer()\n",
    "training_time = toc-tic\n",
    "print(\"Total training time = \" + str(training_time))\n",
    "\n",
    "print(\"--------------Done--------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00dd0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(model,params[\"normalization_type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc040719",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(df,title,xlabel,ylabel,values=['loss'],legends=[]):\n",
    "    \n",
    "    for value in values:\n",
    "        epoch_count = range(1, len(df.index) + 1)\n",
    "        plt.plot(epoch_count, df[value].tolist())\n",
    "    plt.title(title)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xlabel(xlabel)\n",
    "    if legends==[]:\n",
    "        legends = values\n",
    "    plt.legend(legends, loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e0ec15",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b010ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plot_graph(history_dataframe,'Model Train Accuracy','Epoch','Train Accuracy',['acc'])\n",
    "plot_graph(history_dataframe,'Model Train Loss','Epoch','Train Loss',['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319b461d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graph(history_dataframe,'Model Val Accuracy','Epoch','Validation Accuracy',['val_acc'])\n",
    "plot_graph(history_dataframe,'Model Val Loss','Epoch','Validation Loss',['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7913337f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
