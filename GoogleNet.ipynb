{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffd5f299",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-09 12:33:07.406851: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-09 12:33:08.312943: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/nitin_temp/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a93a32cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.12.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wrapt\n",
    "wrapt.__version__   # should be 1.14.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bb57a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras as keras\n",
    "from keras import layers as layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32710dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, timeit\n",
    "from skimage.filters import threshold_otsu\n",
    "import numpy as np\n",
    "from math import inf as inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03618476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccaf9f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spectral.io import envi as envi\n",
    "from spectral import imshow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "998e218d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edeecd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ec15112",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import set_random_seed\n",
    "set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46c1a1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd6a6148",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import platform\n",
    "DATA_DIRECTORY = \"\"\n",
    "SLASH = \"\"\n",
    "if platform == \"linux\" or platform == \"linux2\":\n",
    "    DATA_DIRECTORY = \"/home/nitin_temp/Desktop/BTP/data/BULK/\"\n",
    "    SLASH = \"/\"\n",
    "elif platform == \"win32\":\n",
    "    DATA_DIRECTORY = \"D:\\mvl\\wheat\\data\\BULK\\\\\"\n",
    "    SLASH=\"\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e706d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constants\n",
    "BAND_NUMBER = 60\n",
    "FILLED_AREA_RATIO = 0.9\n",
    "TOTAL_IMAGE_COUNT = 1500\n",
    "IMAGE_COUNT = int(TOTAL_IMAGE_COUNT/4)\n",
    "NUM_VARIETIES = 8\n",
    "\n",
    "IMAGE_WIDTH = 30\n",
    "IMAGE_HEIGHT = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eeee82ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIVATION_TYPE =  \"relu\"\n",
    "BATCH_SIZE = 2*NUM_VARIETIES\n",
    "\n",
    "LEARNING_RATE_BASE = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7939447",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class filter_method(Enum):\n",
    "    none = 0\n",
    "    snv = 1\n",
    "    msc = 2\n",
    "    savgol = 3\n",
    "    \n",
    "FILTER = filter_method(1).name\n",
    "\n",
    "# to be set if filter chosen is savgol\n",
    "WINDOW = 7\n",
    "ORDER = 2\n",
    "DERIVATIVE = \"none\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "447c6a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    " \n",
    "class feature_extraction_method(Enum):\n",
    "    none = 0\n",
    "    pca_loading = 1\n",
    "    lda = 2\n",
    "    ipca = 3\n",
    "\n",
    "FEATURE_EXTRACTION = feature_extraction_method(0).name\n",
    "\n",
    "NUM_OF_BANDS = 3\n",
    "if FEATURE_EXTRACTION == \"pca_loading\" or FEATURE_EXTRACTION == \"ipca\":\n",
    "    NUM_OF_BANDS = 8\n",
    "elif FEATURE_EXTRACTION == \"lda\":\n",
    "    NUM_OF_BANDS = 3\n",
    "    assert NUM_OF_BANDS <= min(NUM_VARIETIES-1,168),\"NUM_OF_BANDS is greater.\"\n",
    "\n",
    "\n",
    "REMOVE_NOISY_BANDS = False\n",
    "FIRST_BAND = 15\n",
    "LAST_BAND = 161"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebc894e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_timer():\n",
    "    print(\"Testing started\")\n",
    "    return timeit.default_timer()\n",
    "\n",
    "def end_timer():\n",
    "    return timeit.default_timer()\n",
    "\n",
    "def show_time(tic,toc): \n",
    "    test_time = toc - tic\n",
    "    print('Testing time (s) = ' + str(test_time) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6748430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List for All varieties\n",
    "VARIETIES = []\n",
    "VARIETIES_CODE = {}\n",
    "\n",
    "for name in os.listdir(DATA_DIRECTORY):\n",
    "    if (name.endswith(\".hdr\") or name.endswith(\".bil\")):\n",
    "        continue\n",
    "    VARIETIES_CODE[name] = len(VARIETIES)\n",
    "    VARIETIES.append(name)\n",
    "    if len(VARIETIES)==NUM_VARIETIES:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "687c094c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_file_name(variety):\n",
    "    name = \"./dataset/V\"+str(variety).zfill(3)+\"_IC_\"+str(TOTAL_IMAGE_COUNT).zfill(5)+\"_FilledArea_\"+str(FILLED_AREA_RATIO)+\"_NumOfBands_\"+str(NUM_OF_BANDS)+\"_FB_\"+str(FIRST_BAND)+\"_LB_\"+str(LAST_BAND)+\"_BandNo_\"+str(BAND_NUMBER)+\"_ImageHeight_\"+str(IMAGE_HEIGHT)+\"_ImageWidth_\"+str(IMAGE_WIDTH)+\"_FILTER_\"+str(FILTER)+\"_FeatureExtraction_\"+str(FEATURE_EXTRACTION)\n",
    "    if REMOVE_NOISY_BANDS:\n",
    "        name+=\"_REMOVE_NOISY_BANDS_\"+str(REMOVE_NOISY_BANDS)\n",
    "    if FILTER == \"savgol\":\n",
    "        name+=\"_WINDOW_\"+str(WINDOW)+\"_ORDER_\"+str(ORDER)\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9eabae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx:  0\n",
      "idx:  1\n",
      "idx:  2\n",
      "idx:  3\n",
      "idx:  4\n",
      "idx:  5\n",
      "idx:  6\n",
      "idx:  7\n"
     ]
    }
   ],
   "source": [
    "train_dataset = []\n",
    "train_dataset_label = []\n",
    "test_dataset=[]\n",
    "test_dataset_label = []\n",
    "\n",
    "for idx, v in enumerate(VARIETIES):\n",
    "    print(\"idx: \",idx)\n",
    "    if idx >= NUM_VARIETIES:\n",
    "        break\n",
    "    train_dataset= train_dataset + np.load(dataset_file_name(v)+\"_train_dataset.npy\").tolist()\n",
    "    train_dataset_label = train_dataset_label + np.load(dataset_file_name(v)+\"_train_dataset_label.npy\").tolist()\n",
    "    test_dataset = test_dataset + np.load(dataset_file_name(v)+\"_test_dataset.npy\").tolist()\n",
    "    test_dataset_label = test_dataset_label + np.load(dataset_file_name(v)+\"_test_dataset_label.npy\").tolist()\n",
    "    \n",
    "train_dataset = np.array(train_dataset)\n",
    "train_dataset_label = np.array(train_dataset_label)\n",
    "test_dataset = np.array(test_dataset)\n",
    "test_dataset_label = np.array(test_dataset_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cdcac6d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import datasets, layers, models, losses, Model\n",
    "from keras.layers import Input, Conv2D, MaxPool2D, MaxPooling2D, Activation, Flatten, Dense, AveragePooling2D, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dbba8400",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeDataWholeSeed(data,normalization_type='max'):\n",
    "    \n",
    "    if normalization_type == 'max':\n",
    "        for idx in range(data.shape[0]):\n",
    "            data[idx,:,:,:] = data[idx,:,:,:]/np.max(abs(data[idx,:,:,:]))\n",
    "            \n",
    "    elif normalization_type == 'l2norm':\n",
    "        from numpy import linalg as LA\n",
    "        for idx in range(data.shape[0]):\n",
    "            data[idx,:,:,:] = data[idx,:,:,:]/LA.norm(data[idx,:,:,:])       \n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e2b9d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception(x,filters_1x1,filters_3x3_reduce,filters_3x3,filters_5x5_reduce,filters_5x5,filters_pool,activation_type='relu'):\n",
    "    path1 = Conv2D(filters_1x1,        (1, 1), padding='same', activation=activation_type)(x)\n",
    "    \n",
    "    path2 = Conv2D(filters_3x3_reduce, (1, 1), padding='same', activation=activation_type)(x)\n",
    "    path2 = Conv2D(filters_3x3,        (1, 1), padding='same', activation=activation_type)(path2)\n",
    "    \n",
    "    path3 = Conv2D(filters_5x5_reduce, (1, 1), padding='same', activation=activation_type)(x)\n",
    "    path3 = Conv2D(filters_5x5,        (1, 1), padding='same', activation=activation_type)(path3)\n",
    "    \n",
    "    path4 = MaxPool2D((3, 3),  strides=(1, 1), padding='same')(x)\n",
    "    path4 = Conv2D(filters_pool,       (1, 1), padding='same', activation=activation_type)(path4)\n",
    "    \n",
    "    return tf.concat([path1, path2, path3, path4], axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ffc219f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auxiliary_classifier(x,num_classes,activation_type='relu'):\n",
    "    aux = AveragePooling2D((5, 5), strides=3)(x)\n",
    "    aux = Conv2D(128, 1, padding='same', activation=activation_type)(aux)\n",
    "    aux = Flatten()(aux)\n",
    "    aux = Dense(1024, activation=activation_type)(aux)\n",
    "    aux = Dropout(0.7)(aux)\n",
    "    aux = Dense(num_classes, activation='softmax')(aux)\n",
    "    return aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee201be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GoogleNetModel(data_num_rows, data_num_cols, num_input_chans=1, num_classes=NUM_VARIETIES, activation_type='relu', dropout_rate=0.0):\n",
    "\n",
    "    inp = Input(shape=(data_num_rows, data_num_cols, num_input_chans))\n",
    "    input_tensor = layers.experimental.preprocessing.Resizing(224, 224, interpolation=\"bilinear\", input_shape=train_dataset.shape[1:])(inp)\n",
    "    x = Conv2D(64,  7, strides=2, padding='same', activation=activation_type)(input_tensor)\n",
    "    x = MaxPooling2D(3, strides=2)(x)\n",
    "    x = Conv2D(64,  1, strides=1, padding='same', activation=activation_type)(x)\n",
    "    x = Conv2D(192, 3, strides=1, padding='same', activation=activation_type)(x)\n",
    "    x = MaxPooling2D(3, strides=2)(x)\n",
    "    x = inception(x, filters_1x1=64 , filters_3x3_reduce=96 , filters_3x3=128, filters_5x5_reduce=16, filters_5x5=32, filters_pool=32)\n",
    "    x = inception(x, filters_1x1=128, filters_3x3_reduce=128, filters_3x3=192, filters_5x5_reduce=32, filters_5x5=96, filters_pool=64)\n",
    "    x = MaxPooling2D(3, strides=2)(x)\n",
    "    x = inception(x, filters_1x1=192, filters_3x3_reduce=96 , filters_3x3=208, filters_5x5_reduce=16, filters_5x5=48, filters_pool=64)\n",
    "    5023\n",
    "    aux1 = auxiliary_classifier(x,num_classes)\n",
    "    \n",
    "    x = inception(x, filters_1x1=160, filters_3x3_reduce=112, filters_3x3=224, filters_5x5_reduce=24, filters_5x5=64, filters_pool=64)\n",
    "    x = inception(x, filters_1x1=128, filters_3x3_reduce=128, filters_3x3=256, filters_5x5_reduce=24, filters_5x5=64, filters_pool=64)\n",
    "    x = inception(x, filters_1x1=112, filters_3x3_reduce=144, filters_3x3=288, filters_5x5_reduce=32, filters_5x5=64, filters_pool=64)\n",
    "    \n",
    "    aux2 = auxiliary_classifier(x,num_classes)\n",
    "    \n",
    "    x = inception(x, filters_1x1=256, filters_3x3_reduce=160, filters_3x3=320, filters_5x5_reduce=32, filters_5x5=128, filters_pool=128)\n",
    "    x = MaxPooling2D(3, strides=2)(x)\n",
    "    x = inception(x, filters_1x1=256, filters_3x3_reduce=160, filters_3x3=320, filters_5x5_reduce=32, filters_5x5=128, filters_pool=128)\n",
    "    x = inception(x, filters_1x1=384, filters_3x3_reduce=192, filters_3x3=384, filters_5x5_reduce=48, filters_5x5=128, filters_pool=128)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    out = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    return Model(inputs = inp, outputs = [out, aux1, aux2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f63e29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGoogleNetModel():\n",
    "    learning_rate_base = LEARNING_RATE_BASE\n",
    "    activation_type = ACTIVATION_TYPE\n",
    "    lr_schedule = ExponentialDecay(\n",
    "            learning_rate_base,\n",
    "            decay_steps = 20000,\n",
    "            decay_rate = 0.01,\n",
    "            staircase = True\n",
    "    )\n",
    "    wheat_types =  VARIETIES\n",
    "    num_classes = len(wheat_types)\n",
    "    dropout_rate = 0.4\n",
    "    print(\"--------------Load Data--------------\")\n",
    "\n",
    "    x_training = np.array(train_dataset)\n",
    "    labels_training = np.array(train_dataset_label)\n",
    "    \n",
    "    # Normalize the data\n",
    "    x_training = normalizeDataWholeSeed(x_training)\n",
    "    \n",
    "    # Extract some information\n",
    "    num_train = x_training.shape[0]\n",
    "    N_spatial = x_training.shape[1:3]\n",
    "    N_channel = x_training.shape[3]\n",
    "    \n",
    "    print(\"--------------Done--------------\")\n",
    "    \n",
    "    ############ Create a model ############\n",
    "    print(\"--------------Create a model--------------\")\n",
    "    \n",
    "    # Generate a model\n",
    "    model = GoogleNetModel(data_num_rows = N_spatial[0], \n",
    "                           data_num_cols = N_spatial[1],\n",
    "                           num_input_chans = N_channel, \n",
    "                           num_classes = num_classes,\n",
    "                           activation_type = activation_type,\n",
    "                           dropout_rate = dropout_rate)\n",
    "\n",
    "    # Compile the model\n",
    "    adam_opt = Adam(learning_rate=lr_schedule, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "    model.compile(optimizer=adam_opt, loss=[losses.sparse_categorical_crossentropy,losses.sparse_categorical_crossentropy,losses.sparse_categorical_crossentropy],loss_weights=[1, 0.3, 0.3],metrics=['accuracy'])\n",
    "    print(\"---------Completed---------\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ee0c5e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(df,title,xlabel,ylabel,values=['loss'],legends=[]):\n",
    "    \n",
    "    for value in values:\n",
    "        epoch_count = range(1, len(df.index) + 1)\n",
    "        plt.plot(epoch_count, df[value].tolist())\n",
    "    plt.title(title)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xlabel(xlabel)\n",
    "    if legends==[]:\n",
    "        legends = values\n",
    "    plt.legend(legends, loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7e16422f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "x_val = []\n",
    "y_val = []\n",
    "\n",
    "for i in range(len(train_dataset)):\n",
    "    if i%5==0:\n",
    "        x_val.append(train_dataset[i])\n",
    "        y_val.append(train_dataset_label[i])\n",
    "    else:\n",
    "        x_train.append(train_dataset[i])\n",
    "        y_train.append(train_dataset_label[i])\n",
    "        \n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "y_train = [y_train,y_train,y_train]\n",
    "\n",
    "x_val = np.array(x_val)\n",
    "\n",
    "y_val = np.array(y_val)\n",
    "y_val = [y_val,y_val,y_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "83b59af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def save_to_csv(file_path, data_frame, header=False):\n",
    "    file_exists = os.path.exists(file_path)\n",
    "\n",
    "    if not file_exists or not header:\n",
    "        data_frame.to_csv(file_path, index=False, mode='w')\n",
    "    else:\n",
    "        data_frame.to_csv(file_path, index=False, mode='a', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c9cfa22c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PBW 343',\n",
       " 'PBW 766',\n",
       " 'PBW 725',\n",
       " 'DBW 187',\n",
       " 'HD 3086',\n",
       " 'PBW 771',\n",
       " 'PBW 677',\n",
       " 'PBW 373']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VARIETIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "907b14a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5f2f0a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "130530fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------Load Data--------------\n",
      "--------------Done--------------\n",
      "--------------Create a model--------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-09 12:41:24.082441: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21665 MB memory:  -> device: 0, name: NVIDIA RTX A5000, pci bus id: 0000:9b:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Completed---------\n"
     ]
    }
   ],
   "source": [
    "model_name = \"GN_\"+\"_IC_\"+str(TOTAL_IMAGE_COUNT).zfill(5)+\"_FilledArea_\"+str(FILLED_AREA_RATIO)+\"_BandNo_\"+str(BAND_NUMBER)+\"_ImageHeight_\"+str(IMAGE_HEIGHT)+\"_ImageWidth_\"+str(IMAGE_WIDTH)+\"_FILTER_\"+str(FILTER)+\"_FeatureExtraction_\"+str(FEATURE_EXTRACTION)\n",
    "if REMOVE_NOISY_BANDS:\n",
    "    model_name+=\"_REMOVE_NOISY_BANDS_\"+str(REMOVE_NOISY_BANDS)+\"_NumOfBands_\"+str(NUM_OF_BANDS)+\"_FB_\"+str(FIRST_BAND)+\"_LB_\"+str(LAST_BAND)\n",
    "if FILTER == \"savgol\":\n",
    "    model_name+=\"_WINDOW_\"+str(WINDOW)+\"_ORDER_\"+str(ORDER)\n",
    "\n",
    "if start_epoch != 1:\n",
    "    model = tf.keras.models.load_model('./GNmodels/'+str(start_epoch-1)+model_name)\n",
    "else:\n",
    "    model = getGoogleNetModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9d4ed88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_epoch = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "55ac4628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 30, 30, 168)]        0         []                            \n",
      "                                                                                                  \n",
      " resizing (Resizing)         (None, 224, 224, 168)        0         ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)             (None, 112, 112, 64)         526912    ['resizing[0][0]']            \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2  (None, 55, 55, 64)           0         ['conv2d[0][0]']              \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)           (None, 55, 55, 64)           4160      ['max_pooling2d[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)           (None, 55, 55, 192)          110784    ['conv2d_1[0][0]']            \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPoolin  (None, 27, 27, 192)          0         ['conv2d_2[0][0]']            \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)           (None, 27, 27, 96)           18528     ['max_pooling2d_1[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)           (None, 27, 27, 16)           3088      ['max_pooling2d_1[0][0]']     \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPoolin  (None, 27, 27, 192)          0         ['max_pooling2d_1[0][0]']     \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)           (None, 27, 27, 64)           12352     ['max_pooling2d_1[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)           (None, 27, 27, 128)          12416     ['conv2d_4[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)           (None, 27, 27, 32)           544       ['conv2d_6[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)           (None, 27, 27, 32)           6176      ['max_pooling2d_2[0][0]']     \n",
      "                                                                                                  \n",
      " tf.concat (TFOpLambda)      (None, 27, 27, 256)          0         ['conv2d_3[0][0]',            \n",
      "                                                                     'conv2d_5[0][0]',            \n",
      "                                                                     'conv2d_7[0][0]',            \n",
      "                                                                     'conv2d_8[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)          (None, 27, 27, 128)          32896     ['tf.concat[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)          (None, 27, 27, 32)           8224      ['tf.concat[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPoolin  (None, 27, 27, 256)          0         ['tf.concat[0][0]']           \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)           (None, 27, 27, 128)          32896     ['tf.concat[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)          (None, 27, 27, 192)          24768     ['conv2d_10[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)          (None, 27, 27, 96)           3168      ['conv2d_12[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)          (None, 27, 27, 64)           16448     ['max_pooling2d_3[0][0]']     \n",
      "                                                                                                  \n",
      " tf.concat_1 (TFOpLambda)    (None, 27, 27, 480)          0         ['conv2d_9[0][0]',            \n",
      "                                                                     'conv2d_11[0][0]',           \n",
      "                                                                     'conv2d_13[0][0]',           \n",
      "                                                                     'conv2d_14[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling2d_4 (MaxPoolin  (None, 13, 13, 480)          0         ['tf.concat_1[0][0]']         \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)          (None, 13, 13, 96)           46176     ['max_pooling2d_4[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)          (None, 13, 13, 16)           7696      ['max_pooling2d_4[0][0]']     \n",
      "                                                                                                  \n",
      " max_pooling2d_5 (MaxPoolin  (None, 13, 13, 480)          0         ['max_pooling2d_4[0][0]']     \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)          (None, 13, 13, 192)          92352     ['max_pooling2d_4[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)          (None, 13, 13, 208)          20176     ['conv2d_16[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)          (None, 13, 13, 48)           816       ['conv2d_18[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)          (None, 13, 13, 64)           30784     ['max_pooling2d_5[0][0]']     \n",
      "                                                                                                  \n",
      " tf.concat_2 (TFOpLambda)    (None, 13, 13, 512)          0         ['conv2d_15[0][0]',           \n",
      "                                                                     'conv2d_17[0][0]',           \n",
      "                                                                     'conv2d_19[0][0]',           \n",
      "                                                                     'conv2d_20[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)          (None, 13, 13, 112)          57456     ['tf.concat_2[0][0]']         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " conv2d_25 (Conv2D)          (None, 13, 13, 24)           12312     ['tf.concat_2[0][0]']         \n",
      "                                                                                                  \n",
      " max_pooling2d_6 (MaxPoolin  (None, 13, 13, 512)          0         ['tf.concat_2[0][0]']         \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)          (None, 13, 13, 160)          82080     ['tf.concat_2[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)          (None, 13, 13, 224)          25312     ['conv2d_23[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_26 (Conv2D)          (None, 13, 13, 64)           1600      ['conv2d_25[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_27 (Conv2D)          (None, 13, 13, 64)           32832     ['max_pooling2d_6[0][0]']     \n",
      "                                                                                                  \n",
      " tf.concat_3 (TFOpLambda)    (None, 13, 13, 512)          0         ['conv2d_22[0][0]',           \n",
      "                                                                     'conv2d_24[0][0]',           \n",
      "                                                                     'conv2d_26[0][0]',           \n",
      "                                                                     'conv2d_27[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_29 (Conv2D)          (None, 13, 13, 128)          65664     ['tf.concat_3[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_31 (Conv2D)          (None, 13, 13, 24)           12312     ['tf.concat_3[0][0]']         \n",
      "                                                                                                  \n",
      " max_pooling2d_7 (MaxPoolin  (None, 13, 13, 512)          0         ['tf.concat_3[0][0]']         \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_28 (Conv2D)          (None, 13, 13, 128)          65664     ['tf.concat_3[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_30 (Conv2D)          (None, 13, 13, 256)          33024     ['conv2d_29[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_32 (Conv2D)          (None, 13, 13, 64)           1600      ['conv2d_31[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_33 (Conv2D)          (None, 13, 13, 64)           32832     ['max_pooling2d_7[0][0]']     \n",
      "                                                                                                  \n",
      " tf.concat_4 (TFOpLambda)    (None, 13, 13, 512)          0         ['conv2d_28[0][0]',           \n",
      "                                                                     'conv2d_30[0][0]',           \n",
      "                                                                     'conv2d_32[0][0]',           \n",
      "                                                                     'conv2d_33[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_35 (Conv2D)          (None, 13, 13, 144)          73872     ['tf.concat_4[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_37 (Conv2D)          (None, 13, 13, 32)           16416     ['tf.concat_4[0][0]']         \n",
      "                                                                                                  \n",
      " max_pooling2d_8 (MaxPoolin  (None, 13, 13, 512)          0         ['tf.concat_4[0][0]']         \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_34 (Conv2D)          (None, 13, 13, 112)          57456     ['tf.concat_4[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_36 (Conv2D)          (None, 13, 13, 288)          41760     ['conv2d_35[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_38 (Conv2D)          (None, 13, 13, 64)           2112      ['conv2d_37[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_39 (Conv2D)          (None, 13, 13, 64)           32832     ['max_pooling2d_8[0][0]']     \n",
      "                                                                                                  \n",
      " tf.concat_5 (TFOpLambda)    (None, 13, 13, 528)          0         ['conv2d_34[0][0]',           \n",
      "                                                                     'conv2d_36[0][0]',           \n",
      "                                                                     'conv2d_38[0][0]',           \n",
      "                                                                     'conv2d_39[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_42 (Conv2D)          (None, 13, 13, 160)          84640     ['tf.concat_5[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_44 (Conv2D)          (None, 13, 13, 32)           16928     ['tf.concat_5[0][0]']         \n",
      "                                                                                                  \n",
      " max_pooling2d_9 (MaxPoolin  (None, 13, 13, 528)          0         ['tf.concat_5[0][0]']         \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_41 (Conv2D)          (None, 13, 13, 256)          135424    ['tf.concat_5[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_43 (Conv2D)          (None, 13, 13, 320)          51520     ['conv2d_42[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_45 (Conv2D)          (None, 13, 13, 128)          4224      ['conv2d_44[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_46 (Conv2D)          (None, 13, 13, 128)          67712     ['max_pooling2d_9[0][0]']     \n",
      "                                                                                                  \n",
      " tf.concat_6 (TFOpLambda)    (None, 13, 13, 832)          0         ['conv2d_41[0][0]',           \n",
      "                                                                     'conv2d_43[0][0]',           \n",
      "                                                                     'conv2d_45[0][0]',           \n",
      "                                                                     'conv2d_46[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling2d_10 (MaxPooli  (None, 6, 6, 832)            0         ['tf.concat_6[0][0]']         \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " conv2d_48 (Conv2D)          (None, 6, 6, 160)            133280    ['max_pooling2d_10[0][0]']    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " conv2d_50 (Conv2D)          (None, 6, 6, 32)             26656     ['max_pooling2d_10[0][0]']    \n",
      "                                                                                                  \n",
      " max_pooling2d_11 (MaxPooli  (None, 6, 6, 832)            0         ['max_pooling2d_10[0][0]']    \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " conv2d_47 (Conv2D)          (None, 6, 6, 256)            213248    ['max_pooling2d_10[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_49 (Conv2D)          (None, 6, 6, 320)            51520     ['conv2d_48[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_51 (Conv2D)          (None, 6, 6, 128)            4224      ['conv2d_50[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_52 (Conv2D)          (None, 6, 6, 128)            106624    ['max_pooling2d_11[0][0]']    \n",
      "                                                                                                  \n",
      " tf.concat_7 (TFOpLambda)    (None, 6, 6, 832)            0         ['conv2d_47[0][0]',           \n",
      "                                                                     'conv2d_49[0][0]',           \n",
      "                                                                     'conv2d_51[0][0]',           \n",
      "                                                                     'conv2d_52[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_54 (Conv2D)          (None, 6, 6, 192)            159936    ['tf.concat_7[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_56 (Conv2D)          (None, 6, 6, 48)             39984     ['tf.concat_7[0][0]']         \n",
      "                                                                                                  \n",
      " max_pooling2d_12 (MaxPooli  (None, 6, 6, 832)            0         ['tf.concat_7[0][0]']         \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " average_pooling2d (Average  (None, 3, 3, 512)            0         ['tf.concat_2[0][0]']         \n",
      " Pooling2D)                                                                                       \n",
      "                                                                                                  \n",
      " average_pooling2d_1 (Avera  (None, 3, 3, 528)            0         ['tf.concat_5[0][0]']         \n",
      " gePooling2D)                                                                                     \n",
      "                                                                                                  \n",
      " conv2d_53 (Conv2D)          (None, 6, 6, 384)            319872    ['tf.concat_7[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_55 (Conv2D)          (None, 6, 6, 384)            74112     ['conv2d_54[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_57 (Conv2D)          (None, 6, 6, 128)            6272      ['conv2d_56[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_58 (Conv2D)          (None, 6, 6, 128)            106624    ['max_pooling2d_12[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)          (None, 3, 3, 128)            65664     ['average_pooling2d[0][0]']   \n",
      "                                                                                                  \n",
      " conv2d_40 (Conv2D)          (None, 3, 3, 128)            67712     ['average_pooling2d_1[0][0]'] \n",
      "                                                                                                  \n",
      " tf.concat_8 (TFOpLambda)    (None, 6, 6, 1024)           0         ['conv2d_53[0][0]',           \n",
      "                                                                     'conv2d_55[0][0]',           \n",
      "                                                                     'conv2d_57[0][0]',           \n",
      "                                                                     'conv2d_58[0][0]']           \n",
      "                                                                                                  \n",
      " flatten (Flatten)           (None, 1152)                 0         ['conv2d_21[0][0]']           \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)         (None, 1152)                 0         ['conv2d_40[0][0]']           \n",
      "                                                                                                  \n",
      " global_average_pooling2d (  (None, 1024)                 0         ['tf.concat_8[0][0]']         \n",
      " GlobalAveragePooling2D)                                                                          \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 1024)                 1180672   ['flatten[0][0]']             \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 1024)                 1180672   ['flatten_1[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)         (None, 1024)                 0         ['global_average_pooling2d[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 1024)                 0         ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 1024)                 0         ['dense_2[0][0]']             \n",
      "                                                                                                  \n",
      " dense_4 (Dense)             (None, 8)                    8200      ['dropout_2[0][0]']           \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 8)                    8200      ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 8)                    8200      ['dropout_1[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5780616 (22.05 MB)\n",
      "Trainable params: 5780616 (22.05 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2315f352",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "\n",
    "directory = './csvs'\n",
    "\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "071db278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing started\n",
      "\n",
      "Epoch:  6\n",
      "480/480 - 33s - loss: 2.0769 - dense_4_loss: 1.2628 - dense_1_loss: 1.3739 - dense_3_loss: 1.3398 - dense_4_accuracy: 0.5242 - dense_1_accuracy: 0.4688 - dense_3_accuracy: 0.4906 - val_loss: 1.9229 - val_dense_4_loss: 1.1639 - val_dense_1_loss: 1.2841 - val_dense_3_loss: 1.2459 - val_dense_4_accuracy: 0.5599 - val_dense_1_accuracy: 0.5047 - val_dense_3_accuracy: 0.5214 - 33s/epoch - 70ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/6GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/6GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  6\n",
      "added to csv\n",
      "\n",
      "Epoch:  7\n",
      "480/480 - 34s - loss: 1.8446 - dense_4_loss: 1.1130 - dense_1_loss: 1.2420 - dense_3_loss: 1.1968 - dense_4_accuracy: 0.5758 - dense_1_accuracy: 0.5219 - dense_3_accuracy: 0.5437 - val_loss: 1.5554 - val_dense_4_loss: 0.9308 - val_dense_1_loss: 1.0744 - val_dense_3_loss: 1.0075 - val_dense_4_accuracy: 0.6432 - val_dense_1_accuracy: 0.6016 - val_dense_3_accuracy: 0.6187 - 34s/epoch - 72ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/7GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/7GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  7\n",
      "added to csv\n",
      "\n",
      "Epoch:  8\n",
      "480/480 - 33s - loss: 1.6316 - dense_4_loss: 0.9759 - dense_1_loss: 1.1280 - dense_3_loss: 1.0576 - dense_4_accuracy: 0.6327 - dense_1_accuracy: 0.5688 - dense_3_accuracy: 0.5962 - val_loss: 1.3453 - val_dense_4_loss: 0.7895 - val_dense_1_loss: 0.9770 - val_dense_3_loss: 0.8758 - val_dense_4_accuracy: 0.6969 - val_dense_1_accuracy: 0.6505 - val_dense_3_accuracy: 0.6766 - 33s/epoch - 70ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/8GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/8GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  8\n",
      "added to csv\n",
      "\n",
      "Epoch:  9\n",
      "480/480 - 33s - loss: 1.4965 - dense_4_loss: 0.8904 - dense_1_loss: 1.0502 - dense_3_loss: 0.9700 - dense_4_accuracy: 0.6652 - dense_1_accuracy: 0.6089 - dense_3_accuracy: 0.6365 - val_loss: 1.2170 - val_dense_4_loss: 0.7142 - val_dense_1_loss: 0.8820 - val_dense_3_loss: 0.7941 - val_dense_4_accuracy: 0.7370 - val_dense_1_accuracy: 0.6802 - val_dense_3_accuracy: 0.7078 - 33s/epoch - 69ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/9GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/9GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  9\n",
      "added to csv\n",
      "\n",
      "Epoch:  10\n",
      "480/480 - 33s - loss: 1.3459 - dense_4_loss: 0.7972 - dense_1_loss: 0.9542 - dense_3_loss: 0.8748 - dense_4_accuracy: 0.7013 - dense_1_accuracy: 0.6358 - dense_3_accuracy: 0.6740 - val_loss: 1.1111 - val_dense_4_loss: 0.6516 - val_dense_1_loss: 0.8126 - val_dense_3_loss: 0.7190 - val_dense_4_accuracy: 0.7516 - val_dense_1_accuracy: 0.7068 - val_dense_3_accuracy: 0.7380 - 33s/epoch - 69ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/10GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/10GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  10\n",
      "added to csv\n",
      "\n",
      "Epoch:  11\n",
      "480/480 - 34s - loss: 1.2410 - dense_4_loss: 0.7340 - dense_1_loss: 0.8873 - dense_3_loss: 0.8029 - dense_4_accuracy: 0.7266 - dense_1_accuracy: 0.6625 - dense_3_accuracy: 0.6967 - val_loss: 1.1414 - val_dense_4_loss: 0.6792 - val_dense_1_loss: 0.7856 - val_dense_3_loss: 0.7553 - val_dense_4_accuracy: 0.7432 - val_dense_1_accuracy: 0.7052 - val_dense_3_accuracy: 0.7083 - 34s/epoch - 70ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/11GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/11GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  11\n",
      "added to csv\n",
      "\n",
      "Epoch:  12\n",
      "480/480 - 33s - loss: 1.1002 - dense_4_loss: 0.6448 - dense_1_loss: 0.7997 - dense_3_loss: 0.7184 - dense_4_accuracy: 0.7553 - dense_1_accuracy: 0.6931 - dense_3_accuracy: 0.7267 - val_loss: 1.1252 - val_dense_4_loss: 0.6646 - val_dense_1_loss: 0.7910 - val_dense_3_loss: 0.7446 - val_dense_4_accuracy: 0.7370 - val_dense_1_accuracy: 0.7010 - val_dense_3_accuracy: 0.7073 - 33s/epoch - 68ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/12GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/12GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  12\n",
      "added to csv\n",
      "\n",
      "Epoch:  13\n",
      "480/480 - 34s - loss: 1.0620 - dense_4_loss: 0.6236 - dense_1_loss: 0.7764 - dense_3_loss: 0.6849 - dense_4_accuracy: 0.7583 - dense_1_accuracy: 0.7098 - dense_3_accuracy: 0.7439 - val_loss: 0.9009 - val_dense_4_loss: 0.5298 - val_dense_1_loss: 0.6526 - val_dense_3_loss: 0.5843 - val_dense_4_accuracy: 0.7932 - val_dense_1_accuracy: 0.7661 - val_dense_3_accuracy: 0.7734 - 34s/epoch - 70ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/13GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/13GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  13\n",
      "added to csv\n",
      "\n",
      "Epoch:  14\n",
      "480/480 - 33s - loss: 0.9754 - dense_4_loss: 0.5678 - dense_1_loss: 0.7227 - dense_3_loss: 0.6358 - dense_4_accuracy: 0.7850 - dense_1_accuracy: 0.7247 - dense_3_accuracy: 0.7605 - val_loss: 0.8786 - val_dense_4_loss: 0.5158 - val_dense_1_loss: 0.6359 - val_dense_3_loss: 0.5733 - val_dense_4_accuracy: 0.7964 - val_dense_1_accuracy: 0.7688 - val_dense_3_accuracy: 0.7771 - 33s/epoch - 69ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/14GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/14GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  14\n",
      "added to csv\n",
      "\n",
      "Epoch:  15\n",
      "480/480 - 33s - loss: 0.8443 - dense_4_loss: 0.4871 - dense_1_loss: 0.6444 - dense_3_loss: 0.5461 - dense_4_accuracy: 0.8086 - dense_1_accuracy: 0.7531 - dense_3_accuracy: 0.7882 - val_loss: 0.9625 - val_dense_4_loss: 0.5446 - val_dense_1_loss: 0.7234 - val_dense_3_loss: 0.6698 - val_dense_4_accuracy: 0.7812 - val_dense_1_accuracy: 0.7219 - val_dense_3_accuracy: 0.7349 - 33s/epoch - 68ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/15GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/15GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  15\n",
      "added to csv\n",
      "\n",
      "Epoch:  16\n",
      "480/480 - 33s - loss: 0.7906 - dense_4_loss: 0.4557 - dense_1_loss: 0.6011 - dense_3_loss: 0.5152 - dense_4_accuracy: 0.8211 - dense_1_accuracy: 0.7737 - dense_3_accuracy: 0.8048 - val_loss: 0.9628 - val_dense_4_loss: 0.5677 - val_dense_1_loss: 0.7073 - val_dense_3_loss: 0.6095 - val_dense_4_accuracy: 0.7672 - val_dense_1_accuracy: 0.7255 - val_dense_3_accuracy: 0.7536 - 33s/epoch - 68ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/16GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/16GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  16\n",
      "added to csv\n",
      "\n",
      "Epoch:  17\n",
      "480/480 - 33s - loss: 0.7429 - dense_4_loss: 0.4219 - dense_1_loss: 0.5828 - dense_3_loss: 0.4872 - dense_4_accuracy: 0.8365 - dense_1_accuracy: 0.7818 - dense_3_accuracy: 0.8089 - val_loss: 0.7739 - val_dense_4_loss: 0.4450 - val_dense_1_loss: 0.5921 - val_dense_3_loss: 0.5044 - val_dense_4_accuracy: 0.8172 - val_dense_1_accuracy: 0.7740 - val_dense_3_accuracy: 0.7953 - 33s/epoch - 68ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/17GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/17GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  17\n",
      "added to csv\n",
      "\n",
      "Epoch:  18\n",
      "480/480 - 33s - loss: 0.7142 - dense_4_loss: 0.4060 - dense_1_loss: 0.5604 - dense_3_loss: 0.4671 - dense_4_accuracy: 0.8444 - dense_1_accuracy: 0.7872 - dense_3_accuracy: 0.8194 - val_loss: 1.7765 - val_dense_4_loss: 1.1265 - val_dense_1_loss: 1.0961 - val_dense_3_loss: 1.0705 - val_dense_4_accuracy: 0.6667 - val_dense_1_accuracy: 0.6344 - val_dense_3_accuracy: 0.6484 - 33s/epoch - 68ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/18GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/18GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  18\n",
      "added to csv\n",
      "\n",
      "Epoch:  19\n",
      "480/480 - 33s - loss: 0.7235 - dense_4_loss: 0.4141 - dense_1_loss: 0.5598 - dense_3_loss: 0.4717 - dense_4_accuracy: 0.8401 - dense_1_accuracy: 0.7866 - dense_3_accuracy: 0.8207 - val_loss: 0.6846 - val_dense_4_loss: 0.3950 - val_dense_1_loss: 0.5134 - val_dense_3_loss: 0.4522 - val_dense_4_accuracy: 0.8411 - val_dense_1_accuracy: 0.7958 - val_dense_3_accuracy: 0.8141 - 33s/epoch - 68ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/19GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/19GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  19\n",
      "added to csv\n",
      "\n",
      "Epoch:  20\n",
      "480/480 - 32s - loss: 0.6320 - dense_4_loss: 0.3529 - dense_1_loss: 0.5159 - dense_3_loss: 0.4144 - dense_4_accuracy: 0.8608 - dense_1_accuracy: 0.8074 - dense_3_accuracy: 0.8389 - val_loss: 0.6214 - val_dense_4_loss: 0.3471 - val_dense_1_loss: 0.5116 - val_dense_3_loss: 0.4027 - val_dense_4_accuracy: 0.8682 - val_dense_1_accuracy: 0.8031 - val_dense_3_accuracy: 0.8359 - 32s/epoch - 67ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/20GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/20GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  20\n",
      "added to csv\n",
      "\n",
      "Epoch:  21\n",
      "480/480 - 33s - loss: 0.5985 - dense_4_loss: 0.3333 - dense_1_loss: 0.4873 - dense_3_loss: 0.3966 - dense_4_accuracy: 0.8707 - dense_1_accuracy: 0.8156 - dense_3_accuracy: 0.8492 - val_loss: 0.5907 - val_dense_4_loss: 0.3406 - val_dense_1_loss: 0.4571 - val_dense_3_loss: 0.3767 - val_dense_4_accuracy: 0.8750 - val_dense_1_accuracy: 0.8250 - val_dense_3_accuracy: 0.8589 - 33s/epoch - 69ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/21GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/21GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  21\n",
      "added to csv\n",
      "\n",
      "Epoch:  22\n",
      "480/480 - 32s - loss: 0.5679 - dense_4_loss: 0.3125 - dense_1_loss: 0.4734 - dense_3_loss: 0.3782 - dense_4_accuracy: 0.8776 - dense_1_accuracy: 0.8227 - dense_3_accuracy: 0.8559 - val_loss: 0.6117 - val_dense_4_loss: 0.3430 - val_dense_1_loss: 0.4678 - val_dense_3_loss: 0.4278 - val_dense_4_accuracy: 0.8656 - val_dense_1_accuracy: 0.8177 - val_dense_3_accuracy: 0.8328 - 32s/epoch - 67ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/22GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/22GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  22\n",
      "added to csv\n",
      "\n",
      "Epoch:  23\n",
      "480/480 - 33s - loss: 0.5464 - dense_4_loss: 0.2985 - dense_1_loss: 0.4681 - dense_3_loss: 0.3583 - dense_4_accuracy: 0.8885 - dense_1_accuracy: 0.8221 - dense_3_accuracy: 0.8637 - val_loss: 0.6858 - val_dense_4_loss: 0.4098 - val_dense_1_loss: 0.5099 - val_dense_3_loss: 0.4102 - val_dense_4_accuracy: 0.8349 - val_dense_1_accuracy: 0.7937 - val_dense_3_accuracy: 0.8370 - 33s/epoch - 70ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/23GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/23GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  23\n",
      "added to csv\n",
      "\n",
      "Epoch:  24\n",
      "480/480 - 32s - loss: 0.4970 - dense_4_loss: 0.2674 - dense_1_loss: 0.4328 - dense_3_loss: 0.3328 - dense_4_accuracy: 0.8952 - dense_1_accuracy: 0.8365 - dense_3_accuracy: 0.8738 - val_loss: 0.6035 - val_dense_4_loss: 0.3338 - val_dense_1_loss: 0.4879 - val_dense_3_loss: 0.4109 - val_dense_4_accuracy: 0.8698 - val_dense_1_accuracy: 0.8104 - val_dense_3_accuracy: 0.8385 - 32s/epoch - 68ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/24GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/24GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  24\n",
      "added to csv\n",
      "\n",
      "Epoch:  25\n",
      "480/480 - 32s - loss: 0.4957 - dense_4_loss: 0.2695 - dense_1_loss: 0.4266 - dense_3_loss: 0.3275 - dense_4_accuracy: 0.8961 - dense_1_accuracy: 0.8367 - dense_3_accuracy: 0.8781 - val_loss: 0.5887 - val_dense_4_loss: 0.3306 - val_dense_1_loss: 0.4777 - val_dense_3_loss: 0.3824 - val_dense_4_accuracy: 0.8714 - val_dense_1_accuracy: 0.8219 - val_dense_3_accuracy: 0.8510 - 32s/epoch - 67ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/25GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/25GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  25\n",
      "added to csv\n",
      "\n",
      "Epoch:  26\n",
      "480/480 - 32s - loss: 0.4684 - dense_4_loss: 0.2523 - dense_1_loss: 0.4071 - dense_3_loss: 0.3130 - dense_4_accuracy: 0.8988 - dense_1_accuracy: 0.8441 - dense_3_accuracy: 0.8829 - val_loss: 0.4700 - val_dense_4_loss: 0.2554 - val_dense_1_loss: 0.3971 - val_dense_3_loss: 0.3180 - val_dense_4_accuracy: 0.9036 - val_dense_1_accuracy: 0.8516 - val_dense_3_accuracy: 0.8802 - 32s/epoch - 68ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/26GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/26GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  26\n",
      "added to csv\n",
      "\n",
      "Epoch:  27\n",
      "480/480 - 33s - loss: 0.4771 - dense_4_loss: 0.2592 - dense_1_loss: 0.4139 - dense_3_loss: 0.3122 - dense_4_accuracy: 0.8990 - dense_1_accuracy: 0.8415 - dense_3_accuracy: 0.8801 - val_loss: 0.4763 - val_dense_4_loss: 0.2543 - val_dense_1_loss: 0.4134 - val_dense_3_loss: 0.3265 - val_dense_4_accuracy: 0.9062 - val_dense_1_accuracy: 0.8464 - val_dense_3_accuracy: 0.8703 - 33s/epoch - 69ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/27GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/27GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  27\n",
      "added to csv\n",
      "\n",
      "Epoch:  28\n",
      "480/480 - 33s - loss: 0.4294 - dense_4_loss: 0.2288 - dense_1_loss: 0.3902 - dense_3_loss: 0.2783 - dense_4_accuracy: 0.9128 - dense_1_accuracy: 0.8497 - dense_3_accuracy: 0.8944 - val_loss: 0.6185 - val_dense_4_loss: 0.3380 - val_dense_1_loss: 0.4587 - val_dense_3_loss: 0.4762 - val_dense_4_accuracy: 0.8823 - val_dense_1_accuracy: 0.8307 - val_dense_3_accuracy: 0.8276 - 33s/epoch - 69ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/28GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/28GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  28\n",
      "added to csv\n",
      "\n",
      "Epoch:  29\n",
      "480/480 - 33s - loss: 0.4149 - dense_4_loss: 0.2178 - dense_1_loss: 0.3780 - dense_3_loss: 0.2791 - dense_4_accuracy: 0.9176 - dense_1_accuracy: 0.8618 - dense_3_accuracy: 0.8957 - val_loss: 0.4916 - val_dense_4_loss: 0.2718 - val_dense_1_loss: 0.4170 - val_dense_3_loss: 0.3157 - val_dense_4_accuracy: 0.8917 - val_dense_1_accuracy: 0.8474 - val_dense_3_accuracy: 0.8766 - 33s/epoch - 68ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/29GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/29GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  29\n",
      "added to csv\n",
      "\n",
      "Epoch:  30\n",
      "480/480 - 33s - loss: 0.3511 - dense_4_loss: 0.1770 - dense_1_loss: 0.3446 - dense_3_loss: 0.2355 - dense_4_accuracy: 0.9310 - dense_1_accuracy: 0.8693 - dense_3_accuracy: 0.9092 - val_loss: 0.5259 - val_dense_4_loss: 0.2854 - val_dense_1_loss: 0.4472 - val_dense_3_loss: 0.3545 - val_dense_4_accuracy: 0.8995 - val_dense_1_accuracy: 0.8323 - val_dense_3_accuracy: 0.8641 - 33s/epoch - 69ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/30GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/30GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  30\n",
      "added to csv\n",
      "\n",
      "Epoch:  31\n",
      "480/480 - 33s - loss: 0.3960 - dense_4_loss: 0.2122 - dense_1_loss: 0.3505 - dense_3_loss: 0.2622 - dense_4_accuracy: 0.9245 - dense_1_accuracy: 0.8732 - dense_3_accuracy: 0.9042 - val_loss: 0.5297 - val_dense_4_loss: 0.3007 - val_dense_1_loss: 0.4236 - val_dense_3_loss: 0.3396 - val_dense_4_accuracy: 0.8870 - val_dense_1_accuracy: 0.8552 - val_dense_3_accuracy: 0.8755 - 33s/epoch - 69ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/31GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/31GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  31\n",
      "added to csv\n",
      "\n",
      "Epoch:  32\n",
      "480/480 - 34s - loss: 0.3420 - dense_4_loss: 0.1730 - dense_1_loss: 0.3362 - dense_3_loss: 0.2271 - dense_4_accuracy: 0.9318 - dense_1_accuracy: 0.8717 - dense_3_accuracy: 0.9137 - val_loss: 0.8288 - val_dense_4_loss: 0.4709 - val_dense_1_loss: 0.6337 - val_dense_3_loss: 0.5595 - val_dense_4_accuracy: 0.8469 - val_dense_1_accuracy: 0.7724 - val_dense_3_accuracy: 0.8167 - 34s/epoch - 70ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/32GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/32GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  32\n",
      "added to csv\n",
      "\n",
      "Epoch:  33\n",
      "480/480 - 33s - loss: 0.3411 - dense_4_loss: 0.1751 - dense_1_loss: 0.3259 - dense_3_loss: 0.2273 - dense_4_accuracy: 0.9328 - dense_1_accuracy: 0.8783 - dense_3_accuracy: 0.9147 - val_loss: 0.9229 - val_dense_4_loss: 0.5584 - val_dense_1_loss: 0.5795 - val_dense_3_loss: 0.6354 - val_dense_4_accuracy: 0.8318 - val_dense_1_accuracy: 0.7937 - val_dense_3_accuracy: 0.8042 - 33s/epoch - 70ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/33GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/33GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  33\n",
      "added to csv\n",
      "\n",
      "Epoch:  34\n",
      "480/480 - 33s - loss: 0.2728 - dense_4_loss: 0.1296 - dense_1_loss: 0.2901 - dense_3_loss: 0.1871 - dense_4_accuracy: 0.9501 - dense_1_accuracy: 0.8895 - dense_3_accuracy: 0.9277 - val_loss: 0.4596 - val_dense_4_loss: 0.2497 - val_dense_1_loss: 0.3855 - val_dense_3_loss: 0.3141 - val_dense_4_accuracy: 0.9094 - val_dense_1_accuracy: 0.8594 - val_dense_3_accuracy: 0.8833 - 33s/epoch - 70ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/34GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/34GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  34\n",
      "added to csv\n",
      "\n",
      "Epoch:  35\n",
      "480/480 - 33s - loss: 0.3215 - dense_4_loss: 0.1620 - dense_1_loss: 0.3143 - dense_3_loss: 0.2174 - dense_4_accuracy: 0.9393 - dense_1_accuracy: 0.8836 - dense_3_accuracy: 0.9186 - val_loss: 0.9436 - val_dense_4_loss: 0.5386 - val_dense_1_loss: 0.6809 - val_dense_3_loss: 0.6693 - val_dense_4_accuracy: 0.8328 - val_dense_1_accuracy: 0.7495 - val_dense_3_accuracy: 0.7854 - 33s/epoch - 70ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/35GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/35GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  35\n",
      "added to csv\n",
      "\n",
      "Epoch:  36\n",
      "480/480 - 33s - loss: 0.2543 - dense_4_loss: 0.1214 - dense_1_loss: 0.2667 - dense_3_loss: 0.1764 - dense_4_accuracy: 0.9517 - dense_1_accuracy: 0.9012 - dense_3_accuracy: 0.9324 - val_loss: 0.8181 - val_dense_4_loss: 0.5072 - val_dense_1_loss: 0.5754 - val_dense_3_loss: 0.4606 - val_dense_4_accuracy: 0.8292 - val_dense_1_accuracy: 0.7875 - val_dense_3_accuracy: 0.8427 - 33s/epoch - 70ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/36GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/36GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  36\n",
      "added to csv\n",
      "\n",
      "Epoch:  37\n",
      "480/480 - 33s - loss: 0.2443 - dense_4_loss: 0.1139 - dense_1_loss: 0.2614 - dense_3_loss: 0.1734 - dense_4_accuracy: 0.9583 - dense_1_accuracy: 0.9038 - dense_3_accuracy: 0.9363 - val_loss: 0.6945 - val_dense_4_loss: 0.4252 - val_dense_1_loss: 0.4602 - val_dense_3_loss: 0.4376 - val_dense_4_accuracy: 0.8625 - val_dense_1_accuracy: 0.8354 - val_dense_3_accuracy: 0.8474 - 33s/epoch - 69ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/37GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/37GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  37\n",
      "added to csv\n",
      "\n",
      "Epoch:  38\n",
      "480/480 - 33s - loss: 0.2329 - dense_4_loss: 0.1084 - dense_1_loss: 0.2477 - dense_3_loss: 0.1676 - dense_4_accuracy: 0.9603 - dense_1_accuracy: 0.9100 - dense_3_accuracy: 0.9361 - val_loss: 0.3780 - val_dense_4_loss: 0.1851 - val_dense_1_loss: 0.3869 - val_dense_3_loss: 0.2560 - val_dense_4_accuracy: 0.9281 - val_dense_1_accuracy: 0.8536 - val_dense_3_accuracy: 0.9042 - 33s/epoch - 69ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/38GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/38GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  38\n",
      "added to csv\n",
      "\n",
      "Epoch:  39\n",
      "480/480 - 33s - loss: 0.2337 - dense_4_loss: 0.1064 - dense_1_loss: 0.2601 - dense_3_loss: 0.1643 - dense_4_accuracy: 0.9602 - dense_1_accuracy: 0.9027 - dense_3_accuracy: 0.9370 - val_loss: 0.7800 - val_dense_4_loss: 0.4757 - val_dense_1_loss: 0.5010 - val_dense_3_loss: 0.5134 - val_dense_4_accuracy: 0.8521 - val_dense_1_accuracy: 0.8203 - val_dense_3_accuracy: 0.8344 - 33s/epoch - 69ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/39GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/39GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  39\n",
      "added to csv\n",
      "\n",
      "Epoch:  40\n",
      "480/480 - 33s - loss: 0.2723 - dense_4_loss: 0.1383 - dense_1_loss: 0.2595 - dense_3_loss: 0.1871 - dense_4_accuracy: 0.9493 - dense_1_accuracy: 0.9016 - dense_3_accuracy: 0.9316 - val_loss: 0.4487 - val_dense_4_loss: 0.2242 - val_dense_1_loss: 0.4462 - val_dense_3_loss: 0.3021 - val_dense_4_accuracy: 0.9250 - val_dense_1_accuracy: 0.8370 - val_dense_3_accuracy: 0.8906 - 33s/epoch - 68ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/40GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/40GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  40\n",
      "added to csv\n",
      "\n",
      "Epoch:  41\n",
      "480/480 - 33s - loss: 0.2016 - dense_4_loss: 0.0876 - dense_1_loss: 0.2326 - dense_3_loss: 0.1474 - dense_4_accuracy: 0.9667 - dense_1_accuracy: 0.9152 - dense_3_accuracy: 0.9439 - val_loss: 1.0026 - val_dense_4_loss: 0.5616 - val_dense_1_loss: 0.7755 - val_dense_3_loss: 0.6945 - val_dense_4_accuracy: 0.8365 - val_dense_1_accuracy: 0.7464 - val_dense_3_accuracy: 0.7937 - 33s/epoch - 68ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/41GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/41GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  41\n",
      "added to csv\n",
      "\n",
      "Epoch:  42\n",
      "480/480 - 33s - loss: 0.2233 - dense_4_loss: 0.1101 - dense_1_loss: 0.2240 - dense_3_loss: 0.1534 - dense_4_accuracy: 0.9656 - dense_1_accuracy: 0.9255 - dense_3_accuracy: 0.9525 - val_loss: 0.3316 - val_dense_4_loss: 0.1631 - val_dense_1_loss: 0.3405 - val_dense_3_loss: 0.2213 - val_dense_4_accuracy: 0.9396 - val_dense_1_accuracy: 0.8755 - val_dense_3_accuracy: 0.9172 - 33s/epoch - 70ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/42GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/42GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  42\n",
      "added to csv\n",
      "\n",
      "Epoch:  43\n",
      "480/480 - 33s - loss: 0.1086 - dense_4_loss: 0.0399 - dense_1_loss: 0.1479 - dense_3_loss: 0.0810 - dense_4_accuracy: 0.9887 - dense_1_accuracy: 0.9504 - dense_3_accuracy: 0.9746 - val_loss: 0.3194 - val_dense_4_loss: 0.1555 - val_dense_1_loss: 0.3307 - val_dense_3_loss: 0.2156 - val_dense_4_accuracy: 0.9406 - val_dense_1_accuracy: 0.8786 - val_dense_3_accuracy: 0.9161 - 33s/epoch - 68ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/43GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/43GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  43\n",
      "added to csv\n",
      "\n",
      "Epoch:  44\n",
      "480/480 - 33s - loss: 0.0991 - dense_4_loss: 0.0359 - dense_1_loss: 0.1384 - dense_3_loss: 0.0724 - dense_4_accuracy: 0.9905 - dense_1_accuracy: 0.9594 - dense_3_accuracy: 0.9777 - val_loss: 0.3134 - val_dense_4_loss: 0.1516 - val_dense_1_loss: 0.3270 - val_dense_3_loss: 0.2124 - val_dense_4_accuracy: 0.9427 - val_dense_1_accuracy: 0.8807 - val_dense_3_accuracy: 0.9156 - 33s/epoch - 68ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/44GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/44GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  44\n",
      "added to csv\n",
      "\n",
      "Epoch:  45\n",
      "480/480 - 34s - loss: 0.0946 - dense_4_loss: 0.0334 - dense_1_loss: 0.1343 - dense_3_loss: 0.0700 - dense_4_accuracy: 0.9902 - dense_1_accuracy: 0.9581 - dense_3_accuracy: 0.9790 - val_loss: 0.3089 - val_dense_4_loss: 0.1488 - val_dense_1_loss: 0.3241 - val_dense_3_loss: 0.2095 - val_dense_4_accuracy: 0.9417 - val_dense_1_accuracy: 0.8802 - val_dense_3_accuracy: 0.9177 - 34s/epoch - 70ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/45GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/45GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  45\n",
      "added to csv\n",
      "\n",
      "Epoch:  46\n",
      "480/480 - 33s - loss: 0.0908 - dense_4_loss: 0.0307 - dense_1_loss: 0.1333 - dense_3_loss: 0.0670 - dense_4_accuracy: 0.9911 - dense_1_accuracy: 0.9586 - dense_3_accuracy: 0.9811 - val_loss: 0.3049 - val_dense_4_loss: 0.1465 - val_dense_1_loss: 0.3213 - val_dense_3_loss: 0.2068 - val_dense_4_accuracy: 0.9417 - val_dense_1_accuracy: 0.8802 - val_dense_3_accuracy: 0.9182 - 33s/epoch - 68ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/46GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/46GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  46\n",
      "added to csv\n",
      "\n",
      "Epoch:  47\n",
      "480/480 - 33s - loss: 0.0885 - dense_4_loss: 0.0296 - dense_1_loss: 0.1322 - dense_3_loss: 0.0643 - dense_4_accuracy: 0.9918 - dense_1_accuracy: 0.9570 - dense_3_accuracy: 0.9816 - val_loss: 0.3021 - val_dense_4_loss: 0.1447 - val_dense_1_loss: 0.3191 - val_dense_3_loss: 0.2054 - val_dense_4_accuracy: 0.9432 - val_dense_1_accuracy: 0.8792 - val_dense_3_accuracy: 0.9187 - 33s/epoch - 69ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/47GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/47GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  47\n",
      "added to csv\n",
      "\n",
      "Epoch:  48\n",
      "480/480 - 34s - loss: 0.0827 - dense_4_loss: 0.0274 - dense_1_loss: 0.1244 - dense_3_loss: 0.0601 - dense_4_accuracy: 0.9921 - dense_1_accuracy: 0.9603 - dense_3_accuracy: 0.9835 - val_loss: 0.3006 - val_dense_4_loss: 0.1439 - val_dense_1_loss: 0.3185 - val_dense_3_loss: 0.2040 - val_dense_4_accuracy: 0.9427 - val_dense_1_accuracy: 0.8781 - val_dense_3_accuracy: 0.9187 - 34s/epoch - 70ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/48GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/48GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  48\n",
      "added to csv\n",
      "\n",
      "Epoch:  49\n",
      "480/480 - 34s - loss: 0.0800 - dense_4_loss: 0.0254 - dense_1_loss: 0.1220 - dense_3_loss: 0.0601 - dense_4_accuracy: 0.9930 - dense_1_accuracy: 0.9620 - dense_3_accuracy: 0.9814 - val_loss: 0.2984 - val_dense_4_loss: 0.1426 - val_dense_1_loss: 0.3174 - val_dense_3_loss: 0.2019 - val_dense_4_accuracy: 0.9438 - val_dense_1_accuracy: 0.8792 - val_dense_3_accuracy: 0.9193 - 34s/epoch - 70ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/49GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/49GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  49\n",
      "added to csv\n",
      "\n",
      "Epoch:  50\n",
      "480/480 - 33s - loss: 0.0764 - dense_4_loss: 0.0245 - dense_1_loss: 0.1164 - dense_3_loss: 0.0565 - dense_4_accuracy: 0.9932 - dense_1_accuracy: 0.9635 - dense_3_accuracy: 0.9829 - val_loss: 0.2986 - val_dense_4_loss: 0.1435 - val_dense_1_loss: 0.3154 - val_dense_3_loss: 0.2014 - val_dense_4_accuracy: 0.9427 - val_dense_1_accuracy: 0.8792 - val_dense_3_accuracy: 0.9208 - 33s/epoch - 70ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/50GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/50GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  50\n",
      "added to csv\n",
      "\n",
      "Epoch:  51\n",
      "480/480 - 33s - loss: 0.0753 - dense_4_loss: 0.0230 - dense_1_loss: 0.1187 - dense_3_loss: 0.0556 - dense_4_accuracy: 0.9928 - dense_1_accuracy: 0.9641 - dense_3_accuracy: 0.9835 - val_loss: 0.2975 - val_dense_4_loss: 0.1433 - val_dense_1_loss: 0.3145 - val_dense_3_loss: 0.1996 - val_dense_4_accuracy: 0.9417 - val_dense_1_accuracy: 0.8807 - val_dense_3_accuracy: 0.9214 - 33s/epoch - 69ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/51GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/51GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  51\n",
      "added to csv\n",
      "\n",
      "Epoch:  52\n",
      "480/480 - 33s - loss: 0.0721 - dense_4_loss: 0.0206 - dense_1_loss: 0.1185 - dense_3_loss: 0.0533 - dense_4_accuracy: 0.9943 - dense_1_accuracy: 0.9641 - dense_3_accuracy: 0.9831 - val_loss: 0.2982 - val_dense_4_loss: 0.1432 - val_dense_1_loss: 0.3155 - val_dense_3_loss: 0.2011 - val_dense_4_accuracy: 0.9427 - val_dense_1_accuracy: 0.8802 - val_dense_3_accuracy: 0.9187 - 33s/epoch - 69ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/52GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/52GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  52\n",
      "added to csv\n",
      "\n",
      "Epoch:  53\n",
      "480/480 - 34s - loss: 0.0697 - dense_4_loss: 0.0200 - dense_1_loss: 0.1137 - dense_3_loss: 0.0520 - dense_4_accuracy: 0.9944 - dense_1_accuracy: 0.9645 - dense_3_accuracy: 0.9827 - val_loss: 0.2970 - val_dense_4_loss: 0.1430 - val_dense_1_loss: 0.3137 - val_dense_3_loss: 0.1998 - val_dense_4_accuracy: 0.9427 - val_dense_1_accuracy: 0.8813 - val_dense_3_accuracy: 0.9219 - 34s/epoch - 70ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/53GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/53GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  53\n",
      "added to csv\n",
      "\n",
      "Epoch:  54\n",
      "480/480 - 34s - loss: 0.0685 - dense_4_loss: 0.0196 - dense_1_loss: 0.1129 - dense_3_loss: 0.0500 - dense_4_accuracy: 0.9944 - dense_1_accuracy: 0.9647 - dense_3_accuracy: 0.9833 - val_loss: 0.2968 - val_dense_4_loss: 0.1428 - val_dense_1_loss: 0.3137 - val_dense_3_loss: 0.1996 - val_dense_4_accuracy: 0.9427 - val_dense_1_accuracy: 0.8813 - val_dense_3_accuracy: 0.9224 - 34s/epoch - 72ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/54GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/54GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  54\n",
      "added to csv\n",
      "\n",
      "Epoch:  55\n",
      "480/480 - 33s - loss: 0.0648 - dense_4_loss: 0.0172 - dense_1_loss: 0.1105 - dense_3_loss: 0.0479 - dense_4_accuracy: 0.9952 - dense_1_accuracy: 0.9646 - dense_3_accuracy: 0.9854 - val_loss: 0.2981 - val_dense_4_loss: 0.1438 - val_dense_1_loss: 0.3143 - val_dense_3_loss: 0.2004 - val_dense_4_accuracy: 0.9427 - val_dense_1_accuracy: 0.8828 - val_dense_3_accuracy: 0.9219 - 33s/epoch - 68ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/55GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/55GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  55\n",
      "added to csv\n",
      "\n",
      "Epoch:  56\n",
      "480/480 - 34s - loss: 0.0635 - dense_4_loss: 0.0166 - dense_1_loss: 0.1095 - dense_3_loss: 0.0469 - dense_4_accuracy: 0.9962 - dense_1_accuracy: 0.9665 - dense_3_accuracy: 0.9875 - val_loss: 0.2959 - val_dense_4_loss: 0.1428 - val_dense_1_loss: 0.3122 - val_dense_3_loss: 0.1981 - val_dense_4_accuracy: 0.9453 - val_dense_1_accuracy: 0.8839 - val_dense_3_accuracy: 0.9240 - 34s/epoch - 70ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/56GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/56GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  56\n",
      "added to csv\n",
      "\n",
      "Epoch:  57\n",
      "480/480 - 33s - loss: 0.0617 - dense_4_loss: 0.0164 - dense_1_loss: 0.1049 - dense_3_loss: 0.0463 - dense_4_accuracy: 0.9954 - dense_1_accuracy: 0.9665 - dense_3_accuracy: 0.9849 - val_loss: 0.2955 - val_dense_4_loss: 0.1424 - val_dense_1_loss: 0.3123 - val_dense_3_loss: 0.1978 - val_dense_4_accuracy: 0.9443 - val_dense_1_accuracy: 0.8818 - val_dense_3_accuracy: 0.9240 - 33s/epoch - 68ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/57GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/57GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  57\n",
      "added to csv\n",
      "\n",
      "Epoch:  58\n",
      "480/480 - 33s - loss: 0.0600 - dense_4_loss: 0.0154 - dense_1_loss: 0.1047 - dense_3_loss: 0.0440 - dense_4_accuracy: 0.9958 - dense_1_accuracy: 0.9668 - dense_3_accuracy: 0.9861 - val_loss: 0.2981 - val_dense_4_loss: 0.1438 - val_dense_1_loss: 0.3132 - val_dense_3_loss: 0.2008 - val_dense_4_accuracy: 0.9443 - val_dense_1_accuracy: 0.8854 - val_dense_3_accuracy: 0.9240 - 33s/epoch - 69ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/58GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/58GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  58\n",
      "added to csv\n",
      "\n",
      "Epoch:  59\n",
      "480/480 - 33s - loss: 0.0579 - dense_4_loss: 0.0149 - dense_1_loss: 0.1003 - dense_3_loss: 0.0429 - dense_4_accuracy: 0.9962 - dense_1_accuracy: 0.9681 - dense_3_accuracy: 0.9859 - val_loss: 0.2974 - val_dense_4_loss: 0.1439 - val_dense_1_loss: 0.3121 - val_dense_3_loss: 0.1995 - val_dense_4_accuracy: 0.9443 - val_dense_1_accuracy: 0.8849 - val_dense_3_accuracy: 0.9255 - 33s/epoch - 69ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/59GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/59GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  59\n",
      "added to csv\n",
      "\n",
      "Epoch:  60\n",
      "480/480 - 33s - loss: 0.0585 - dense_4_loss: 0.0143 - dense_1_loss: 0.1047 - dense_3_loss: 0.0424 - dense_4_accuracy: 0.9958 - dense_1_accuracy: 0.9660 - dense_3_accuracy: 0.9865 - val_loss: 0.2980 - val_dense_4_loss: 0.1447 - val_dense_1_loss: 0.3113 - val_dense_3_loss: 0.1997 - val_dense_4_accuracy: 0.9453 - val_dense_1_accuracy: 0.8865 - val_dense_3_accuracy: 0.9250 - 33s/epoch - 69ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/60GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/60GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  60\n",
      "added to csv\n",
      "\n",
      "Epoch:  61\n",
      "480/480 - 33s - loss: 0.0553 - dense_4_loss: 0.0132 - dense_1_loss: 0.0989 - dense_3_loss: 0.0415 - dense_4_accuracy: 0.9970 - dense_1_accuracy: 0.9682 - dense_3_accuracy: 0.9858 - val_loss: 0.2985 - val_dense_4_loss: 0.1448 - val_dense_1_loss: 0.3122 - val_dense_3_loss: 0.2000 - val_dense_4_accuracy: 0.9458 - val_dense_1_accuracy: 0.8859 - val_dense_3_accuracy: 0.9260 - 33s/epoch - 69ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/61GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/61GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  61\n",
      "added to csv\n",
      "\n",
      "Epoch:  62\n",
      "480/480 - 34s - loss: 0.0547 - dense_4_loss: 0.0134 - dense_1_loss: 0.0973 - dense_3_loss: 0.0404 - dense_4_accuracy: 0.9970 - dense_1_accuracy: 0.9701 - dense_3_accuracy: 0.9880 - val_loss: 0.2991 - val_dense_4_loss: 0.1453 - val_dense_1_loss: 0.3124 - val_dense_3_loss: 0.2002 - val_dense_4_accuracy: 0.9458 - val_dense_1_accuracy: 0.8865 - val_dense_3_accuracy: 0.9255 - 34s/epoch - 70ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/62GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/62GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  62\n",
      "added to csv\n",
      "\n",
      "Epoch:  63\n",
      "480/480 - 33s - loss: 0.0530 - dense_4_loss: 0.0120 - dense_1_loss: 0.0977 - dense_3_loss: 0.0390 - dense_4_accuracy: 0.9971 - dense_1_accuracy: 0.9703 - dense_3_accuracy: 0.9878 - val_loss: 0.2988 - val_dense_4_loss: 0.1451 - val_dense_1_loss: 0.3126 - val_dense_3_loss: 0.1997 - val_dense_4_accuracy: 0.9448 - val_dense_1_accuracy: 0.8865 - val_dense_3_accuracy: 0.9271 - 33s/epoch - 70ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/63GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/63GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  63\n",
      "added to csv\n",
      "\n",
      "Epoch:  64\n",
      "480/480 - 33s - loss: 0.0504 - dense_4_loss: 0.0112 - dense_1_loss: 0.0924 - dense_3_loss: 0.0383 - dense_4_accuracy: 0.9973 - dense_1_accuracy: 0.9710 - dense_3_accuracy: 0.9891 - val_loss: 0.3023 - val_dense_4_loss: 0.1476 - val_dense_1_loss: 0.3136 - val_dense_3_loss: 0.2021 - val_dense_4_accuracy: 0.9464 - val_dense_1_accuracy: 0.8849 - val_dense_3_accuracy: 0.9255 - 33s/epoch - 69ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/64GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/64GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  64\n",
      "added to csv\n",
      "\n",
      "Epoch:  65\n",
      "480/480 - 33s - loss: 0.0498 - dense_4_loss: 0.0111 - dense_1_loss: 0.0923 - dense_3_loss: 0.0368 - dense_4_accuracy: 0.9982 - dense_1_accuracy: 0.9717 - dense_3_accuracy: 0.9882 - val_loss: 0.3017 - val_dense_4_loss: 0.1467 - val_dense_1_loss: 0.3140 - val_dense_3_loss: 0.2026 - val_dense_4_accuracy: 0.9453 - val_dense_1_accuracy: 0.8849 - val_dense_3_accuracy: 0.9266 - 33s/epoch - 68ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/65GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/65GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  65\n",
      "added to csv\n",
      "\n",
      "Epoch:  66\n",
      "480/480 - 34s - loss: 0.0502 - dense_4_loss: 0.0108 - dense_1_loss: 0.0948 - dense_3_loss: 0.0365 - dense_4_accuracy: 0.9978 - dense_1_accuracy: 0.9710 - dense_3_accuracy: 0.9891 - val_loss: 0.3001 - val_dense_4_loss: 0.1457 - val_dense_1_loss: 0.3130 - val_dense_3_loss: 0.2017 - val_dense_4_accuracy: 0.9469 - val_dense_1_accuracy: 0.8854 - val_dense_3_accuracy: 0.9266 - 34s/epoch - 70ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/66GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/66GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  66\n",
      "added to csv\n",
      "\n",
      "Epoch:  67\n",
      "480/480 - 33s - loss: 0.0493 - dense_4_loss: 0.0099 - dense_1_loss: 0.0958 - dense_3_loss: 0.0355 - dense_4_accuracy: 0.9986 - dense_1_accuracy: 0.9678 - dense_3_accuracy: 0.9893 - val_loss: 0.3040 - val_dense_4_loss: 0.1490 - val_dense_1_loss: 0.3133 - val_dense_3_loss: 0.2032 - val_dense_4_accuracy: 0.9458 - val_dense_1_accuracy: 0.8854 - val_dense_3_accuracy: 0.9255 - 33s/epoch - 68ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/67GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/67GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  67\n",
      "added to csv\n",
      "\n",
      "Epoch:  68\n",
      "480/480 - 33s - loss: 0.0482 - dense_4_loss: 0.0094 - dense_1_loss: 0.0943 - dense_3_loss: 0.0350 - dense_4_accuracy: 0.9983 - dense_1_accuracy: 0.9710 - dense_3_accuracy: 0.9895 - val_loss: 0.3028 - val_dense_4_loss: 0.1478 - val_dense_1_loss: 0.3146 - val_dense_3_loss: 0.2022 - val_dense_4_accuracy: 0.9474 - val_dense_1_accuracy: 0.8854 - val_dense_3_accuracy: 0.9276 - 33s/epoch - 69ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/68GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/68GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  68\n",
      "added to csv\n",
      "\n",
      "Epoch:  69\n",
      "480/480 - 33s - loss: 0.0478 - dense_4_loss: 0.0095 - dense_1_loss: 0.0924 - dense_3_loss: 0.0352 - dense_4_accuracy: 0.9982 - dense_1_accuracy: 0.9695 - dense_3_accuracy: 0.9889 - val_loss: 0.3055 - val_dense_4_loss: 0.1493 - val_dense_1_loss: 0.3141 - val_dense_3_loss: 0.2063 - val_dense_4_accuracy: 0.9464 - val_dense_1_accuracy: 0.8875 - val_dense_3_accuracy: 0.9281 - 33s/epoch - 69ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/69GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/69GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  69\n",
      "added to csv\n",
      "\n",
      "Epoch:  70\n",
      "480/480 - 33s - loss: 0.0454 - dense_4_loss: 0.0086 - dense_1_loss: 0.0884 - dense_3_loss: 0.0344 - dense_4_accuracy: 0.9983 - dense_1_accuracy: 0.9716 - dense_3_accuracy: 0.9885 - val_loss: 0.3033 - val_dense_4_loss: 0.1483 - val_dense_1_loss: 0.3134 - val_dense_3_loss: 0.2033 - val_dense_4_accuracy: 0.9458 - val_dense_1_accuracy: 0.8875 - val_dense_3_accuracy: 0.9276 - 33s/epoch - 68ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/70GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/70GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  70\n",
      "added to csv\n",
      "\n",
      "Epoch:  71\n",
      "480/480 - 34s - loss: 0.0454 - dense_4_loss: 0.0088 - dense_1_loss: 0.0890 - dense_3_loss: 0.0331 - dense_4_accuracy: 0.9983 - dense_1_accuracy: 0.9699 - dense_3_accuracy: 0.9908 - val_loss: 0.3041 - val_dense_4_loss: 0.1487 - val_dense_1_loss: 0.3138 - val_dense_3_loss: 0.2039 - val_dense_4_accuracy: 0.9474 - val_dense_1_accuracy: 0.8875 - val_dense_3_accuracy: 0.9260 - 34s/epoch - 70ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/71GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/71GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  71\n",
      "added to csv\n",
      "\n",
      "Epoch:  72\n",
      "480/480 - 33s - loss: 0.0440 - dense_4_loss: 0.0077 - dense_1_loss: 0.0891 - dense_3_loss: 0.0320 - dense_4_accuracy: 0.9988 - dense_1_accuracy: 0.9691 - dense_3_accuracy: 0.9906 - val_loss: 0.3091 - val_dense_4_loss: 0.1527 - val_dense_1_loss: 0.3160 - val_dense_3_loss: 0.2052 - val_dense_4_accuracy: 0.9464 - val_dense_1_accuracy: 0.8870 - val_dense_3_accuracy: 0.9276 - 33s/epoch - 69ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/72GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/72GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  72\n",
      "added to csv\n",
      "\n",
      "Epoch:  73\n",
      "480/480 - 33s - loss: 0.0450 - dense_4_loss: 0.0078 - dense_1_loss: 0.0904 - dense_3_loss: 0.0333 - dense_4_accuracy: 0.9991 - dense_1_accuracy: 0.9688 - dense_3_accuracy: 0.9896 - val_loss: 0.3043 - val_dense_4_loss: 0.1490 - val_dense_1_loss: 0.3127 - val_dense_3_loss: 0.2046 - val_dense_4_accuracy: 0.9464 - val_dense_1_accuracy: 0.8917 - val_dense_3_accuracy: 0.9276 - 33s/epoch - 68ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/73GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/73GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  73\n",
      "added to csv\n",
      "\n",
      "Epoch:  74\n",
      "480/480 - 33s - loss: 0.0434 - dense_4_loss: 0.0073 - dense_1_loss: 0.0878 - dense_3_loss: 0.0325 - dense_4_accuracy: 0.9988 - dense_1_accuracy: 0.9695 - dense_3_accuracy: 0.9905 - val_loss: 0.3057 - val_dense_4_loss: 0.1502 - val_dense_1_loss: 0.3137 - val_dense_3_loss: 0.2046 - val_dense_4_accuracy: 0.9484 - val_dense_1_accuracy: 0.8880 - val_dense_3_accuracy: 0.9271 - 33s/epoch - 68ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/74GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/74GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  74\n",
      "added to csv\n",
      "\n",
      "Epoch:  75\n",
      "480/480 - 33s - loss: 0.0420 - dense_4_loss: 0.0063 - dense_1_loss: 0.0883 - dense_3_loss: 0.0307 - dense_4_accuracy: 0.9997 - dense_1_accuracy: 0.9729 - dense_3_accuracy: 0.9906 - val_loss: 0.3086 - val_dense_4_loss: 0.1519 - val_dense_1_loss: 0.3153 - val_dense_3_loss: 0.2071 - val_dense_4_accuracy: 0.9479 - val_dense_1_accuracy: 0.8875 - val_dense_3_accuracy: 0.9245 - 33s/epoch - 68ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/75GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/75GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  75\n",
      "added to csv\n",
      "\n",
      "Epoch:  76\n",
      "480/480 - 33s - loss: 0.0424 - dense_4_loss: 0.0065 - dense_1_loss: 0.0882 - dense_3_loss: 0.0314 - dense_4_accuracy: 0.9993 - dense_1_accuracy: 0.9711 - dense_3_accuracy: 0.9897 - val_loss: 0.3058 - val_dense_4_loss: 0.1493 - val_dense_1_loss: 0.3153 - val_dense_3_loss: 0.2062 - val_dense_4_accuracy: 0.9474 - val_dense_1_accuracy: 0.8891 - val_dense_3_accuracy: 0.9286 - 33s/epoch - 69ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/76GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/76GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  76\n",
      "added to csv\n",
      "\n",
      "Epoch:  77\n",
      "480/480 - 33s - loss: 0.0410 - dense_4_loss: 0.0062 - dense_1_loss: 0.0862 - dense_3_loss: 0.0298 - dense_4_accuracy: 0.9993 - dense_1_accuracy: 0.9724 - dense_3_accuracy: 0.9902 - val_loss: 0.3098 - val_dense_4_loss: 0.1521 - val_dense_1_loss: 0.3176 - val_dense_3_loss: 0.2083 - val_dense_4_accuracy: 0.9464 - val_dense_1_accuracy: 0.8880 - val_dense_3_accuracy: 0.9276 - 33s/epoch - 69ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/77GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/77GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  77\n",
      "added to csv\n",
      "\n",
      "Epoch:  78\n",
      "480/480 - 33s - loss: 0.0413 - dense_4_loss: 0.0062 - dense_1_loss: 0.0849 - dense_3_loss: 0.0320 - dense_4_accuracy: 0.9993 - dense_1_accuracy: 0.9707 - dense_3_accuracy: 0.9891 - val_loss: 0.3073 - val_dense_4_loss: 0.1509 - val_dense_1_loss: 0.3147 - val_dense_3_loss: 0.2066 - val_dense_4_accuracy: 0.9484 - val_dense_1_accuracy: 0.8901 - val_dense_3_accuracy: 0.9286 - 33s/epoch - 69ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/78GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/78GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  78\n",
      "added to csv\n",
      "\n",
      "Epoch:  79\n",
      "480/480 - 33s - loss: 0.0404 - dense_4_loss: 0.0058 - dense_1_loss: 0.0852 - dense_3_loss: 0.0301 - dense_4_accuracy: 0.9995 - dense_1_accuracy: 0.9693 - dense_3_accuracy: 0.9909 - val_loss: 0.3096 - val_dense_4_loss: 0.1527 - val_dense_1_loss: 0.3155 - val_dense_3_loss: 0.2074 - val_dense_4_accuracy: 0.9474 - val_dense_1_accuracy: 0.8880 - val_dense_3_accuracy: 0.9266 - 33s/epoch - 69ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/79GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/79GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  79\n",
      "added to csv\n",
      "\n",
      "Epoch:  80\n",
      "480/480 - 33s - loss: 0.0393 - dense_4_loss: 0.0055 - dense_1_loss: 0.0833 - dense_3_loss: 0.0296 - dense_4_accuracy: 0.9996 - dense_1_accuracy: 0.9710 - dense_3_accuracy: 0.9910 - val_loss: 0.3147 - val_dense_4_loss: 0.1563 - val_dense_1_loss: 0.3175 - val_dense_3_loss: 0.2105 - val_dense_4_accuracy: 0.9479 - val_dense_1_accuracy: 0.8880 - val_dense_3_accuracy: 0.9307 - 33s/epoch - 68ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/80GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/80GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  80\n",
      "added to csv\n",
      "\n",
      "Epoch:  81\n",
      "480/480 - 32s - loss: 0.0387 - dense_4_loss: 0.0053 - dense_1_loss: 0.0829 - dense_3_loss: 0.0284 - dense_4_accuracy: 0.9997 - dense_1_accuracy: 0.9704 - dense_3_accuracy: 0.9906 - val_loss: 0.3121 - val_dense_4_loss: 0.1545 - val_dense_1_loss: 0.3175 - val_dense_3_loss: 0.2080 - val_dense_4_accuracy: 0.9490 - val_dense_1_accuracy: 0.8870 - val_dense_3_accuracy: 0.9312 - 32s/epoch - 68ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/81GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/81GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  81\n",
      "added to csv\n",
      "\n",
      "Epoch:  82\n",
      "480/480 - 33s - loss: 0.0374 - dense_4_loss: 0.0052 - dense_1_loss: 0.0782 - dense_3_loss: 0.0290 - dense_4_accuracy: 0.9991 - dense_1_accuracy: 0.9743 - dense_3_accuracy: 0.9911 - val_loss: 0.3158 - val_dense_4_loss: 0.1572 - val_dense_1_loss: 0.3177 - val_dense_3_loss: 0.2108 - val_dense_4_accuracy: 0.9474 - val_dense_1_accuracy: 0.8870 - val_dense_3_accuracy: 0.9286 - 33s/epoch - 69ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/82GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/82GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  82\n",
      "added to csv\n",
      "\n",
      "Epoch:  83\n",
      "480/480 - 33s - loss: 0.0373 - dense_4_loss: 0.0045 - dense_1_loss: 0.0825 - dense_3_loss: 0.0267 - dense_4_accuracy: 0.9995 - dense_1_accuracy: 0.9711 - dense_3_accuracy: 0.9921 - val_loss: 0.3130 - val_dense_4_loss: 0.1538 - val_dense_1_loss: 0.3200 - val_dense_3_loss: 0.2107 - val_dense_4_accuracy: 0.9479 - val_dense_1_accuracy: 0.8865 - val_dense_3_accuracy: 0.9266 - 33s/epoch - 68ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/83GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/83GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  83\n",
      "added to csv\n",
      "\n",
      "Epoch:  84\n",
      "480/480 - 33s - loss: 0.0366 - dense_4_loss: 0.0046 - dense_1_loss: 0.0794 - dense_3_loss: 0.0271 - dense_4_accuracy: 1.0000 - dense_1_accuracy: 0.9719 - dense_3_accuracy: 0.9924 - val_loss: 0.3119 - val_dense_4_loss: 0.1531 - val_dense_1_loss: 0.3195 - val_dense_3_loss: 0.2099 - val_dense_4_accuracy: 0.9484 - val_dense_1_accuracy: 0.8865 - val_dense_3_accuracy: 0.9271 - 33s/epoch - 68ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/84GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/84GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  84\n",
      "added to csv\n",
      "\n",
      "Epoch:  85\n",
      "480/480 - 33s - loss: 0.0375 - dense_4_loss: 0.0052 - dense_1_loss: 0.0804 - dense_3_loss: 0.0274 - dense_4_accuracy: 0.9990 - dense_1_accuracy: 0.9725 - dense_3_accuracy: 0.9917 - val_loss: 0.3110 - val_dense_4_loss: 0.1525 - val_dense_1_loss: 0.3192 - val_dense_3_loss: 0.2093 - val_dense_4_accuracy: 0.9484 - val_dense_1_accuracy: 0.8870 - val_dense_3_accuracy: 0.9271 - 33s/epoch - 68ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/85GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/85GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  85\n",
      "added to csv\n",
      "\n",
      "Epoch:  86\n",
      "480/480 - 33s - loss: 0.0365 - dense_4_loss: 0.0044 - dense_1_loss: 0.0802 - dense_3_loss: 0.0267 - dense_4_accuracy: 0.9999 - dense_1_accuracy: 0.9738 - dense_3_accuracy: 0.9911 - val_loss: 0.3105 - val_dense_4_loss: 0.1522 - val_dense_1_loss: 0.3191 - val_dense_3_loss: 0.2089 - val_dense_4_accuracy: 0.9484 - val_dense_1_accuracy: 0.8875 - val_dense_3_accuracy: 0.9271 - 33s/epoch - 68ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/86GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/86GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  86\n",
      "added to csv\n",
      "\n",
      "Epoch:  87\n",
      "480/480 - 33s - loss: 0.0368 - dense_4_loss: 0.0045 - dense_1_loss: 0.0813 - dense_3_loss: 0.0264 - dense_4_accuracy: 0.9995 - dense_1_accuracy: 0.9719 - dense_3_accuracy: 0.9914 - val_loss: 0.3100 - val_dense_4_loss: 0.1518 - val_dense_1_loss: 0.3188 - val_dense_3_loss: 0.2085 - val_dense_4_accuracy: 0.9484 - val_dense_1_accuracy: 0.8885 - val_dense_3_accuracy: 0.9271 - 33s/epoch - 69ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/87GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/87GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  87\n",
      "added to csv\n",
      "\n",
      "Epoch:  88\n",
      "480/480 - 33s - loss: 0.0350 - dense_4_loss: 0.0042 - dense_1_loss: 0.0758 - dense_3_loss: 0.0269 - dense_4_accuracy: 0.9999 - dense_1_accuracy: 0.9763 - dense_3_accuracy: 0.9914 - val_loss: 0.3097 - val_dense_4_loss: 0.1517 - val_dense_1_loss: 0.3186 - val_dense_3_loss: 0.2082 - val_dense_4_accuracy: 0.9490 - val_dense_1_accuracy: 0.8891 - val_dense_3_accuracy: 0.9266 - 33s/epoch - 70ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/88GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/88GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  88\n",
      "added to csv\n",
      "\n",
      "Epoch:  89\n",
      "480/480 - 33s - loss: 0.0361 - dense_4_loss: 0.0043 - dense_1_loss: 0.0803 - dense_3_loss: 0.0258 - dense_4_accuracy: 0.9997 - dense_1_accuracy: 0.9737 - dense_3_accuracy: 0.9921 - val_loss: 0.3094 - val_dense_4_loss: 0.1515 - val_dense_1_loss: 0.3185 - val_dense_3_loss: 0.2080 - val_dense_4_accuracy: 0.9495 - val_dense_1_accuracy: 0.8891 - val_dense_3_accuracy: 0.9271 - 33s/epoch - 68ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/89GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/89GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  89\n",
      "added to csv\n",
      "\n",
      "Epoch:  90\n",
      "480/480 - 32s - loss: 0.0369 - dense_4_loss: 0.0047 - dense_1_loss: 0.0803 - dense_3_loss: 0.0271 - dense_4_accuracy: 0.9993 - dense_1_accuracy: 0.9743 - dense_3_accuracy: 0.9921 - val_loss: 0.3091 - val_dense_4_loss: 0.1513 - val_dense_1_loss: 0.3183 - val_dense_3_loss: 0.2077 - val_dense_4_accuracy: 0.9490 - val_dense_1_accuracy: 0.8891 - val_dense_3_accuracy: 0.9266 - 32s/epoch - 68ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/90GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/90GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  90\n",
      "added to csv\n",
      "\n",
      "Epoch:  91\n",
      "480/480 - 32s - loss: 0.0364 - dense_4_loss: 0.0044 - dense_1_loss: 0.0796 - dense_3_loss: 0.0272 - dense_4_accuracy: 0.9996 - dense_1_accuracy: 0.9717 - dense_3_accuracy: 0.9911 - val_loss: 0.3089 - val_dense_4_loss: 0.1512 - val_dense_1_loss: 0.3181 - val_dense_3_loss: 0.2075 - val_dense_4_accuracy: 0.9490 - val_dense_1_accuracy: 0.8891 - val_dense_3_accuracy: 0.9271 - 32s/epoch - 68ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/91GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/91GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  91\n",
      "added to csv\n",
      "\n",
      "Epoch:  92\n",
      "480/480 - 33s - loss: 0.0366 - dense_4_loss: 0.0046 - dense_1_loss: 0.0804 - dense_3_loss: 0.0265 - dense_4_accuracy: 0.9995 - dense_1_accuracy: 0.9730 - dense_3_accuracy: 0.9910 - val_loss: 0.3087 - val_dense_4_loss: 0.1511 - val_dense_1_loss: 0.3180 - val_dense_3_loss: 0.2073 - val_dense_4_accuracy: 0.9490 - val_dense_1_accuracy: 0.8896 - val_dense_3_accuracy: 0.9281 - 33s/epoch - 68ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/92GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/92GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  92\n",
      "added to csv\n",
      "\n",
      "Epoch:  93\n",
      "480/480 - 33s - loss: 0.0363 - dense_4_loss: 0.0046 - dense_1_loss: 0.0789 - dense_3_loss: 0.0268 - dense_4_accuracy: 0.9996 - dense_1_accuracy: 0.9757 - dense_3_accuracy: 0.9915 - val_loss: 0.3085 - val_dense_4_loss: 0.1510 - val_dense_1_loss: 0.3179 - val_dense_3_loss: 0.2073 - val_dense_4_accuracy: 0.9490 - val_dense_1_accuracy: 0.8896 - val_dense_3_accuracy: 0.9286 - 33s/epoch - 68ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/93GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/93GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  93\n",
      "added to csv\n",
      "\n",
      "Epoch:  94\n",
      "480/480 - 33s - loss: 0.0352 - dense_4_loss: 0.0046 - dense_1_loss: 0.0761 - dense_3_loss: 0.0259 - dense_4_accuracy: 0.9999 - dense_1_accuracy: 0.9760 - dense_3_accuracy: 0.9918 - val_loss: 0.3085 - val_dense_4_loss: 0.1510 - val_dense_1_loss: 0.3179 - val_dense_3_loss: 0.2072 - val_dense_4_accuracy: 0.9490 - val_dense_1_accuracy: 0.8901 - val_dense_3_accuracy: 0.9286 - 33s/epoch - 68ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/94GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/94GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  94\n",
      "added to csv\n",
      "\n",
      "Epoch:  95\n",
      "480/480 - 33s - loss: 0.0366 - dense_4_loss: 0.0040 - dense_1_loss: 0.0802 - dense_3_loss: 0.0283 - dense_4_accuracy: 0.9999 - dense_1_accuracy: 0.9738 - dense_3_accuracy: 0.9914 - val_loss: 0.3084 - val_dense_4_loss: 0.1509 - val_dense_1_loss: 0.3178 - val_dense_3_loss: 0.2071 - val_dense_4_accuracy: 0.9490 - val_dense_1_accuracy: 0.8901 - val_dense_3_accuracy: 0.9292 - 33s/epoch - 68ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/95GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/95GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  95\n",
      "added to csv\n",
      "\n",
      "Epoch:  96\n",
      "480/480 - 33s - loss: 0.0362 - dense_4_loss: 0.0042 - dense_1_loss: 0.0799 - dense_3_loss: 0.0267 - dense_4_accuracy: 0.9997 - dense_1_accuracy: 0.9755 - dense_3_accuracy: 0.9923 - val_loss: 0.3084 - val_dense_4_loss: 0.1510 - val_dense_1_loss: 0.3178 - val_dense_3_loss: 0.2071 - val_dense_4_accuracy: 0.9495 - val_dense_1_accuracy: 0.8901 - val_dense_3_accuracy: 0.9292 - 33s/epoch - 68ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/96GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/96GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  96\n",
      "added to csv\n",
      "\n",
      "Epoch:  97\n",
      "480/480 - 33s - loss: 0.0363 - dense_4_loss: 0.0045 - dense_1_loss: 0.0780 - dense_3_loss: 0.0279 - dense_4_accuracy: 0.9997 - dense_1_accuracy: 0.9759 - dense_3_accuracy: 0.9911 - val_loss: 0.3082 - val_dense_4_loss: 0.1508 - val_dense_1_loss: 0.3177 - val_dense_3_loss: 0.2069 - val_dense_4_accuracy: 0.9495 - val_dense_1_accuracy: 0.8901 - val_dense_3_accuracy: 0.9292 - 33s/epoch - 68ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/97GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/97GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  97\n",
      "added to csv\n",
      "\n",
      "Epoch:  98\n",
      "480/480 - 33s - loss: 0.0356 - dense_4_loss: 0.0046 - dense_1_loss: 0.0772 - dense_3_loss: 0.0264 - dense_4_accuracy: 0.9996 - dense_1_accuracy: 0.9755 - dense_3_accuracy: 0.9919 - val_loss: 0.3081 - val_dense_4_loss: 0.1508 - val_dense_1_loss: 0.3176 - val_dense_3_loss: 0.2068 - val_dense_4_accuracy: 0.9495 - val_dense_1_accuracy: 0.8901 - val_dense_3_accuracy: 0.9297 - 33s/epoch - 69ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/98GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/98GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  98\n",
      "added to csv\n",
      "\n",
      "Epoch:  99\n",
      "480/480 - 33s - loss: 0.0355 - dense_4_loss: 0.0044 - dense_1_loss: 0.0770 - dense_3_loss: 0.0267 - dense_4_accuracy: 0.9999 - dense_1_accuracy: 0.9736 - dense_3_accuracy: 0.9915 - val_loss: 0.3081 - val_dense_4_loss: 0.1508 - val_dense_1_loss: 0.3176 - val_dense_3_loss: 0.2068 - val_dense_4_accuracy: 0.9495 - val_dense_1_accuracy: 0.8901 - val_dense_3_accuracy: 0.9297 - 33s/epoch - 68ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/99GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/99GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  99\n",
      "added to csv\n",
      "\n",
      "Epoch:  100\n",
      "480/480 - 33s - loss: 0.0364 - dense_4_loss: 0.0043 - dense_1_loss: 0.0796 - dense_3_loss: 0.0273 - dense_4_accuracy: 0.9996 - dense_1_accuracy: 0.9736 - dense_3_accuracy: 0.9923 - val_loss: 0.3079 - val_dense_4_loss: 0.1507 - val_dense_1_loss: 0.3175 - val_dense_3_loss: 0.2067 - val_dense_4_accuracy: 0.9495 - val_dense_1_accuracy: 0.8901 - val_dense_3_accuracy: 0.9297 - 33s/epoch - 69ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/100GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/100GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  100\n",
      "added to csv\n",
      "\n",
      "Epoch:  101\n",
      "480/480 - 32s - loss: 0.0363 - dense_4_loss: 0.0045 - dense_1_loss: 0.0788 - dense_3_loss: 0.0272 - dense_4_accuracy: 0.9997 - dense_1_accuracy: 0.9743 - dense_3_accuracy: 0.9917 - val_loss: 0.3079 - val_dense_4_loss: 0.1507 - val_dense_1_loss: 0.3175 - val_dense_3_loss: 0.2067 - val_dense_4_accuracy: 0.9495 - val_dense_1_accuracy: 0.8901 - val_dense_3_accuracy: 0.9302 - 32s/epoch - 67ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/101GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/101GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  101\n",
      "added to csv\n",
      "\n",
      "Epoch:  102\n",
      "480/480 - 33s - loss: 0.0365 - dense_4_loss: 0.0044 - dense_1_loss: 0.0804 - dense_3_loss: 0.0267 - dense_4_accuracy: 0.9997 - dense_1_accuracy: 0.9721 - dense_3_accuracy: 0.9923 - val_loss: 0.3077 - val_dense_4_loss: 0.1506 - val_dense_1_loss: 0.3173 - val_dense_3_loss: 0.2065 - val_dense_4_accuracy: 0.9495 - val_dense_1_accuracy: 0.8906 - val_dense_3_accuracy: 0.9302 - 33s/epoch - 68ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/102GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/102GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  102\n",
      "added to csv\n",
      "\n",
      "Epoch:  103\n",
      "480/480 - 33s - loss: 0.0359 - dense_4_loss: 0.0044 - dense_1_loss: 0.0798 - dense_3_loss: 0.0252 - dense_4_accuracy: 0.9995 - dense_1_accuracy: 0.9734 - dense_3_accuracy: 0.9928 - val_loss: 0.3077 - val_dense_4_loss: 0.1506 - val_dense_1_loss: 0.3173 - val_dense_3_loss: 0.2065 - val_dense_4_accuracy: 0.9495 - val_dense_1_accuracy: 0.8901 - val_dense_3_accuracy: 0.9307 - 33s/epoch - 69ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/103GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/103GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  103\n",
      "added to csv\n",
      "\n",
      "Epoch:  104\n",
      "480/480 - 33s - loss: 0.0359 - dense_4_loss: 0.0044 - dense_1_loss: 0.0802 - dense_3_loss: 0.0249 - dense_4_accuracy: 1.0000 - dense_1_accuracy: 0.9728 - dense_3_accuracy: 0.9926 - val_loss: 0.3077 - val_dense_4_loss: 0.1506 - val_dense_1_loss: 0.3173 - val_dense_3_loss: 0.2065 - val_dense_4_accuracy: 0.9495 - val_dense_1_accuracy: 0.8901 - val_dense_3_accuracy: 0.9307 - 33s/epoch - 69ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/104GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/104GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  104\n",
      "added to csv\n",
      "\n",
      "Epoch:  105\n",
      "480/480 - 33s - loss: 0.0358 - dense_4_loss: 0.0043 - dense_1_loss: 0.0796 - dense_3_loss: 0.0253 - dense_4_accuracy: 0.9996 - dense_1_accuracy: 0.9725 - dense_3_accuracy: 0.9915 - val_loss: 0.3078 - val_dense_4_loss: 0.1506 - val_dense_1_loss: 0.3173 - val_dense_3_loss: 0.2066 - val_dense_4_accuracy: 0.9495 - val_dense_1_accuracy: 0.8901 - val_dense_3_accuracy: 0.9307 - 33s/epoch - 69ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/105GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/105GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  105\n",
      "added to csv\n",
      "\n",
      "Epoch:  106\n",
      "480/480 - 33s - loss: 0.0354 - dense_4_loss: 0.0042 - dense_1_loss: 0.0788 - dense_3_loss: 0.0251 - dense_4_accuracy: 0.9999 - dense_1_accuracy: 0.9732 - dense_3_accuracy: 0.9919 - val_loss: 0.3078 - val_dense_4_loss: 0.1507 - val_dense_1_loss: 0.3173 - val_dense_3_loss: 0.2066 - val_dense_4_accuracy: 0.9495 - val_dense_1_accuracy: 0.8901 - val_dense_3_accuracy: 0.9307 - 33s/epoch - 68ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/106GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/106GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  106\n",
      "added to csv\n",
      "\n",
      "Epoch:  107\n",
      "480/480 - 33s - loss: 0.0365 - dense_4_loss: 0.0039 - dense_1_loss: 0.0815 - dense_3_loss: 0.0270 - dense_4_accuracy: 1.0000 - dense_1_accuracy: 0.9724 - dense_3_accuracy: 0.9913 - val_loss: 0.3078 - val_dense_4_loss: 0.1507 - val_dense_1_loss: 0.3173 - val_dense_3_loss: 0.2066 - val_dense_4_accuracy: 0.9495 - val_dense_1_accuracy: 0.8901 - val_dense_3_accuracy: 0.9307 - 33s/epoch - 69ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/107GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/107GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  107\n",
      "added to csv\n",
      "\n",
      "Epoch:  108\n",
      "480/480 - 33s - loss: 0.0352 - dense_4_loss: 0.0042 - dense_1_loss: 0.0766 - dense_3_loss: 0.0267 - dense_4_accuracy: 0.9999 - dense_1_accuracy: 0.9750 - dense_3_accuracy: 0.9919 - val_loss: 0.3078 - val_dense_4_loss: 0.1507 - val_dense_1_loss: 0.3173 - val_dense_3_loss: 0.2065 - val_dense_4_accuracy: 0.9495 - val_dense_1_accuracy: 0.8896 - val_dense_3_accuracy: 0.9307 - 33s/epoch - 69ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/108GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/108GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  108\n",
      "added to csv\n",
      "\n",
      "Epoch:  109\n",
      "480/480 - 33s - loss: 0.0356 - dense_4_loss: 0.0047 - dense_1_loss: 0.0774 - dense_3_loss: 0.0258 - dense_4_accuracy: 0.9996 - dense_1_accuracy: 0.9755 - dense_3_accuracy: 0.9921 - val_loss: 0.3077 - val_dense_4_loss: 0.1506 - val_dense_1_loss: 0.3172 - val_dense_3_loss: 0.2065 - val_dense_4_accuracy: 0.9495 - val_dense_1_accuracy: 0.8896 - val_dense_3_accuracy: 0.9307 - 33s/epoch - 68ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/109GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/109GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  109\n",
      "added to csv\n",
      "\n",
      "Epoch:  110\n",
      "480/480 - 34s - loss: 0.0362 - dense_4_loss: 0.0043 - dense_1_loss: 0.0796 - dense_3_loss: 0.0270 - dense_4_accuracy: 0.9996 - dense_1_accuracy: 0.9711 - dense_3_accuracy: 0.9923 - val_loss: 0.3077 - val_dense_4_loss: 0.1506 - val_dense_1_loss: 0.3172 - val_dense_3_loss: 0.2065 - val_dense_4_accuracy: 0.9495 - val_dense_1_accuracy: 0.8896 - val_dense_3_accuracy: 0.9307 - 34s/epoch - 71ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/110GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/110GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  110\n",
      "added to csv\n",
      "\n",
      "Epoch:  111\n",
      "480/480 - 33s - loss: 0.0367 - dense_4_loss: 0.0042 - dense_1_loss: 0.0820 - dense_3_loss: 0.0262 - dense_4_accuracy: 0.9996 - dense_1_accuracy: 0.9725 - dense_3_accuracy: 0.9909 - val_loss: 0.3077 - val_dense_4_loss: 0.1506 - val_dense_1_loss: 0.3172 - val_dense_3_loss: 0.2065 - val_dense_4_accuracy: 0.9495 - val_dense_1_accuracy: 0.8896 - val_dense_3_accuracy: 0.9307 - 33s/epoch - 68ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/111GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/111GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  111\n",
      "added to csv\n",
      "\n",
      "Epoch:  112\n",
      "480/480 - 33s - loss: 0.0362 - dense_4_loss: 0.0042 - dense_1_loss: 0.0807 - dense_3_loss: 0.0259 - dense_4_accuracy: 0.9997 - dense_1_accuracy: 0.9727 - dense_3_accuracy: 0.9935 - val_loss: 0.3078 - val_dense_4_loss: 0.1507 - val_dense_1_loss: 0.3172 - val_dense_3_loss: 0.2065 - val_dense_4_accuracy: 0.9495 - val_dense_1_accuracy: 0.8901 - val_dense_3_accuracy: 0.9307 - 33s/epoch - 68ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/112GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/112GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  112\n",
      "added to csv\n",
      "\n",
      "Epoch:  113\n",
      "480/480 - 32s - loss: 0.0351 - dense_4_loss: 0.0040 - dense_1_loss: 0.0780 - dense_3_loss: 0.0255 - dense_4_accuracy: 0.9997 - dense_1_accuracy: 0.9746 - dense_3_accuracy: 0.9924 - val_loss: 0.3080 - val_dense_4_loss: 0.1509 - val_dense_1_loss: 0.3172 - val_dense_3_loss: 0.2067 - val_dense_4_accuracy: 0.9495 - val_dense_1_accuracy: 0.8891 - val_dense_3_accuracy: 0.9307 - 32s/epoch - 67ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/113GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/113GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  113\n",
      "added to csv\n",
      "\n",
      "Epoch:  114\n",
      "480/480 - 33s - loss: 0.0365 - dense_4_loss: 0.0043 - dense_1_loss: 0.0799 - dense_3_loss: 0.0274 - dense_4_accuracy: 0.9995 - dense_1_accuracy: 0.9730 - dense_3_accuracy: 0.9917 - val_loss: 0.3079 - val_dense_4_loss: 0.1507 - val_dense_1_loss: 0.3172 - val_dense_3_loss: 0.2066 - val_dense_4_accuracy: 0.9495 - val_dense_1_accuracy: 0.8896 - val_dense_3_accuracy: 0.9307 - 33s/epoch - 68ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/114GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/114GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  114\n",
      "added to csv\n",
      "\n",
      "Epoch:  115\n",
      "480/480 - 32s - loss: 0.0360 - dense_4_loss: 0.0045 - dense_1_loss: 0.0769 - dense_3_loss: 0.0280 - dense_4_accuracy: 0.9999 - dense_1_accuracy: 0.9755 - dense_3_accuracy: 0.9923 - val_loss: 0.3078 - val_dense_4_loss: 0.1507 - val_dense_1_loss: 0.3171 - val_dense_3_loss: 0.2066 - val_dense_4_accuracy: 0.9500 - val_dense_1_accuracy: 0.8896 - val_dense_3_accuracy: 0.9307 - 32s/epoch - 68ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/115GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/115GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  115\n",
      "added to csv\n",
      "\n",
      "Epoch:  116\n",
      "480/480 - 33s - loss: 0.0364 - dense_4_loss: 0.0043 - dense_1_loss: 0.0809 - dense_3_loss: 0.0258 - dense_4_accuracy: 0.9997 - dense_1_accuracy: 0.9716 - dense_3_accuracy: 0.9935 - val_loss: 0.3078 - val_dense_4_loss: 0.1507 - val_dense_1_loss: 0.3170 - val_dense_3_loss: 0.2065 - val_dense_4_accuracy: 0.9495 - val_dense_1_accuracy: 0.8896 - val_dense_3_accuracy: 0.9307 - 33s/epoch - 68ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/116GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/116GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  116\n",
      "added to csv\n",
      "\n",
      "Epoch:  117\n",
      "480/480 - 33s - loss: 0.0355 - dense_4_loss: 0.0042 - dense_1_loss: 0.0773 - dense_3_loss: 0.0270 - dense_4_accuracy: 0.9997 - dense_1_accuracy: 0.9741 - dense_3_accuracy: 0.9924 - val_loss: 0.3077 - val_dense_4_loss: 0.1507 - val_dense_1_loss: 0.3170 - val_dense_3_loss: 0.2064 - val_dense_4_accuracy: 0.9495 - val_dense_1_accuracy: 0.8896 - val_dense_3_accuracy: 0.9302 - 33s/epoch - 68ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/117GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/117GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  117\n",
      "added to csv\n",
      "\n",
      "Epoch:  118\n",
      "480/480 - 33s - loss: 0.0366 - dense_4_loss: 0.0047 - dense_1_loss: 0.0796 - dense_3_loss: 0.0266 - dense_4_accuracy: 0.9993 - dense_1_accuracy: 0.9749 - dense_3_accuracy: 0.9921 - val_loss: 0.3078 - val_dense_4_loss: 0.1508 - val_dense_1_loss: 0.3170 - val_dense_3_loss: 0.2064 - val_dense_4_accuracy: 0.9495 - val_dense_1_accuracy: 0.8896 - val_dense_3_accuracy: 0.9302 - 33s/epoch - 68ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/118GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/118GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  118\n",
      "added to csv\n",
      "\n",
      "Epoch:  119\n",
      "480/480 - 32s - loss: 0.0363 - dense_4_loss: 0.0047 - dense_1_loss: 0.0799 - dense_3_loss: 0.0255 - dense_4_accuracy: 0.9995 - dense_1_accuracy: 0.9742 - dense_3_accuracy: 0.9926 - val_loss: 0.3078 - val_dense_4_loss: 0.1508 - val_dense_1_loss: 0.3171 - val_dense_3_loss: 0.2064 - val_dense_4_accuracy: 0.9495 - val_dense_1_accuracy: 0.8896 - val_dense_3_accuracy: 0.9312 - 32s/epoch - 67ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/119GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/119GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  119\n",
      "added to csv\n",
      "\n",
      "Epoch:  120\n",
      "480/480 - 33s - loss: 0.0360 - dense_4_loss: 0.0047 - dense_1_loss: 0.0788 - dense_3_loss: 0.0255 - dense_4_accuracy: 0.9999 - dense_1_accuracy: 0.9754 - dense_3_accuracy: 0.9926 - val_loss: 0.3079 - val_dense_4_loss: 0.1508 - val_dense_1_loss: 0.3171 - val_dense_3_loss: 0.2065 - val_dense_4_accuracy: 0.9495 - val_dense_1_accuracy: 0.8896 - val_dense_3_accuracy: 0.9307 - 33s/epoch - 68ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/120GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/120GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  120\n",
      "added to csv\n",
      "\n",
      "Epoch:  121\n",
      "480/480 - 34s - loss: 0.0354 - dense_4_loss: 0.0040 - dense_1_loss: 0.0800 - dense_3_loss: 0.0246 - dense_4_accuracy: 0.9997 - dense_1_accuracy: 0.9714 - dense_3_accuracy: 0.9926 - val_loss: 0.3079 - val_dense_4_loss: 0.1508 - val_dense_1_loss: 0.3171 - val_dense_3_loss: 0.2065 - val_dense_4_accuracy: 0.9495 - val_dense_1_accuracy: 0.8896 - val_dense_3_accuracy: 0.9312 - 34s/epoch - 70ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/121GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/121GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  121\n",
      "added to csv\n",
      "\n",
      "Epoch:  122\n",
      "480/480 - 33s - loss: 0.0360 - dense_4_loss: 0.0046 - dense_1_loss: 0.0795 - dense_3_loss: 0.0253 - dense_4_accuracy: 0.9999 - dense_1_accuracy: 0.9747 - dense_3_accuracy: 0.9927 - val_loss: 0.3080 - val_dense_4_loss: 0.1509 - val_dense_1_loss: 0.3171 - val_dense_3_loss: 0.2066 - val_dense_4_accuracy: 0.9495 - val_dense_1_accuracy: 0.8901 - val_dense_3_accuracy: 0.9312 - 33s/epoch - 68ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/122GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/122GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  122\n",
      "added to csv\n",
      "\n",
      "Epoch:  123\n",
      "480/480 - 32s - loss: 0.0351 - dense_4_loss: 0.0042 - dense_1_loss: 0.0763 - dense_3_loss: 0.0267 - dense_4_accuracy: 0.9997 - dense_1_accuracy: 0.9763 - dense_3_accuracy: 0.9921 - val_loss: 0.3079 - val_dense_4_loss: 0.1509 - val_dense_1_loss: 0.3171 - val_dense_3_loss: 0.2065 - val_dense_4_accuracy: 0.9495 - val_dense_1_accuracy: 0.8901 - val_dense_3_accuracy: 0.9312 - 32s/epoch - 67ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/123GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/123GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  123\n",
      "added to csv\n",
      "\n",
      "Epoch:  124\n",
      "480/480 - 32s - loss: 0.0370 - dense_4_loss: 0.0045 - dense_1_loss: 0.0827 - dense_3_loss: 0.0256 - dense_4_accuracy: 0.9996 - dense_1_accuracy: 0.9732 - dense_3_accuracy: 0.9921 - val_loss: 0.3080 - val_dense_4_loss: 0.1509 - val_dense_1_loss: 0.3171 - val_dense_3_loss: 0.2065 - val_dense_4_accuracy: 0.9500 - val_dense_1_accuracy: 0.8901 - val_dense_3_accuracy: 0.9312 - 32s/epoch - 67ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/124GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/124GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  124\n",
      "added to csv\n",
      "\n",
      "Epoch:  125\n",
      "480/480 - 33s - loss: 0.0354 - dense_4_loss: 0.0042 - dense_1_loss: 0.0788 - dense_3_loss: 0.0254 - dense_4_accuracy: 0.9997 - dense_1_accuracy: 0.9734 - dense_3_accuracy: 0.9919 - val_loss: 0.3079 - val_dense_4_loss: 0.1508 - val_dense_1_loss: 0.3171 - val_dense_3_loss: 0.2064 - val_dense_4_accuracy: 0.9495 - val_dense_1_accuracy: 0.8901 - val_dense_3_accuracy: 0.9307 - 33s/epoch - 69ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/125GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/125GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  125\n",
      "added to csv\n",
      "\n",
      "Epoch:  126\n",
      "480/480 - 33s - loss: 0.0343 - dense_4_loss: 0.0040 - dense_1_loss: 0.0747 - dense_3_loss: 0.0262 - dense_4_accuracy: 0.9999 - dense_1_accuracy: 0.9776 - dense_3_accuracy: 0.9926 - val_loss: 0.3079 - val_dense_4_loss: 0.1508 - val_dense_1_loss: 0.3171 - val_dense_3_loss: 0.2064 - val_dense_4_accuracy: 0.9495 - val_dense_1_accuracy: 0.8901 - val_dense_3_accuracy: 0.9302 - 33s/epoch - 68ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/126GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/126GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  126\n",
      "added to csv\n",
      "\n",
      "Epoch:  127\n",
      "480/480 - 34s - loss: 0.0365 - dense_4_loss: 0.0045 - dense_1_loss: 0.0807 - dense_3_loss: 0.0258 - dense_4_accuracy: 0.9997 - dense_1_accuracy: 0.9715 - dense_3_accuracy: 0.9922 - val_loss: 0.3079 - val_dense_4_loss: 0.1508 - val_dense_1_loss: 0.3171 - val_dense_3_loss: 0.2064 - val_dense_4_accuracy: 0.9495 - val_dense_1_accuracy: 0.8901 - val_dense_3_accuracy: 0.9302 - 34s/epoch - 70ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/127GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/127GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  127\n",
      "added to csv\n",
      "\n",
      "Epoch:  128\n",
      "480/480 - 32s - loss: 0.0356 - dense_4_loss: 0.0045 - dense_1_loss: 0.0781 - dense_3_loss: 0.0256 - dense_4_accuracy: 0.9995 - dense_1_accuracy: 0.9766 - dense_3_accuracy: 0.9935 - val_loss: 0.3079 - val_dense_4_loss: 0.1508 - val_dense_1_loss: 0.3171 - val_dense_3_loss: 0.2064 - val_dense_4_accuracy: 0.9495 - val_dense_1_accuracy: 0.8896 - val_dense_3_accuracy: 0.9302 - 32s/epoch - 68ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/128GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/128GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  128\n",
      "added to csv\n",
      "\n",
      "Epoch:  129\n",
      "480/480 - 34s - loss: 0.0355 - dense_4_loss: 0.0043 - dense_1_loss: 0.0782 - dense_3_loss: 0.0258 - dense_4_accuracy: 0.9997 - dense_1_accuracy: 0.9751 - dense_3_accuracy: 0.9917 - val_loss: 0.3079 - val_dense_4_loss: 0.1508 - val_dense_1_loss: 0.3171 - val_dense_3_loss: 0.2064 - val_dense_4_accuracy: 0.9495 - val_dense_1_accuracy: 0.8901 - val_dense_3_accuracy: 0.9302 - 34s/epoch - 70ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/129GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/129GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  129\n",
      "added to csv\n",
      "\n",
      "Epoch:  130\n",
      "480/480 - 32s - loss: 0.0354 - dense_4_loss: 0.0041 - dense_1_loss: 0.0778 - dense_3_loss: 0.0267 - dense_4_accuracy: 0.9997 - dense_1_accuracy: 0.9745 - dense_3_accuracy: 0.9923 - val_loss: 0.3079 - val_dense_4_loss: 0.1508 - val_dense_1_loss: 0.3171 - val_dense_3_loss: 0.2064 - val_dense_4_accuracy: 0.9495 - val_dense_1_accuracy: 0.8896 - val_dense_3_accuracy: 0.9302 - 32s/epoch - 67ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/130GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/130GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  130\n",
      "added to csv\n",
      "\n",
      "Epoch:  131\n",
      "480/480 - 33s - loss: 0.0359 - dense_4_loss: 0.0044 - dense_1_loss: 0.0797 - dense_3_loss: 0.0254 - dense_4_accuracy: 0.9997 - dense_1_accuracy: 0.9733 - dense_3_accuracy: 0.9922 - val_loss: 0.3079 - val_dense_4_loss: 0.1508 - val_dense_1_loss: 0.3171 - val_dense_3_loss: 0.2064 - val_dense_4_accuracy: 0.9495 - val_dense_1_accuracy: 0.8901 - val_dense_3_accuracy: 0.9307 - 33s/epoch - 69ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/131GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/131GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  131\n",
      "added to csv\n",
      "\n",
      "Epoch:  132\n",
      "480/480 - 32s - loss: 0.0357 - dense_4_loss: 0.0040 - dense_1_loss: 0.0794 - dense_3_loss: 0.0263 - dense_4_accuracy: 1.0000 - dense_1_accuracy: 0.9741 - dense_3_accuracy: 0.9922 - val_loss: 0.3079 - val_dense_4_loss: 0.1508 - val_dense_1_loss: 0.3171 - val_dense_3_loss: 0.2064 - val_dense_4_accuracy: 0.9495 - val_dense_1_accuracy: 0.8901 - val_dense_3_accuracy: 0.9302 - 32s/epoch - 67ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/132GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/132GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  132\n",
      "added to csv\n",
      "\n",
      "Epoch:  133\n",
      "480/480 - 33s - loss: 0.0361 - dense_4_loss: 0.0044 - dense_1_loss: 0.0781 - dense_3_loss: 0.0274 - dense_4_accuracy: 0.9997 - dense_1_accuracy: 0.9757 - dense_3_accuracy: 0.9921 - val_loss: 0.3079 - val_dense_4_loss: 0.1508 - val_dense_1_loss: 0.3171 - val_dense_3_loss: 0.2064 - val_dense_4_accuracy: 0.9495 - val_dense_1_accuracy: 0.8901 - val_dense_3_accuracy: 0.9302 - 33s/epoch - 68ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/133GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/133GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  133\n",
      "added to csv\n",
      "\n",
      "Epoch:  134\n",
      "480/480 - 33s - loss: 0.0369 - dense_4_loss: 0.0044 - dense_1_loss: 0.0818 - dense_3_loss: 0.0266 - dense_4_accuracy: 0.9996 - dense_1_accuracy: 0.9720 - dense_3_accuracy: 0.9915 - val_loss: 0.3079 - val_dense_4_loss: 0.1508 - val_dense_1_loss: 0.3171 - val_dense_3_loss: 0.2064 - val_dense_4_accuracy: 0.9495 - val_dense_1_accuracy: 0.8901 - val_dense_3_accuracy: 0.9302 - 33s/epoch - 68ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/134GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/134GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  134\n",
      "added to csv\n",
      "\n",
      "Epoch:  135\n",
      "480/480 - 32s - loss: 0.0352 - dense_4_loss: 0.0039 - dense_1_loss: 0.0767 - dense_3_loss: 0.0277 - dense_4_accuracy: 0.9999 - dense_1_accuracy: 0.9753 - dense_3_accuracy: 0.9911 - val_loss: 0.3079 - val_dense_4_loss: 0.1508 - val_dense_1_loss: 0.3171 - val_dense_3_loss: 0.2064 - val_dense_4_accuracy: 0.9495 - val_dense_1_accuracy: 0.8896 - val_dense_3_accuracy: 0.9302 - 32s/epoch - 67ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/135GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/135GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  135\n",
      "added to csv\n",
      "\n",
      "Epoch:  136\n",
      "480/480 - 32s - loss: 0.0355 - dense_4_loss: 0.0041 - dense_1_loss: 0.0773 - dense_3_loss: 0.0275 - dense_4_accuracy: 1.0000 - dense_1_accuracy: 0.9740 - dense_3_accuracy: 0.9914 - val_loss: 0.3079 - val_dense_4_loss: 0.1508 - val_dense_1_loss: 0.3171 - val_dense_3_loss: 0.2064 - val_dense_4_accuracy: 0.9495 - val_dense_1_accuracy: 0.8901 - val_dense_3_accuracy: 0.9302 - 32s/epoch - 66ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/136GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/136GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  136\n",
      "added to csv\n",
      "\n",
      "Epoch:  137\n",
      "480/480 - 32s - loss: 0.0370 - dense_4_loss: 0.0046 - dense_1_loss: 0.0813 - dense_3_loss: 0.0268 - dense_4_accuracy: 0.9996 - dense_1_accuracy: 0.9714 - dense_3_accuracy: 0.9931 - val_loss: 0.3079 - val_dense_4_loss: 0.1508 - val_dense_1_loss: 0.3171 - val_dense_3_loss: 0.2064 - val_dense_4_accuracy: 0.9495 - val_dense_1_accuracy: 0.8896 - val_dense_3_accuracy: 0.9307 - 32s/epoch - 67ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/137GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/137GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  137\n",
      "added to csv\n",
      "\n",
      "Epoch:  138\n",
      "480/480 - 33s - loss: 0.0357 - dense_4_loss: 0.0042 - dense_1_loss: 0.0788 - dense_3_loss: 0.0261 - dense_4_accuracy: 1.0000 - dense_1_accuracy: 0.9727 - dense_3_accuracy: 0.9913 - val_loss: 0.3079 - val_dense_4_loss: 0.1508 - val_dense_1_loss: 0.3171 - val_dense_3_loss: 0.2064 - val_dense_4_accuracy: 0.9495 - val_dense_1_accuracy: 0.8896 - val_dense_3_accuracy: 0.9302 - 33s/epoch - 69ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/138GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/138GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  138\n",
      "added to csv\n",
      "\n",
      "Epoch:  139\n",
      "480/480 - 33s - loss: 0.0361 - dense_4_loss: 0.0044 - dense_1_loss: 0.0783 - dense_3_loss: 0.0273 - dense_4_accuracy: 0.9995 - dense_1_accuracy: 0.9751 - dense_3_accuracy: 0.9910 - val_loss: 0.3079 - val_dense_4_loss: 0.1509 - val_dense_1_loss: 0.3171 - val_dense_3_loss: 0.2064 - val_dense_4_accuracy: 0.9495 - val_dense_1_accuracy: 0.8896 - val_dense_3_accuracy: 0.9302 - 33s/epoch - 69ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/139GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/139GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  139\n",
      "added to csv\n",
      "\n",
      "Epoch:  140\n",
      "480/480 - 33s - loss: 0.0356 - dense_4_loss: 0.0041 - dense_1_loss: 0.0786 - dense_3_loss: 0.0264 - dense_4_accuracy: 1.0000 - dense_1_accuracy: 0.9742 - dense_3_accuracy: 0.9927 - val_loss: 0.3079 - val_dense_4_loss: 0.1509 - val_dense_1_loss: 0.3171 - val_dense_3_loss: 0.2064 - val_dense_4_accuracy: 0.9495 - val_dense_1_accuracy: 0.8896 - val_dense_3_accuracy: 0.9302 - 33s/epoch - 68ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/140GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/140GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  140\n",
      "added to csv\n",
      "\n",
      "Epoch:  141\n",
      "480/480 - 33s - loss: 0.0354 - dense_4_loss: 0.0044 - dense_1_loss: 0.0775 - dense_3_loss: 0.0256 - dense_4_accuracy: 0.9999 - dense_1_accuracy: 0.9740 - dense_3_accuracy: 0.9928 - val_loss: 0.3079 - val_dense_4_loss: 0.1509 - val_dense_1_loss: 0.3171 - val_dense_3_loss: 0.2064 - val_dense_4_accuracy: 0.9495 - val_dense_1_accuracy: 0.8901 - val_dense_3_accuracy: 0.9302 - 33s/epoch - 68ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/141GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/141GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  141\n",
      "added to csv\n",
      "\n",
      "Epoch:  142\n",
      "480/480 - 32s - loss: 0.0364 - dense_4_loss: 0.0048 - dense_1_loss: 0.0803 - dense_3_loss: 0.0252 - dense_4_accuracy: 0.9993 - dense_1_accuracy: 0.9729 - dense_3_accuracy: 0.9931 - val_loss: 0.3079 - val_dense_4_loss: 0.1508 - val_dense_1_loss: 0.3171 - val_dense_3_loss: 0.2064 - val_dense_4_accuracy: 0.9495 - val_dense_1_accuracy: 0.8896 - val_dense_3_accuracy: 0.9302 - 32s/epoch - 68ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/142GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/142GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  142\n",
      "added to csv\n",
      "\n",
      "Epoch:  143\n",
      "480/480 - 33s - loss: 0.0353 - dense_4_loss: 0.0045 - dense_1_loss: 0.0774 - dense_3_loss: 0.0253 - dense_4_accuracy: 0.9996 - dense_1_accuracy: 0.9762 - dense_3_accuracy: 0.9934 - val_loss: 0.3079 - val_dense_4_loss: 0.1509 - val_dense_1_loss: 0.3171 - val_dense_3_loss: 0.2064 - val_dense_4_accuracy: 0.9495 - val_dense_1_accuracy: 0.8896 - val_dense_3_accuracy: 0.9302 - 33s/epoch - 68ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/143GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/143GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  143\n",
      "added to csv\n",
      "\n",
      "Epoch:  144\n",
      "480/480 - 32s - loss: 0.0359 - dense_4_loss: 0.0042 - dense_1_loss: 0.0788 - dense_3_loss: 0.0266 - dense_4_accuracy: 0.9999 - dense_1_accuracy: 0.9757 - dense_3_accuracy: 0.9919 - val_loss: 0.3079 - val_dense_4_loss: 0.1509 - val_dense_1_loss: 0.3171 - val_dense_3_loss: 0.2064 - val_dense_4_accuracy: 0.9495 - val_dense_1_accuracy: 0.8896 - val_dense_3_accuracy: 0.9302 - 32s/epoch - 68ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/144GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/144GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  144\n",
      "added to csv\n",
      "\n",
      "Epoch:  145\n",
      "480/480 - 32s - loss: 0.0358 - dense_4_loss: 0.0045 - dense_1_loss: 0.0779 - dense_3_loss: 0.0266 - dense_4_accuracy: 0.9999 - dense_1_accuracy: 0.9755 - dense_3_accuracy: 0.9922 - val_loss: 0.3079 - val_dense_4_loss: 0.1508 - val_dense_1_loss: 0.3171 - val_dense_3_loss: 0.2064 - val_dense_4_accuracy: 0.9495 - val_dense_1_accuracy: 0.8901 - val_dense_3_accuracy: 0.9307 - 32s/epoch - 67ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/145GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/145GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  145\n",
      "added to csv\n",
      "\n",
      "Epoch:  146\n",
      "480/480 - 32s - loss: 0.0375 - dense_4_loss: 0.0042 - dense_1_loss: 0.0827 - dense_3_loss: 0.0283 - dense_4_accuracy: 0.9999 - dense_1_accuracy: 0.9724 - dense_3_accuracy: 0.9906 - val_loss: 0.3079 - val_dense_4_loss: 0.1508 - val_dense_1_loss: 0.3171 - val_dense_3_loss: 0.2064 - val_dense_4_accuracy: 0.9495 - val_dense_1_accuracy: 0.8901 - val_dense_3_accuracy: 0.9307 - 32s/epoch - 67ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/146GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/146GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  146\n",
      "added to csv\n",
      "\n",
      "Epoch:  147\n",
      "480/480 - 33s - loss: 0.0361 - dense_4_loss: 0.0041 - dense_1_loss: 0.0797 - dense_3_loss: 0.0268 - dense_4_accuracy: 0.9996 - dense_1_accuracy: 0.9753 - dense_3_accuracy: 0.9917 - val_loss: 0.3079 - val_dense_4_loss: 0.1508 - val_dense_1_loss: 0.3171 - val_dense_3_loss: 0.2064 - val_dense_4_accuracy: 0.9495 - val_dense_1_accuracy: 0.8901 - val_dense_3_accuracy: 0.9307 - 33s/epoch - 69ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/147GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/147GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  147\n",
      "added to csv\n",
      "\n",
      "Epoch:  148\n",
      "480/480 - 32s - loss: 0.0346 - dense_4_loss: 0.0043 - dense_1_loss: 0.0767 - dense_3_loss: 0.0241 - dense_4_accuracy: 0.9997 - dense_1_accuracy: 0.9733 - dense_3_accuracy: 0.9940 - val_loss: 0.3079 - val_dense_4_loss: 0.1508 - val_dense_1_loss: 0.3171 - val_dense_3_loss: 0.2064 - val_dense_4_accuracy: 0.9495 - val_dense_1_accuracy: 0.8896 - val_dense_3_accuracy: 0.9307 - 32s/epoch - 68ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/148GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/148GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  148\n",
      "added to csv\n",
      "\n",
      "Epoch:  149\n",
      "480/480 - 33s - loss: 0.0364 - dense_4_loss: 0.0043 - dense_1_loss: 0.0809 - dense_3_loss: 0.0262 - dense_4_accuracy: 0.9999 - dense_1_accuracy: 0.9737 - dense_3_accuracy: 0.9917 - val_loss: 0.3079 - val_dense_4_loss: 0.1508 - val_dense_1_loss: 0.3171 - val_dense_3_loss: 0.2064 - val_dense_4_accuracy: 0.9495 - val_dense_1_accuracy: 0.8896 - val_dense_3_accuracy: 0.9307 - 33s/epoch - 69ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/149GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/149GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  149\n",
      "added to csv\n",
      "\n",
      "Epoch:  150\n",
      "480/480 - 33s - loss: 0.0362 - dense_4_loss: 0.0044 - dense_1_loss: 0.0783 - dense_3_loss: 0.0274 - dense_4_accuracy: 0.9996 - dense_1_accuracy: 0.9747 - dense_3_accuracy: 0.9913 - val_loss: 0.3079 - val_dense_4_loss: 0.1508 - val_dense_1_loss: 0.3171 - val_dense_3_loss: 0.2064 - val_dense_4_accuracy: 0.9495 - val_dense_1_accuracy: 0.8896 - val_dense_3_accuracy: 0.9307 - 33s/epoch - 68ms/step\n",
      "INFO:tensorflow:Assets written to: ./GNmodels/150GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./GNmodels/150GN__IC_01500_FilledArea_0.9_BandNo_60_ImageHeight_30_ImageWidth_30_FILTER_snv_FeatureExtraction_none/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved on epoch:  150\n",
      "added to csv\n",
      "Testing time (s) = 9143.142106663\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tic = start_timer()\n",
    "while start_epoch<=last_epoch:\n",
    "    print(\"\\nEpoch: \",start_epoch)\n",
    "    history = model.fit(x_train ,y_train ,batch_size=BATCH_SIZE ,epochs=1, verbose=2, validation_data=(x_val, y_val), shuffle=True)\n",
    "    model.save('./GNmodels/'+str(start_epoch)+model_name)\n",
    "    print(\"Model saved on epoch: \",start_epoch)\n",
    "    \n",
    "    history_dataframe = pd.DataFrame.from_dict(history.history)\n",
    "    save_to_csv('./csvs/'+model_name+'.csv', history_dataframe, header=True)\n",
    "    print(\"added to csv\")\n",
    "    start_epoch+=1\n",
    "    \n",
    "toc = end_timer()\n",
    "show_time(tic,toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "746a8bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 5s 55ms/step\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "y_pred = model.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9669eed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_labels = np.argmax(y_pred, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dba28a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9533333333333334\n",
      "Confusion Matrix:\n",
      "[[297   0   3   0   0   0   0   0]\n",
      " [  0 267   0  21   6   0   2   4]\n",
      " [  8   0 292   0   0   0   0   0]\n",
      " [  0  24   1 261   6   0   2   6]\n",
      " [  0   1   0   1 291   0   1   6]\n",
      " [  0   0   0   0   0 300   0   0]\n",
      " [  0   0   0   5   0   1 292   2]\n",
      " [  0   3   0   5   4   0   0 288]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       300\n",
      "           1       0.91      0.89      0.90       300\n",
      "           2       0.99      0.97      0.98       300\n",
      "           3       0.89      0.87      0.88       300\n",
      "           4       0.95      0.97      0.96       300\n",
      "           5       1.00      1.00      1.00       300\n",
      "           6       0.98      0.97      0.98       300\n",
      "           7       0.94      0.96      0.95       300\n",
      "\n",
      "    accuracy                           0.95      2400\n",
      "   macro avg       0.95      0.95      0.95      2400\n",
      "weighted avg       0.95      0.95      0.95      2400\n",
      "\n",
      "Accuracy: 0.8920833333333333\n",
      "Confusion Matrix:\n",
      "[[292   0   7   1   0   0   0   0]\n",
      " [  0 228   0  47  11   0   4  10]\n",
      " [ 14   0 286   0   0   0   0   0]\n",
      " [  0  43   0 228   5   0  12  12]\n",
      " [  0   7   0   6 270   1   2  14]\n",
      " [  0   0   0   0   0 299   1   0]\n",
      " [  0   2   0  15   7   0 268   8]\n",
      " [  0   6   0  11   9   0   4 270]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96       300\n",
      "           1       0.80      0.76      0.78       300\n",
      "           2       0.98      0.95      0.96       300\n",
      "           3       0.74      0.76      0.75       300\n",
      "           4       0.89      0.90      0.90       300\n",
      "           5       1.00      1.00      1.00       300\n",
      "           6       0.92      0.89      0.91       300\n",
      "           7       0.86      0.90      0.88       300\n",
      "\n",
      "    accuracy                           0.89      2400\n",
      "   macro avg       0.89      0.89      0.89      2400\n",
      "weighted avg       0.89      0.89      0.89      2400\n",
      "\n",
      "Accuracy: 0.93125\n",
      "Confusion Matrix:\n",
      "[[296   1   3   0   0   0   0   0]\n",
      " [  0 242   0  43   8   0   2   5]\n",
      " [  9   0 291   0   0   0   0   0]\n",
      " [  0  34   0 250   4   0   6   6]\n",
      " [  0   3   0   4 285   0   1   7]\n",
      " [  0   0   0   0   0 299   1   0]\n",
      " [  0   0   0   7   2   1 286   4]\n",
      " [  0   4   0   4   5   0   1 286]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       300\n",
      "           1       0.85      0.81      0.83       300\n",
      "           2       0.99      0.97      0.98       300\n",
      "           3       0.81      0.83      0.82       300\n",
      "           4       0.94      0.95      0.94       300\n",
      "           5       1.00      1.00      1.00       300\n",
      "           6       0.96      0.95      0.96       300\n",
      "           7       0.93      0.95      0.94       300\n",
      "\n",
      "    accuracy                           0.93      2400\n",
      "   macro avg       0.93      0.93      0.93      2400\n",
      "weighted avg       0.93      0.93      0.93      2400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "\n",
    "for y_pred_label in y_pred_labels:\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(test_dataset_label, y_pred_label)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    \n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(test_dataset_label, y_pred_label)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    if(i==0):\n",
    "        cm1=cm\n",
    "        i=i+1\n",
    "    \n",
    "    # Calculate precision, recall, and F1-score for each class\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(test_dataset_label, y_pred_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "74c24af8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "df_cm = pd.DataFrame(cm1,index =[i for i in VARIETIES],columns=[i for i in VARIETIES])\n",
    "sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16},fmt='.0f') # font size\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298f76d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00ac758",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
