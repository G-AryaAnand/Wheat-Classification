{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb661222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install opencv-python scikit-learn scikit-image matplotlib spectral keras_tuner vis\n",
    "# !pip install tensorflow numpy pandas\n",
    "# !pip install spectral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b605089",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e4a0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cc1a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow import keras as keras\n",
    "from tensorflow.keras import layers as layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c004aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, timeit\n",
    "from skimage.filters import threshold_otsu\n",
    "import numpy as np\n",
    "from math import inf as inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10614a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9300cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spectral.io import envi as envi\n",
    "from spectral import imshow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee790ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3717a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd26930",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a8000a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# print(gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f72da7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import platform\n",
    "DATA_DIRECTORY = \"\"\n",
    "SLASH = \"\"\n",
    "if platform == \"linux\" or platform == \"linux2\":\n",
    "    DATA_DIRECTORY = \"/home/ishu_g.iitr/wheat/data/BULK/\"\n",
    "    SLASH = \"/\"\n",
    "elif platform == \"win32\":\n",
    "    DATA_DIRECTORY = \"D:\\mvl\\wheat\\data\\BULK\\\\\"\n",
    "    SLASH=\"\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5979451f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constants\n",
    "BAND_NUMBER = 60\n",
    "FILLED_AREA_RATIO = 0.9\n",
    "TOTAL_IMAGE_COUNT = 2400\n",
    "IMAGE_COUNT = int(TOTAL_IMAGE_COUNT/4)\n",
    "NUM_VARIETIES = 4\n",
    "\n",
    "IMAGE_WIDTH = 30\n",
    "IMAGE_HEIGHT = 30\n",
    "\n",
    "TRAIN_IMAGES = 1200\n",
    "TEST_IMAGES = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066f0249",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIVATION_TYPE =  \"relu\"\n",
    "BATCH_SIZE = 2*NUM_VARIETIES\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE_BASE = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f32e132",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class filter_method(Enum):\n",
    "    none = 0\n",
    "    snv = 1\n",
    "    msc = 2\n",
    "    savgol = 3\n",
    "\n",
    "FILT = 1\n",
    "FILTER = filter_method(FILT).name\n",
    "\n",
    "# to be set if filter chosen is savgol\n",
    "WINDOW = 7\n",
    "ORDER = 2\n",
    "DERIVATIVE = \"none\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3a1d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    " \n",
    "class feature_extraction_method(Enum):\n",
    "    none = 0\n",
    "    pca_loading = 1\n",
    "    lda = 2\n",
    "    ipca = 3\n",
    "\n",
    "FEAT_EXT = 0\n",
    "FEATURE_EXTRACTION = feature_extraction_method(FEAT_EXT).name\n",
    "\n",
    "NUM_OF_BANDS = 3\n",
    "if FEATURE_EXTRACTION == \"pca_loading\" or FEATURE_EXTRACTION == \"ipca\":\n",
    "    NUM_OF_BANDS = 8\n",
    "elif FEATURE_EXTRACTION == \"lda\":\n",
    "    NUM_OF_BANDS = 3\n",
    "    assert NUM_OF_BANDS <= min(NUM_VARIETIES-1,168),\"NUM_OF_BANDS is greater.\"\n",
    "\n",
    "\n",
    "REMOVE_NOISY_BANDS = False\n",
    "FIRST_BAND = 15\n",
    "LAST_BAND = 161"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61072a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_timer():\n",
    "    print(\"Testing started\")\n",
    "    return timeit.default_timer()\n",
    "\n",
    "def end_timer():\n",
    "    return timeit.default_timer()\n",
    "\n",
    "def show_time(tic,toc): \n",
    "    test_time = toc - tic\n",
    "    print('Testing time (s) = ' + str(test_time) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16e403c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List for All varieties\n",
    "VARIETIES = []\n",
    "VARIETIES_CODE = {}\n",
    "\n",
    "for name in os.listdir(DATA_DIRECTORY):\n",
    "    if (name.endswith(\".hdr\") or name.endswith(\".bil\")):\n",
    "        continue\n",
    "    VARIETIES_CODE[name] = len(VARIETIES)\n",
    "    VARIETIES.append(name)\n",
    "    if len(VARIETIES)==NUM_VARIETIES:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff961d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "VARIETIES = ['DBW 187', 'DBW222', 'HD 3086', 'PBW 291']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72409e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_file_name(variety):\n",
    "    name = \"./dataset/\"+str(variety).zfill(3)+\"_IC_\"+str(TOTAL_IMAGE_COUNT).zfill(5)+\"_FilledArea_\"+str(FILLED_AREA_RATIO)+\"_BandNo_\"+str(BAND_NUMBER)+\"_ImageHeight_\"+str(IMAGE_HEIGHT)+\"_ImageWidth_\"+str(IMAGE_WIDTH)\n",
    "    if FILT != 0:\n",
    "        name+=\"_FILTER_\"+str(FILTER)\n",
    "    if FEAT_EXT !=0:\n",
    "        name+=\"_FeatureExtraction_\"+str(FEATURE_EXTRACTION)+\"_NumOfBands_\"+str(NUM_OF_BANDS)\n",
    "    if REMOVE_NOISY_BANDS:\n",
    "        name+=\"_REMOVE_NOISY_BANDS_\"+str(REMOVE_NOISY_BANDS)+\"_FB_\"+str(FIRST_BAND)+\"_LB_\"+str(LAST_BAND)\n",
    "    if FILTER == \"savgol\":\n",
    "        name+=\"_WINDOW_\"+str(WINDOW)+\"_ORDER_\"+str(ORDER)\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9dd572",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(VARIETIES[:NUM_VARIETIES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74afa6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = np.empty([0, IMAGE_HEIGHT, IMAGE_WIDTH, 168])\n",
    "train_dataset_label = np.empty([0,], dtype=int)\n",
    "test_dataset= np.empty([0, IMAGE_HEIGHT, IMAGE_WIDTH, 168])\n",
    "test_dataset_label = np.empty([0,], dtype=int)\n",
    "\n",
    "for idx, v in enumerate(VARIETIES):\n",
    "    print(\"idx: \",idx)\n",
    "    if idx >= NUM_VARIETIES:\n",
    "        break\n",
    "    train_dataset= np.concatenate((train_dataset, np.load(dataset_file_name(v)+\"_train_dataset.npy\")[:TRAIN_IMAGES]), axis =0)\n",
    "    train_dataset_label = np.concatenate((train_dataset_label, np.load(dataset_file_name(v)+\"_train_dataset_label.npy\")[:TRAIN_IMAGES]), axis =0)\n",
    "    test_dataset = np.concatenate((test_dataset, np.load(dataset_file_name(v)+\"_test_dataset.npy\")[:TEST_IMAGES]), axis =0)\n",
    "    test_dataset_label = np.concatenate((test_dataset_label, np.load(dataset_file_name(v)+\"_test_dataset_label.npy\")[:TEST_IMAGES]), axis =0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa2f8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_dataset.reshape(train_dataset.shape[0], -1)\n",
    "y_train = train_dataset_label.reshape(train_dataset_label.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c752599c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import datasets, layers, models, losses, Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPool2D, MaxPooling2D, Activation, Flatten, Dense, AveragePooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48cd5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeDataWholeSeed(data,normalization_type='max'):\n",
    "    \n",
    "    if normalization_type == 'max':\n",
    "        for idx in range(data.shape[0]):\n",
    "            data[idx,:,:,:] = data[idx,:,:,:]/np.max(abs(data[idx,:,:,:]))\n",
    "            \n",
    "    elif normalization_type == 'l2norm':\n",
    "        from numpy import linalg as LA\n",
    "        for idx in range(data.shape[0]):\n",
    "            data[idx,:,:,:] = data[idx,:,:,:]/LA.norm(data[idx,:,:,:])       \n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca8be82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception(x,filters_1x1,filters_3x3_reduce,filters_3x3,filters_5x5_reduce,filters_5x5,filters_pool,activation_type='relu'):\n",
    "    path1 = Conv2D(filters_1x1,        (1, 1), padding='same', activation=activation_type)(x)\n",
    "    \n",
    "    path2 = Conv2D(filters_3x3_reduce, (1, 1), padding='same', activation=activation_type)(x)\n",
    "    path2 = Conv2D(filters_3x3,        (1, 1), padding='same', activation=activation_type)(path2)\n",
    "    \n",
    "    path3 = Conv2D(filters_5x5_reduce, (1, 1), padding='same', activation=activation_type)(x)\n",
    "    path3 = Conv2D(filters_5x5,        (1, 1), padding='same', activation=activation_type)(path3)\n",
    "    \n",
    "    path4 = MaxPool2D((3, 3),  strides=(1, 1), padding='same')(x)\n",
    "    path4 = Conv2D(filters_pool,       (1, 1), padding='same', activation=activation_type)(path4)\n",
    "    \n",
    "    return tf.concat([path1, path2, path3, path4], axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686f6ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auxiliary_classifier(x,num_classes,activation_type='relu'):\n",
    "    aux = AveragePooling2D((5, 5), strides=3)(x)\n",
    "    aux = Conv2D(128, 1, padding='same', activation=activation_type)(aux)\n",
    "    aux = Flatten()(aux)\n",
    "    aux = Dense(1024, activation=activation_type)(aux)\n",
    "    aux = Dropout(0.7)(aux)\n",
    "    aux = Dense(num_classes, activation='softmax')(aux)\n",
    "    return aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d8a8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import experimental as experimental\n",
    "def GoogleNetModel(data_num_rows, data_num_cols, num_input_chans=1, num_classes=NUM_VARIETIES, activation_type='relu', dropout_rate=0.0):\n",
    "\n",
    "    inp = Input(shape=(data_num_rows, data_num_cols, num_input_chans))\n",
    "    input_tensor = experimental.preprocessing.Resizing(224, 224, interpolation=\"bilinear\", input_shape=train_dataset.shape[1:])(inp)\n",
    "    x = Conv2D(64,  7, strides=2, padding='same', activation=activation_type)(input_tensor)\n",
    "    x = MaxPooling2D(3, strides=2)(x)\n",
    "    x = Conv2D(64,  1, strides=1, padding='same', activation=activation_type)(x)\n",
    "    x = Conv2D(192, 3, strides=1, padding='same', activation=activation_type)(x)\n",
    "    x = MaxPooling2D(3, strides=2)(x)\n",
    "    x = inception(x, filters_1x1=64 , filters_3x3_reduce=96 , filters_3x3=128, filters_5x5_reduce=16, filters_5x5=32, filters_pool=32)\n",
    "    x = inception(x, filters_1x1=128, filters_3x3_reduce=128, filters_3x3=192, filters_5x5_reduce=32, filters_5x5=96, filters_pool=64)\n",
    "    x = MaxPooling2D(3, strides=2)(x)\n",
    "    x = inception(x, filters_1x1=192, filters_3x3_reduce=96 , filters_3x3=208, filters_5x5_reduce=16, filters_5x5=48, filters_pool=64)\n",
    "    \n",
    "    aux1 = auxiliary_classifier(x,num_classes)\n",
    "    \n",
    "    x = inception(x, filters_1x1=160, filters_3x3_reduce=112, filters_3x3=224, filters_5x5_reduce=24, filters_5x5=64, filters_pool=64)\n",
    "    x = inception(x, filters_1x1=128, filters_3x3_reduce=128, filters_3x3=256, filters_5x5_reduce=24, filters_5x5=64, filters_pool=64)\n",
    "    x = inception(x, filters_1x1=112, filters_3x3_reduce=144, filters_3x3=288, filters_5x5_reduce=32, filters_5x5=64, filters_pool=64)\n",
    "    \n",
    "    aux2 = auxiliary_classifier(x,num_classes)\n",
    "    \n",
    "    x = inception(x, filters_1x1=256, filters_3x3_reduce=160, filters_3x3=320, filters_5x5_reduce=32, filters_5x5=128, filters_pool=128)\n",
    "    x = MaxPooling2D(3, strides=2)(x)\n",
    "    x = inception(x, filters_1x1=256, filters_3x3_reduce=160, filters_3x3=320, filters_5x5_reduce=32, filters_5x5=128, filters_pool=128)\n",
    "    x = inception(x, filters_1x1=384, filters_3x3_reduce=192, filters_3x3=384, filters_5x5_reduce=48, filters_5x5=128, filters_pool=128)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    out = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    return Model(inputs = inp, outputs = [out, aux1, aux2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3528776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGoogleNetModel():\n",
    "    learning_rate_base = LEARNING_RATE_BASE\n",
    "    activation_type = ACTIVATION_TYPE\n",
    "    wheat_types =  VARIETIES\n",
    "    num_classes = len(wheat_types)\n",
    "    dropout_rate = 0.4\n",
    "    print(\"--------------Load Data--------------\")\n",
    "\n",
    "    x_training = np.array(train_dataset)\n",
    "    labels_training = np.array(train_dataset_label)\n",
    "    \n",
    "    # Normalize the data\n",
    "    x_training = normalizeDataWholeSeed(x_training)\n",
    "    \n",
    "    # Extract some information\n",
    "    num_train = x_training.shape[0]\n",
    "    N_spatial = x_training.shape[1:3]\n",
    "    N_channel = x_training.shape[3]\n",
    "    \n",
    "    print(\"--------------Done--------------\")\n",
    "    \n",
    "    ############ Create a model ############\n",
    "    print(\"--------------Create a model--------------\")\n",
    "    \n",
    "    # Generate a model\n",
    "    model = GoogleNetModel(data_num_rows = N_spatial[0], \n",
    "                           data_num_cols = N_spatial[1],\n",
    "                           num_input_chans = N_channel, \n",
    "                           num_classes = num_classes,\n",
    "                           activation_type = activation_type,\n",
    "                           dropout_rate = dropout_rate)\n",
    "\n",
    "    # Compile the model\n",
    "    adam_opt = Adam(learning_rate=LEARNING_RATE_BASE, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.01)\n",
    "    model.compile(optimizer=adam_opt, loss=[losses.sparse_categorical_crossentropy,losses.sparse_categorical_crossentropy,losses.sparse_categorical_crossentropy],loss_weights=[1, 0.3, 0.3],metrics=['accuracy'])\n",
    "    print(\"---------Completed---------\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0c2868",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(df,title,xlabel,ylabel,values=['loss'],legends=[]):\n",
    "    \n",
    "    for value in values:\n",
    "        epoch_count = range(1, len(df.index) + 1)\n",
    "        plt.plot(epoch_count, df[value].tolist())\n",
    "    plt.title(title)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xlabel(xlabel)\n",
    "    if legends==[]:\n",
    "        legends = values\n",
    "    plt.legend(legends, loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df49795",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = getGoogleNetModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89c2d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df8cc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "x_val = []\n",
    "y_val = []\n",
    "\n",
    "for i in range(len(train_dataset)):\n",
    "    if i%5==0:\n",
    "        x_val.append(train_dataset[i])\n",
    "        y_val.append(train_dataset_label[i])\n",
    "    else:\n",
    "        x_train.append(train_dataset[i])\n",
    "        y_train.append(train_dataset_label[i])\n",
    "        \n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "y_train = [y_train,y_train,y_train]\n",
    "\n",
    "x_val = np.array(x_val)\n",
    "y_val = np.array(y_val)\n",
    "y_val = [y_val,y_val,y_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e30efad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eva = []\n",
    "test_eva = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd5eaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dataframe = pd.DataFrame() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863f63f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = start_timer()\n",
    "for x in range(EPOCHS):\n",
    "    print(\"\\nEpoch: \",x+1)\n",
    "    model.fit(x_train ,y_train ,batch_size=BATCH_SIZE ,epochs=1, verbose=2, validation_data=(x_val, y_val), shuffle=True)\n",
    "    if history_dataframe.size == 0:\n",
    "        history_dataframe = pd.DataFrame.from_dict(history.history)\n",
    "    else:\n",
    "        history_dataframe = pd.concat([history_dataframe,pd.DataFrame.from_dict(history.history)],axis=0,ignore_index=True)\n",
    "    print(\"for testing\")\n",
    "    print(model.evaluate(test_dataset,test_dataset_label))\n",
    "    print(\"for training\")\n",
    "    print(model.evaluate(train_dataset,train_dataset_label))\n",
    "toc = end_timer()\n",
    "show_time(tic,toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec37c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8463e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eva_df = pd.DataFrame(train_eva,columns=['loss','dense_4_loss','dense_1_loss','dense_3_loss','dense_4_accuracy','dense_1_accuracy','dense_3_accuracy'])\n",
    "test_eva_df = pd.DataFrame(test_eva,columns=['loss','dense_4_loss','dense_1_loss','dense_3_loss','dense_4_accuracy','dense_1_accuracy','dense_3_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c781fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graph(train_eva_df,'Model Train Loss','Epoch','Loss',['loss'],['train_loss'])\n",
    "plot_graph(test_eva_df,'Model Test Loss','Epoch','Loss',['loss'],['test_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1626ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graph(train_eva_df,'Model Train Accuracy','Epoch','Dense Accuracy',['dense_4_accuracy', 'dense_1_accuracy', 'dense_3_accuracy'])\n",
    "plot_graph(test_eva_df,'Model Test Accuracy','Epoch','Dense Accuracy',['dense_4_accuracy', 'dense_1_accuracy', 'dense_3_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae448b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graph(train_eva_df,'Model Train Loss','Epoch','Dense Loss',['dense_4_loss', 'dense_1_loss', 'dense_3_loss'])\n",
    "plot_graph(test_eva_df,'Model Test Loss','Epoch','Dense Loss',['dense_4_loss', 'dense_1_loss', 'dense_3_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b6c1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graph(history_dataframe,'Model Loss','Epoch','Loss',['loss','val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f027f1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graph(history_dataframe,'Model Train Accuracy','Epoch','Dense Accuracy',['dense_4_accuracy', 'dense_1_accuracy', 'dense_3_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70409bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graph(history_dataframe,'Model Train Loss','Epoch','Dense Loss',['dense_4_loss', 'dense_1_loss', 'dense_3_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332856c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graph(history_dataframe,'Model Val Accuracy','Epoch','Validation Accuracy',['val_dense_4_accuracy', 'val_dense_1_accuracy', 'val_dense_3_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd5ecbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graph(history_dataframe,'Model Val Loss','Epoch','Validation Loss',['val_dense_4_loss', 'val_dense_1_loss', 'val_dense_3_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a809b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
