{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f93eb968",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-03 18:01:42.235566: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-03 18:01:43.166635: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c004aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,timeit\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9824e5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dd26930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f72da7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import platform\n",
    "DATA_DIRECTORY = \"\"\n",
    "SLASH = \"\"\n",
    "if platform == \"linux\" or platform == \"linux2\":\n",
    "    DATA_DIRECTORY = \"/home/tyagi/Desktop/wheat/data/BULK/\"\n",
    "    SLASH = \"/\"\n",
    "elif platform == \"win32\":\n",
    "    DATA_DIRECTORY = \"D:\\mvl\\wheat\\data\\BULK\\\\\"\n",
    "    SLASH=\"\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5979451f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constants\n",
    "BAND_NUMBER = 60\n",
    "FILLED_AREA_RATIO = 0.90\n",
    "TOTAL_IMAGE_COUNT = 2400\n",
    "IMAGE_COUNT = int(TOTAL_IMAGE_COUNT/4)\n",
    "NUM_VARIETIES = 4\n",
    "NUM_OF_BANDS = 15\n",
    "\n",
    "REMOVE_NOISY_BANDS = False\n",
    "FIRST_BAND = 15\n",
    "LAST_BAND = 161\n",
    "\n",
    "IMAGE_WIDTH = 30\n",
    "IMAGE_HEIGHT = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06331561-95ec-46b3-b3b1-0754d38c5c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class filter_method(Enum):\n",
    "    none = 0\n",
    "    snv = 1\n",
    "    msc = 2\n",
    "    savgol = 3\n",
    "    \n",
    "FILTER = filter_method(1).name\n",
    "\n",
    "# to be set if filter chosen is savgol\n",
    "WINDOW = 7\n",
    "ORDER = 2\n",
    "DERIVATIVE = \"none\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "737284cd-da62-4bf5-9eb4-0a649f0834f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    " \n",
    "class feature_extraction_method(Enum):\n",
    "    none = 0\n",
    "    pca_loading = 1\n",
    "    lda = 2\n",
    "    ipca = 3\n",
    "\n",
    "FEATURE_EXTRACTION = feature_extraction_method(0).name\n",
    "\n",
    "NUM_OF_BANDS = 3\n",
    "if FEATURE_EXTRACTION == \"pca_loading\" or FEATURE_EXTRACTION == \"ipca\":\n",
    "    NUM_OF_BANDS = 8\n",
    "elif FEATURE_EXTRACTION == \"lda\":\n",
    "    NUM_OF_BANDS = 3\n",
    "    assert NUM_OF_BANDS <= min(NUM_VARIETIES-1,168),\"NUM_OF_BANDS is greater.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8db0be23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List for All varieties\n",
    "VARIETIES = []\n",
    "VARIETIES_CODE = {}\n",
    "\n",
    "for name in os.listdir(DATA_DIRECTORY):\n",
    "    if (name.endswith(\".hdr\") or name.endswith(\".bil\")):\n",
    "        continue\n",
    "    VARIETIES_CODE[name] = len(VARIETIES)\n",
    "    VARIETIES.append(name)\n",
    "    if len(VARIETIES)==NUM_VARIETIES:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e61072a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_timer():\n",
    "    print(\"Testing started\")\n",
    "    return timeit.default_timer()\n",
    "\n",
    "def end_timer():\n",
    "    return timeit.default_timer()\n",
    "\n",
    "def show_time(tic,toc): \n",
    "    test_time = toc - tic\n",
    "    print('Testing time (s) = ' + str(test_time) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72409e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_file_name(variety):\n",
    "    name = \"./dataset/V\"+str(variety).zfill(3)+\"_IC_\"+str(TOTAL_IMAGE_COUNT).zfill(5)+\"_FilledArea_\"+str(FILLED_AREA_RATIO)+\"_NumOfBands_\"+str(NUM_OF_BANDS)+\"_FB_\"+str(FIRST_BAND)+\"_LB_\"+str(LAST_BAND)+\"_BandNo_\"+str(BAND_NUMBER)+\"_ImageHeight_\"+str(IMAGE_HEIGHT)+\"_ImageWidth_\"+str(IMAGE_WIDTH)+\"_FILTER_\"+str(FILTER)+\"_FeatureExtraction_\"+str(FEATURE_EXTRACTION)\n",
    "    if REMOVE_NOISY_BANDS:\n",
    "        name+=\"_REMOVE_NOISY_BANDS_\"+str(REMOVE_NOISY_BANDS)\n",
    "    if FILTER == \"savgol\":\n",
    "        name+=\"_WINDOW_\"+str(WINDOW)+\"_ORDER_\"+str(ORDER)\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb9af0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx:  0\n",
      "idx:  1\n",
      "idx:  2\n",
      "idx:  3\n"
     ]
    }
   ],
   "source": [
    "train_dataset = []\n",
    "train_dataset_label = []\n",
    "test_dataset=[]\n",
    "test_dataset_label = []\n",
    "\n",
    "for idx, v in enumerate(VARIETIES):\n",
    "    print(\"idx: \",idx)\n",
    "    if idx >= NUM_VARIETIES:\n",
    "        break\n",
    "    train_dataset= train_dataset + np.load(dataset_file_name(v)+\"_train_dataset.npy\").tolist()\n",
    "    train_dataset_label = train_dataset_label + np.load(dataset_file_name(v)+\"_train_dataset_label.npy\").tolist()\n",
    "    test_dataset = test_dataset + np.load(dataset_file_name(v)+\"_test_dataset.npy\").tolist()\n",
    "    test_dataset_label = test_dataset_label + np.load(dataset_file_name(v)+\"_test_dataset_label.npy\").tolist()\n",
    "    \n",
    "train_dataset = np.array(train_dataset)\n",
    "train_dataset_label = np.array(train_dataset_label)\n",
    "test_dataset = np.array(test_dataset)\n",
    "test_dataset_label = np.array(test_dataset_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f4701f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73fb5fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamples, nx, ny,nb = train_dataset.shape\n",
    "nsamplestest, nx, ny,nb = test_dataset.shape\n",
    "d2_train_dataset = train_dataset.reshape((nsamples,nx*ny*nb))\n",
    "d2_test_dataset = test_dataset.reshape((nsamplestest,nx*ny*nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab71c240-8a62-42f1-9057-b4ba28064876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the images to 2D for Naive Bayes implementation\n",
    "X_train_flat = train_dataset.reshape(-1, 30 * 30 * 168)\n",
    "X_test_flat = test_dataset.reshape(-1, 30 * 30 * 168)\n",
    "\n",
    "# Create TensorFlow placeholders for input data and labels\n",
    "input_images = tf.placeholder(tf.float32, shape=(None, 30 * 30 * 168))\n",
    "input_labels = tf.placeholder(tf.int32, shape=(None,))\n",
    "\n",
    "# Calculate the prior probabilities\n",
    "unique_labels, label_counts = np.unique(train_dataset_label, return_counts=True)\n",
    "prior_probs = label_counts / len(train_dataset_label)\n",
    "\n",
    "# Calculate the likelihoods assuming Gaussian distribution for each feature and each class\n",
    "num_classes = 4\n",
    "likelihoods = []\n",
    "for label in range(num_classes):\n",
    "    label_images = X_train_flat[train_dataset_label == label]\n",
    "    label_likelihood = tf.reduce_mean(label_images, axis=0)\n",
    "    likelihoods.append(label_likelihood)\n",
    "\n",
    "# Convert likelihoods to TensorFlow tensors\n",
    "likelihoods = tf.convert_to_tensor(likelihoods)\n",
    "\n",
    "# Classify new images using the Naive Bayes classifier\n",
    "log_likelihoods = tf.math.log(likelihoods)\n",
    "log_priors = tf.math.log(prior_probs)\n",
    "log_posteriors = log_likelihoods + log_priors\n",
    "predictions = tf.argmax(log_posteriors, axis=0)\n",
    "\n",
    "# Calculate accuracy for evaluation\n",
    "correct_predictions = tf.equal(predictions, input_labels)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.01\n",
    "num_epochs = 50\n",
    "batch_size = 32\n",
    "\n",
    "# Define the loss function (e.g., cross-entropy) and optimizer (e.g., gradient descent)\n",
    "loss = ...  # Define your loss function here\n",
    "optimizer = ...  # Define your optimizer here\n",
    "\n",
    "# Create a TensorFlow session\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_batches = len(X_train_flat) // batch_size\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        avg_loss = 0.0\n",
    "        for batch in range(num_batches):\n",
    "            start_idx = batch * batch_size\n",
    "            end_idx = (batch + 1) * batch_size\n",
    "            batch_X = X_train_flat[start_idx:end_idx]\n",
    "            batch_y = train_dataset_label[start_idx:end_idx]\n",
    "\n",
    "            _, batch_loss = sess.run([optimizer, loss], feed_dict={input_images: batch_X, input_labels: batch_y})\n",
    "            avg_loss += batch_loss\n",
    "\n",
    "        avg_loss /= num_batches\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Average Loss: {avg_loss}\")\n",
    "\n",
    "    # Evaluation\n",
    "    test_accuracy = sess.run(accuracy, feed_dict={input_images: X_test_flat, input_labels: test_dataset_label})\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a03e37c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY OF THE MODEL:  0.9401041666666666\n"
     ]
    }
   ],
   "source": [
    "# performing predictions on the test dataset\n",
    "y_pred = svm_model.predict(d2_test_dataset)\n",
    "\n",
    "print(\"ACCURACY OF THE MODEL: \", accuracy_score(test_dataset_label, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4f4cbe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[451   0  29   0]\n",
      " [  0 446   0  34]\n",
      " [ 12   0 468   0]\n",
      " [  0  40   0 440]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(test_dataset_label, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4abb46f7-a5ab-4901-a90e-4b61cc73a3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_poly = SVC(kernel='poly', degree=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e407d80-1702-4ddd-bc15-9e17fbbdcdbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing started\n",
      "Testing time (s) = 4496.371396896997\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tic = start_timer()\n",
    "svm_poly.fit(d2_train_dataset, train_dataset_label)\n",
    "toc = end_timer()\n",
    "show_time(tic,toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e575332-dcba-4fa2-a73b-d2ebcd27cac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY OF THE MODEL:  0.3984375\n"
     ]
    }
   ],
   "source": [
    "# performing predictions on the test dataset\n",
    "y_pred = svm_poly.predict(d2_test_dataset)\n",
    "\n",
    "print(\"ACCURACY OF THE MODEL: \", accuracy_score(test_dataset_label, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d98273d-0937-4bb9-a853-a1cfed75939c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[257  85  63  75]\n",
      " [135 122 119 104]\n",
      " [ 66  91 211 112]\n",
      " [ 83  98 124 175]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(test_dataset_label, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc70c7a-6e5c-4c45-9b60-225e8a970448",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
