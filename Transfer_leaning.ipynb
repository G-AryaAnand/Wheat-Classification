{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b605089",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6cc1a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras as keras\n",
    "from keras import layers as layers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c004aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from skimage.filters import threshold_otsu\n",
    "import matplotlib.pyplot as plt\n",
    "from math import inf as inf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c9300cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spectral.io import envi as envi\n",
    "from spectral import imshow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee790ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3dd26930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f72da7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import platform\n",
    "DATA_DIRECTORY = \"\"\n",
    "\n",
    "if platform == \"linux\" or platform == \"linux2\":\n",
    "    DATA_DIRECTORY = \"\\mnt\\\\\"\n",
    "elif platform == \"win32\":\n",
    "    DATA_DIRECTORY = \"D:\\mvl\\wheat\\data\\BULK\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5979451f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing Constants\n",
    "TESTING = False\n",
    "\n",
    "#Constants\n",
    "BAND_NUMBER = 60\n",
    "FILLED_AREA_RATIO = 0.85\n",
    "IMAGE_COUNT = 100\n",
    "NUM_VARIETIES = 4\n",
    "NUM_OF_BANDS = 20\n",
    "FIRST_BAND = 21\n",
    "LAST_BAND = 149\n",
    "FACTOR = 2\n",
    "IMAGE_WIDTH = 40\n",
    "IMAGE_HEIGHT = 40\n",
    "\n",
    "NUM_EPOCHS = int(20 * FACTOR)\n",
    "ACTIVATION_TYPE = 'ReLU'\n",
    "BATCH_SIZE = 2*NUM_VARIETIES\n",
    "LEARNING_RATE_BASE = 0.0001/FACTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5e32422",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exactPathHDR(variety,file):\n",
    "    return DATA_DIRECTORY+variety+\"\\\\\"+file+\".bil.hdr\"\n",
    "\n",
    "def exactPathBIL(variety,file):\n",
    "    return DATA_DIRECTORY+variety+\"\\\\\"+file+\".bil\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10991a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getROI(img,band_number):\n",
    "    img_band = img.read_band(band_number)\n",
    "    threshold = threshold_otsu(img_band)\n",
    "    roi=[]\n",
    "    for x in range(img_band.shape[0]):\n",
    "        a=[]\n",
    "        for y in range(img_band.shape[1]):\n",
    "            if img_band[x][y]>threshold:\n",
    "                a.append(1)\n",
    "            else:\n",
    "                a.append(0)\n",
    "        roi.append(a)\n",
    "    return roi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f92ed01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns range for x and y from where we have to crop images\n",
    "def getRangeXandY(img,band_number):\n",
    "    img_band = img.read_band(band_number)\n",
    "    roi = getROI(img,band_number)\n",
    "    xmin = inf\n",
    "    xmax = 0\n",
    "    ymin = inf\n",
    "    ymax = 0\n",
    "    for x in range(img_band.shape[0]):\n",
    "        for y in range(img_band.shape[1]):\n",
    "            if roi[x][y]==1:\n",
    "                if x<xmin:\n",
    "                    xmin=x\n",
    "                if x>xmax:\n",
    "                    xmax=x\n",
    "                if y<ymin:\n",
    "                    ymin=y\n",
    "                if y>ymax:\n",
    "                    ymax=y\n",
    "    return xmin, xmax, ymin, ymax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09705166",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCroppedImage(img,band_number):\n",
    "    xmin, xmax, ymin, ymax = getRangeXandY(img,band_number)\n",
    "    new_img = img[xmin:xmax, ymin:ymax, :]\n",
    "    return new_img    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97d1f20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCroppedROI(img,band_number):\n",
    "    xmin, xmax, ymin, ymax = getRangeXandY(img,band_number)\n",
    "    roi = np.array(getROI(img,band_number))\n",
    "    roi = roi[xmin:xmax, ymin:ymax]\n",
    "    return roi   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "318efc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUsefulImage(img,band_number):\n",
    "    crop_img = getCroppedImage(img,band_number)\n",
    "    crop_roi = getCroppedROI(img,band_number)\n",
    "    for x in range(crop_img.shape[2]):\n",
    "        band = crop_img[:,:,x]\n",
    "        crop_img[:,:,x] = band*crop_roi\n",
    "    return crop_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad2376a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential([\n",
    "    layers.RandomCrop(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
    "    layers.RandomRotation(factor=(-0.1, 0.1)),\n",
    "    layers.RandomZoom(height_factor=(-0.1, 0.1), width_factor=(-0.1,0.1)),\n",
    "    layers.RandomFlip(mode=\"horizontal_and_vertical\", seed=None)\n",
    "])\n",
    "\n",
    "def getAugumentedImage(img,band_number):\n",
    "    new_img = getUsefulImage(img,band_number)\n",
    "    augmented_image = data_augmentation(new_img) \n",
    "    return augmented_image\n",
    "\n",
    "def checkAugumentedImage(augmented_image):\n",
    "    aug_band = augmented_image[:,:,0]\n",
    "    filled_area_ratio = (np.count_nonzero(aug_band))/(aug_band.shape[0]*aug_band.shape[1])\n",
    "    if filled_area_ratio > FILLED_AREA_RATIO :\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad47ac5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dimensional Reduction Method\n",
    "def DL_Method(HSI, numComponents = NUM_OF_BANDS):\n",
    "    RHSI = np.reshape(HSI, (-1, HSI.shape[2]))\n",
    "    n_batches = 10\n",
    "    inc_pca = IncrementalPCA(n_components=numComponents)\n",
    "    for X_batch in np.array_split(RHSI, n_batches):\n",
    "        inc_pca.partial_fit(X_batch)\n",
    "    X_ipca = inc_pca.transform(RHSI)\n",
    "    RHSI = np.reshape(X_ipca, (HSI.shape[0],HSI.shape[1], numComponents))\n",
    "    return RHSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e16e403c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List for All varieties\n",
    "VARIETIES = []\n",
    "VARIETIES_CODE = {}\n",
    "\n",
    "for name in os.listdir(DATA_DIRECTORY):\n",
    "    if (name.endswith(\".hdr\") or name.endswith(\".bil\")):\n",
    "        continue\n",
    "    VARIETIES_CODE[name] = len(VARIETIES)\n",
    "    VARIETIES.append(name)\n",
    "    if len(VARIETIES)==NUM_VARIETIES:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20b64642",
   "metadata": {},
   "outputs": [],
   "source": [
    "#List for all file names in varities\n",
    "FILES = []\n",
    "MAX_FILE_NUM = 4\n",
    "for x in range(1,MAX_FILE_NUM+1):\n",
    "    FILES.append(\"B_\"+str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a072b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of all images\n",
    "images = []\n",
    "images_label = []\n",
    "for v in VARIETIES:\n",
    "    for f in FILES:\n",
    "        try:\n",
    "            img = envi.open(exactPathHDR(v,f),exactPathBIL(v,f))\n",
    "            images.append(img)\n",
    "            images_label.append(v)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c910c23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = []\n",
    "train_dataset_label = []\n",
    "test_dataset = []\n",
    "test_dataset_label = []\n",
    "\n",
    "for index, img in enumerate(images):\n",
    "    count = 0\n",
    "    label = images_label[index]\n",
    "    while count<IMAGE_COUNT:\n",
    "        aug_img = getAugumentedImage(img,BAND_NUMBER)\n",
    "        \n",
    "        if checkAugumentedImage(aug_img):\n",
    "            aug_img = DL_Method(aug_img[:,:,FIRST_BAND:LAST_BAND+1])\n",
    "            if count%5 == 0:\n",
    "                test_dataset.append(aug_img)\n",
    "                test_dataset_label.append(label)\n",
    "            else:\n",
    "                train_dataset.append(aug_img)\n",
    "                train_dataset_label.append(label)\n",
    "            count+=1  \n",
    "            \n",
    "    if TESTING:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d65e88c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n"
     ]
    }
   ],
   "source": [
    "for index,data in enumerate(test_dataset):\n",
    "#     imshow(data)\n",
    "    print(test_dataset_label[index])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5fc54f45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW 187\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "DBW222\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "HD 3086\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n",
      "PBW 291\n"
     ]
    }
   ],
   "source": [
    "for index,data in enumerate(train_dataset):\n",
    "#     imshow(data)\n",
    "    print(train_dataset_label[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e6c0621",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "# import math, sys, pdb, os\n",
    "import keras\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Activation, BatchNormalization, Add, Conv2DTranspose, Flatten, Dense, Conv1D, AveragePooling2D, LeakyReLU, PReLU, GlobalAveragePooling2D\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers import concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "import os, pdb, timeit\n",
    "import numpy as np\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "import matplotlib.cm as cm\n",
    "import cv2\n",
    "from keras import activations\n",
    "import vis\n",
    "# from vis.visualization import visualize_saliency, overlay\n",
    "# from vis.utils import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f98cd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeDataWholeSeed(data,normalization_type='max'):\n",
    "    \n",
    "    if normalization_type == 'max':\n",
    "        for idx in range(data.shape[0]):\n",
    "            data[idx,:,:,:] = data[idx,:,:,:]/np.max(abs(data[idx,:,:,:]))\n",
    "            \n",
    "    elif normalization_type == 'l2norm':\n",
    "        from numpy import linalg as LA\n",
    "        for idx in range(data.shape[0]):\n",
    "            data[idx,:,:,:] = data[idx,:,:,:]/LA.norm(data[idx,:,:,:]) # L2-norm by default        \n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ce22266",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hyperparam_string(USE_DATA_AUG, learning_rate_base, batch_size, kernel_size, dropout_rate, num_training,\n",
    "                           num_nodes_fc, activation_type):\n",
    "    hparam = \"\"\n",
    "\n",
    "    # Hyper-parameters\n",
    "    if USE_DATA_AUG:\n",
    "        hparam += \"AUG_\"\n",
    "\n",
    "    hparam += str(num_nodes_fc) + \"nodes_\" + str(learning_rate_base) + \"lr_\" + str(batch_size) + \"batch_\" + str(\n",
    "        kernel_size) + \"kernel_\" + str(dropout_rate) + \"drop_\" + str(\n",
    "        num_training) + \"train_\" + activation_type\n",
    "\n",
    "    return hparam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1fd4a22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "\n",
    "    import itertools\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.clim(0,sum(cm[0,:]))\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d67d07a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_K_classification_accuracy(y_predicted, y_true, K=1):\n",
    "\n",
    "    num_samples = y_predicted.shape[0]\n",
    "    num_classes = y_predicted.shape[1]\n",
    "\n",
    "    if K > num_classes:\n",
    "        sys.exit(1)\n",
    "\n",
    "    temp = np.zeros((num_samples,))\n",
    "\n",
    "    for idx in range(num_samples):\n",
    "        curr_predicted = np.argsort(y_predicted[idx,:])\n",
    "        curr_predicted = curr_predicted[::-1] # descending\n",
    "\n",
    "        if y_true[idx] in curr_predicted[:K]:\n",
    "            temp[idx] = 1\n",
    "\n",
    "    return 100.0 * np.sum(temp)/num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "351725d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2D_ResNet(x, kernel_size, activation_type, dropout_rate, num_filters_first_conv1D):\n",
    "\n",
    "    x_orig = x\n",
    "\n",
    "    # Batch norm\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # 1x1 Conv2D\n",
    "    x = Conv2D(num_filters_first_conv1D, kernel_size=1, activation=None, use_bias=False, padding='same',\n",
    "               kernel_initializer='truncated_normal')(x)\n",
    "\n",
    "    # Activation\n",
    "    if activation_type == 'LeakyReLU':\n",
    "        x = LeakyReLU()(x)\n",
    "    elif activation_type == 'PReLU':\n",
    "        x = PReLU()(x)\n",
    "    else:\n",
    "        x = Activation(activation_type)(x)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # 3x3 Conv2D\n",
    "    x = Conv2D(num_filters_first_conv1D, kernel_size, activation=None, use_bias=True, padding='same',\n",
    "               kernel_initializer='truncated_normal')(x)\n",
    "\n",
    "    # Activation\n",
    "    if activation_type == 'LeakyReLU':\n",
    "        x = LeakyReLU()(x)\n",
    "    elif activation_type == 'PReLU':\n",
    "        x = PReLU()(x)\n",
    "    else:\n",
    "        x = Activation(activation_type)(x)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # 1x1 Conv2D\n",
    "    x = Conv2D(num_filters_first_conv1D*4, kernel_size=1, activation=None, use_bias=False, padding='same',\n",
    "               kernel_initializer='truncated_normal')(x)\n",
    "\n",
    "    # Skip connection\n",
    "    if int(x.shape[3]) != int(x_orig.shape[3]):\n",
    "        x_orig = Conv2D(int(x.shape[3]), kernel_size=1, activation=None, use_bias=False, padding='same',\n",
    "               kernel_initializer='truncated_normal')(x_orig)\n",
    "\n",
    "    # Activation\n",
    "    if activation_type == 'LeakyReLU':\n",
    "        x = LeakyReLU()(x)\n",
    "    elif activation_type == 'PReLU':\n",
    "        x = PReLU()(x)\n",
    "    else:\n",
    "        x = Activation(activation_type)(x)\n",
    "\n",
    "    x = Add()([x, x_orig])\n",
    "\n",
    "    # Dropout\n",
    "    return Dropout(dropout_rate)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0731a5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createBlock_ResNet2D(x, num_layers, kernel_size, activation_type, dropout_rate, num_filters_first_conv1D):\n",
    "\n",
    "    for idx_layer in range(num_layers):\n",
    "\n",
    "        x = conv2D_ResNet(x, kernel_size, activation_type, dropout_rate, num_filters_first_conv1D)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "45bd16db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# growth_rate: number of filters for each normal convolution ('k' in the paper)\n",
    "def ResNet2D_classifier(data_num_rows, data_num_cols, num_classes, kernel_size=3, num_layers_each_block=[6, 12, 24, 16],\n",
    "                        num_chan_per_block = [64,128,256,512], activation_type='swish', dropout_rate=0.0, num_input_chans=1, num_nodes_fc=64):\n",
    "\n",
    "    input_data = Input(shape=(data_num_rows, data_num_cols, num_input_chans))\n",
    "\n",
    "    # Input layer: Conv2D -> activation\n",
    "    x = Conv2D(num_chan_per_block[0], kernel_size, activation=None, use_bias=True, padding='same',\n",
    "               kernel_initializer='truncated_normal')(input_data)\n",
    "\n",
    "    # Activation\n",
    "    if activation_type == 'LeakyReLU':\n",
    "        x = LeakyReLU()(x)\n",
    "    elif activation_type == 'PReLU':\n",
    "        x = PReLU()(x)\n",
    "    else:\n",
    "        x = Activation(activation_type)(x)\n",
    "\n",
    "    #  Blocks & Downsampling Layers\n",
    "    for idx_block in range(len(num_layers_each_block)):\n",
    "        x = createBlock_ResNet2D(x, num_layers_each_block[idx_block], kernel_size, activation_type, dropout_rate,\n",
    "                                 num_chan_per_block[idx_block])\n",
    "\n",
    "        x = BatchNormalization()(x)\n",
    "\n",
    "        if idx_block != len(num_layers_each_block)-1:\n",
    "            x = Conv2D(num_chan_per_block[idx_block]*2, kernel_size, strides = 2, activation=None, use_bias=True, padding='valid',\n",
    "                   kernel_initializer='truncated_normal')(x)\n",
    "        else:\n",
    "            x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    # Output layer\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(units=num_nodes_fc, activation=None, kernel_initializer='truncated_normal')(x)\n",
    "\n",
    "    # Activation\n",
    "    if activation_type == 'LeakyReLU':\n",
    "        x = LeakyReLU()(x)\n",
    "    elif activation_type == 'PReLU':\n",
    "        x = PReLU()(x)\n",
    "    else:\n",
    "        x = Activation(activation_type)(x)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    output_data = Dense(units=num_classes, activation='softmax', kernel_initializer='truncated_normal')(x)\n",
    "\n",
    "    return Model(inputs=input_data, outputs=output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2f40895c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model,dataset,dataset_label,normalization_type):\n",
    "    print(\"--------------Make Predictions--------------\")    \n",
    "    x = np.array(dataset)\n",
    "    labels = np.array(dataset_label)\n",
    "    labels = [VARIETIES_CODE[label] for label in labels]\n",
    "    \n",
    "    # Normalize the data\n",
    "    x = normalizeDataWholeSeed(x,normalization_type=normalization_type)\n",
    "    \n",
    "    num = x.shape[0]\n",
    "\n",
    "    print(\"Testing started\")\n",
    "    tic = timeit.default_timer()\n",
    "    labels_predicted = model.predict(x)\n",
    "    toc = timeit.default_timer()\n",
    "    test_time = toc - tic\n",
    "    print('Testing time (s) = ' + str(test_time) + '\\n')\n",
    "    \n",
    "    print(labels_predicted)\n",
    "    print(\"--------\")\n",
    "    # Classification accuracy\n",
    "    labels_integer_format = labels\n",
    "    labels_predicted_integer_format = np.argmax(labels_predicted, axis=1)\n",
    "\n",
    "    acc_top2 = top_K_classification_accuracy(labels_predicted, labels_integer_format, K=2)\n",
    "    acc_top1 = top_K_classification_accuracy(labels_predicted, labels_integer_format, K=1)\n",
    "    \n",
    "    # Confusion matrices\n",
    "    confusion_matrix_results = confusion_matrix(labels_integer_format, labels_predicted_integer_format)\n",
    "    print(\"Confusion matrix = \")\n",
    "    print(confusion_matrix_results)\n",
    "    print(\"------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5e4c541b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model,normalization_type):\n",
    "    evaluate(model,train_dataset,train_dataset_label,normalization_type)\n",
    "    \n",
    "    evaluate(model,test_dataset,test_dataset_label,normalization_type)\n",
    "    \n",
    "    \n",
    "    # Precision, Recall, F1\n",
    "#     macro_avg = np.asarray(\n",
    "#         precision_recall_fscore_support(labels_test_integer_format, labels_predicted_test_integer_format,\n",
    "#                                         average='macro'))\n",
    "#     macro_avg_precision = macro_avg[0]\n",
    "#     macro_avg_recall = macro_avg[1]\n",
    "#     macro_avg_fscore = macro_avg[2]\n",
    "\n",
    "#     print('Top-1 accuracy (%) = ' + str(acc_top1) + '\\n')\n",
    "#     print('Top-2 accuracy (%) = ' + str(acc_top2) + '\\n')\n",
    "#     print('Macro-avg precision = ' + str(macro_avg_precision) + '\\n')\n",
    "#     print('Macro-avg recall = ' + str(macro_avg_recall) + '\\n')\n",
    "#     print('Macro-avg f-score = ' + str(macro_avg_fscore) + '\\n')\n",
    "\n",
    "#     print(\"--------------Done--------------\")\n",
    "\n",
    "#     print(\"--------------Compute Saliency Maps--------------\")\n",
    "#     results_test_dir = os.path.join(results_dir, 'test')\n",
    "#     if not os.path.exists(results_test_dir):\n",
    "#         os.makedirs(results_test_dir)\n",
    "\n",
    "#     # Swap softmax with linear\n",
    "#     model.layers[-1].activation = activations.linear\n",
    "#     model = utils.apply_modifications(model)\n",
    "\n",
    "#     for idx_wheat in range(num_test):\n",
    "\n",
    "#         grads = visualize_saliency(model, layer_idx=-1, filter_indices=np.argmax(labels_test[idx_wheat, :], axis=0),\n",
    "#                                    seed_input=x_test[idx_wheat], backprop_modifier=None)\n",
    "\n",
    "#         ss_img = np.sqrt(np.sum(abs(x_test[idx_wheat, :, :, :]) ** 2, axis=2))\n",
    "#         ss_img /= np.max(ss_img)\n",
    "\n",
    "#         plt.figure(1)\n",
    "#         plt.subplot(3, 1, 1)\n",
    "#         plt.imshow(ss_img, cmap='gray')\n",
    "#         plt.clim(0, 1)\n",
    "#         plt.axis('off')\n",
    "#         plt.colorbar()\n",
    "\n",
    "#         plt.subplot(3, 1, 2)\n",
    "#         plt.imshow((grads * np.uint8(255)).astype('uint8'), cmap='jet')\n",
    "#         plt.clim(0, 255)\n",
    "#         plt.axis('off')\n",
    "#         plt.colorbar()\n",
    "\n",
    "#         jet_heatmap = np.uint8(cm.jet(grads)[..., :3] * np.uint8(255))\n",
    "\n",
    "#         plt.subplot(3, 1, 3)\n",
    "#         ss_img = cv2.cvtColor((ss_img * np.uint8(255)).astype('uint8'), cv2.COLOR_GRAY2RGB)\n",
    "#         plt.imshow(overlay(jet_heatmap, ss_img, alpha=0.3))\n",
    "#         plt.clim(0, 255)\n",
    "#         plt.axis('off')\n",
    "#         plt.colorbar()\n",
    "\n",
    "#         plt.savefig(os.path.join(results_test_dir, str(idx_wheat+1) + '.png'))\n",
    "#         plt.clf()\n",
    "\n",
    "#     print(\"--------------Done--------------\")\n",
    "\n",
    "#     print(\"--------------Save the information--------------\")\n",
    "\n",
    "#     # Write some information to files\n",
    "#     f = open(os.path.join(results_test_dir, 'testing_info.txt'), 'w')\n",
    "#     f.write(\"Wheat types = \" + str(wheat_types) + \"\\n\")\n",
    "#     f.write(\"Confusion matrix \\n\")\n",
    "#     f.write(str(confusion_matrix_results) + \"\\n\")\n",
    "#     f.write(\"Normalization type = \" + str(normalization_type) + \"\\n\")\n",
    "#     f.write(\"# test samples = %d \\n\" % (num_test))\n",
    "#     f.write(\"Top-1 test accuracy = %f \\n\" % (acc_top1))\n",
    "#     f.write(\"Top-2 test accuracy = %f \\n\" % (acc_top2))\n",
    "#     f.write(\"Macro-avg precision = %f \\n\" % (macro_avg_precision))\n",
    "#     f.write(\"Macro-avg recall = %f \\n\" % (macro_avg_recall))\n",
    "#     f.write(\"Macro-avg f-score = %f \\n\" % (macro_avg_fscore))\n",
    "#     f.write(\"Test time (s) = \" + str(test_time) + \"\\n\")\n",
    "#     f.close()\n",
    "\n",
    "#     # Save confusion matrices\n",
    "#     plt.figure(1)\n",
    "#     plot_confusion_matrix(confusion_matrix_results, classes=wheat_types, normalize=False, title='Confusion matrix')\n",
    "#     plt.savefig(os.path.join(results_test_dir,'confusionMatrix.png'))\n",
    "#     plt.clf()\n",
    "\n",
    "#     print(\"--------------Done--------------\")\n",
    "\n",
    "#     print(\"--------------Save the information for the training phase--------------\")\n",
    "    \n",
    "#     import pandas as pd\n",
    "    \n",
    "#     # Save the trained model\n",
    "#     model.save_weights(os.path.join(results_dir, 'trainedResNetB_weights.h5'))\n",
    "    \n",
    "#     # Extract the training loss   \n",
    "#     training_loss = hist.history['loss']\n",
    "\n",
    "#     # Save the training loss\n",
    "#     df = pd.DataFrame(data={'training loss': training_loss},index=np.arange(num_epochs)+1)\n",
    "#     df.to_csv(os.path.join(results_dir,'training_loss.csv'))\n",
    "    \n",
    "#     # Save the training loss as a figure\n",
    "#     plt.figure(1)\n",
    "#     plt.title('Loss')\n",
    "#     plt.plot(training_loss, color='b',label='Training')\n",
    "#     plt.legend()\n",
    "#     plt.grid()\n",
    "#     plt.savefig(os.path.join(results_dir,'training_loss.png'))\n",
    "#     plt.clf()   \n",
    "    \n",
    "#     # Write a file with general information\n",
    "#     f = open(os.path.join(results_dir,'training_info.txt'),'w')\n",
    "#     f.write(hparams + '\\n')\n",
    "#     f.write('Wheat types = ' + str(wheat_types)+'\\n')\n",
    "#     f.write('Training time (s) = %f \\n' %(training_time))\n",
    "#     f.write('Normalization type = ' + str(normalization_type)+ '\\n')\n",
    "#     f.write('# epochs = ' + str(num_epochs) + '\\n')\n",
    "#     f.write('# training samples = %d \\n' %(num_training))\n",
    "#     f.close()\n",
    "    \n",
    "#     print(\"--------------Done--------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6797662b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createAndTrainResNetB(params):\n",
    "                                        \n",
    "    ############ Extract params ############\n",
    "    USE_DATA_AUG = params['USE_DATA_AUG']\n",
    "    learning_rate_base = params['learning_rate_base']\n",
    "    kernel_size = params['kernel_size']\n",
    "    num_epochs = params['num_epochs']\n",
    "    batch_size = params['batch_size']\n",
    "    dropout_rate = params['dropout_rate']\n",
    "    activation_type = params['activation_type']\n",
    "    num_nodes_fc = params['num_nodes_fc']\n",
    "    wheat_types = params['wheat_types']\n",
    "    normalization_type = params['normalization_type']\n",
    "    num_layers_each_block = params['num_layers_each_block']\n",
    "    num_chan_per_block = params['num_chan_per_block']\n",
    "    N_classes = len(wheat_types)\n",
    "    \n",
    "    \n",
    "    ############ Load data ############\n",
    "    print(\"--------------Load Data--------------\")\n",
    "\n",
    "    # Load training data and their corresponding labels\n",
    "    x_training = np.array(train_dataset)\n",
    "    labels_training = np.array(train_dataset_label)\n",
    "    labels_training = [VARIETIES_CODE[label] for label in labels_training]\n",
    "    \n",
    "    # Normalize the data\n",
    "    x_training = normalizeDataWholeSeed(x_training,normalization_type=normalization_type)\n",
    "    \n",
    "    # Extract some information\n",
    "    num_training = x_training.shape[0]\n",
    "    N_spatial = x_training.shape[1:3]\n",
    "    N_bands = x_training.shape[3]\n",
    "    num_batch_per_epoch = int(num_training/batch_size)\n",
    "    \n",
    "    print('#training = %d' %(num_training))\n",
    "    print('#batches per epoch = %d' %(num_batch_per_epoch))\n",
    "    \n",
    "    print(\"--------------Done--------------\")\n",
    "    \n",
    "    \n",
    "    ############ Prepare the path for saving the models/stats ############\n",
    "    print(\"--------------Prepare a path for saving the models/stats--------------\")\n",
    "    \n",
    "    hparams = make_hyperparam_string(USE_DATA_AUG, learning_rate_base, batch_size, kernel_size, dropout_rate,\n",
    "                                     num_training, num_nodes_fc, activation_type)\n",
    "    print('Saving the model to...')\n",
    "    \n",
    "    results_dir = os.path.join(params['results_base_directory'],hparams)\n",
    "    \n",
    "    if not os.path.exists(results_dir):\n",
    "        os.makedirs(results_dir)\n",
    "    print(results_dir)\n",
    "\n",
    "    print(\"--------------Done--------------\")\n",
    "\n",
    "    ############ Create a model ############\n",
    "    print(\"--------------Create a model--------------\")\n",
    "    \n",
    "    # Generate a model\n",
    "    model = ResNet2D_classifier(data_num_rows=N_spatial[0], data_num_cols=N_spatial[1], num_classes=N_classes,\n",
    "                                kernel_size=kernel_size, num_layers_each_block=num_layers_each_block,\n",
    "                                num_chan_per_block=num_chan_per_block, activation_type=activation_type,\n",
    "                                dropout_rate=dropout_rate, num_input_chans=N_bands, num_nodes_fc=num_nodes_fc)\n",
    "\n",
    "    # Compile the model\n",
    "    adam_opt = Adam(lr=learning_rate_base / batch_size, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.01)\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=adam_opt, metrics=['acc'])\n",
    "\n",
    "    # Create a Tensorboard callback\n",
    "    tbCallBack = TensorBoard(log_dir=results_dir, histogram_freq=0, write_graph=False, write_images=False)\n",
    "    \n",
    "    print(\"--------------Done--------------\")\n",
    "\n",
    "    ############ Train the model ############\n",
    "    print(\"--------------Begin training the model--------------\")\n",
    "\n",
    "    # Possibly perform data augmentation\n",
    "    from keras.preprocessing.image import ImageDataGenerator\n",
    "    \n",
    "    if USE_DATA_AUG:\n",
    "        width_shift_range = 0.04\n",
    "        height_shift_range = 0.04\n",
    "        HORIZONTAL_FLIP = True\n",
    "        VERTICAL_FLIP = True\n",
    "        data_gen_args = dict(\n",
    "            rotation_range=0.,\n",
    "            width_shift_range=width_shift_range,\n",
    "            height_shift_range=height_shift_range,\n",
    "            horizontal_flip=HORIZONTAL_FLIP,\n",
    "            vertical_flip=VERTICAL_FLIP,\n",
    "            fill_mode = 'wrap')\n",
    "\n",
    "        image_datagen = ImageDataGenerator(**data_gen_args)\n",
    "    else:\n",
    "        image_datagen = ImageDataGenerator()\n",
    "\n",
    "    # Define a data generator to generate random batches\n",
    "    def myGenerator(batch_size):\n",
    "        for x_batch, y_batch in image_datagen.flow(x_training, labels_training, batch_size=batch_size, shuffle = True):\n",
    "            yield (x_batch, y_batch)\n",
    "\n",
    "    my_generator = myGenerator(batch_size)\n",
    "\n",
    "    tic = timeit.default_timer()\n",
    "    \n",
    "    # Train the model\n",
    "    hist = model.fit(my_generator, steps_per_epoch=num_batch_per_epoch, epochs = num_epochs, initial_epoch = 0, verbose=2, callbacks = [tbCallBack])\n",
    "\n",
    "    toc = timeit.default_timer()\n",
    "    training_time = toc-tic\n",
    "    print(\"Total training time = \" + str(training_time))\n",
    "    \n",
    "    print(\"--------------Done--------------\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0f8da36f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Parameters (mostly determined using validation datasets)\n",
    "params = dict()\n",
    "params['normalization_type'] = 'max'                    # Data normalization type\n",
    "params['wheat_types'] = VARIETIES\n",
    "params['num_epochs'] = NUM_EPOCHS                              # Number of epochs\n",
    "params['kernel_size'] = 3                               # Kernel size\n",
    "params['dropout_rate'] = 0.15                            # Dropout rate\n",
    "params['num_nodes_fc'] = 512                            # Number of  nodes in the fully-connected layers\n",
    "params['num_layers_each_block'] = [3, 3, 5, 3]         # Number of layers per block\n",
    "params['num_chan_per_block'] = [128, 128, 256, 256]     # Number of filters in the conv layers\n",
    "params['results_base_directory'] = './results/'  # Directory of saving results\n",
    "params['activation_type'] = ACTIVATION_TYPE\n",
    "# In our experiment where we have the full training set, we set\n",
    "# USE_DATA_AUG = True, learning_rate_base = 0.005, and batch_size = 4.\n",
    "# However, in this example script, we modified them to simplify our training process on this\n",
    "# example dataset\n",
    "params['batch_size'] = BATCH_SIZE                    # Batch size\n",
    "params['USE_DATA_AUG'] = False              # Use data augmentation (In the paper, we set it to True)\n",
    "params['learning_rate_base'] = LEARNING_RATE_BASE      # Initial learning rate (In the paper, we set it to 0.005)\n",
    "\n",
    "# Add 'swish' activation\n",
    "if params['activation_type'] == 'swish':\n",
    "\n",
    "    from tensorflow.keras.utils import get_custom_objects\n",
    "    import keras.backend as K\n",
    "\n",
    "    # Taken from https://github.com/dataplayer12/swish-activation/blob/master/MNIST/activations.ipynb\n",
    "    def swish(x):\n",
    "        beta = tf.Variable(initial_value=1.0,trainable=True)\n",
    "        return x*tf.nn.sigmoid(beta*x)\n",
    "\n",
    "    get_custom_objects().update({'swish': swish})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "54ac47e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------Load Data--------------\n",
      "#training = 1280\n",
      "#batches per epoch = 160\n",
      "--------------Done--------------\n",
      "--------------Prepare a path for saving the models/stats--------------\n",
      "Saving the model to...\n",
      "./results/512nodes_5e-05lr_8batch_3kernel_0.15drop_1280train_ReLU\n",
      "--------------Done--------------\n",
      "--------------Create a model--------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ganga\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\ganga\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\preprocessing\\image.py:766: UserWarning: NumpyArrayIterator is set to use the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3, or 4 channels on axis 3. However, it was passed an array with shape (1280, 40, 40, 20) (20 channels).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------Done--------------\n",
      "--------------Begin training the model--------------\n",
      "Epoch 1/40\n",
      "160/160 - 380s - loss: 1.7201 - acc: 0.2516 - 380s/epoch - 2s/step\n",
      "Epoch 2/40\n",
      "160/160 - 372s - loss: 1.7026 - acc: 0.2648 - 372s/epoch - 2s/step\n",
      "Epoch 3/40\n",
      "160/160 - 391s - loss: 1.7412 - acc: 0.2422 - 391s/epoch - 2s/step\n",
      "Epoch 4/40\n",
      "160/160 - 389s - loss: 1.6933 - acc: 0.2648 - 389s/epoch - 2s/step\n",
      "Epoch 5/40\n",
      "160/160 - 404s - loss: 1.6775 - acc: 0.2750 - 404s/epoch - 3s/step\n",
      "Epoch 6/40\n",
      "160/160 - 411s - loss: 1.6570 - acc: 0.2773 - 411s/epoch - 3s/step\n",
      "Epoch 7/40\n",
      "160/160 - 390s - loss: 1.6393 - acc: 0.2906 - 390s/epoch - 2s/step\n",
      "Epoch 8/40\n",
      "160/160 - 393s - loss: 1.6756 - acc: 0.2750 - 393s/epoch - 2s/step\n",
      "Epoch 9/40\n",
      "160/160 - 390s - loss: 1.6746 - acc: 0.2641 - 390s/epoch - 2s/step\n",
      "Epoch 10/40\n",
      "160/160 - 397s - loss: 1.6344 - acc: 0.2711 - 397s/epoch - 2s/step\n",
      "Epoch 11/40\n",
      "160/160 - 393s - loss: 1.6876 - acc: 0.2633 - 393s/epoch - 2s/step\n",
      "Epoch 12/40\n",
      "160/160 - 394s - loss: 1.6536 - acc: 0.2688 - 394s/epoch - 2s/step\n",
      "Epoch 13/40\n",
      "160/160 - 393s - loss: 1.6713 - acc: 0.2758 - 393s/epoch - 2s/step\n",
      "Epoch 14/40\n",
      "160/160 - 377s - loss: 1.6875 - acc: 0.2594 - 377s/epoch - 2s/step\n",
      "Epoch 15/40\n",
      "160/160 - 396s - loss: 1.6235 - acc: 0.2875 - 396s/epoch - 2s/step\n",
      "Epoch 16/40\n",
      "160/160 - 415s - loss: 1.6414 - acc: 0.2781 - 415s/epoch - 3s/step\n",
      "Epoch 17/40\n",
      "160/160 - 405s - loss: 1.6203 - acc: 0.2898 - 405s/epoch - 3s/step\n",
      "Epoch 18/40\n",
      "160/160 - 397s - loss: 1.6500 - acc: 0.2602 - 397s/epoch - 2s/step\n",
      "Epoch 19/40\n",
      "160/160 - 392s - loss: 1.6312 - acc: 0.2812 - 392s/epoch - 2s/step\n",
      "Epoch 20/40\n",
      "160/160 - 376s - loss: 1.6402 - acc: 0.2805 - 376s/epoch - 2s/step\n",
      "Epoch 21/40\n",
      "160/160 - 377s - loss: 1.6370 - acc: 0.2805 - 377s/epoch - 2s/step\n",
      "Epoch 22/40\n",
      "160/160 - 393s - loss: 1.6537 - acc: 0.2844 - 393s/epoch - 2s/step\n",
      "Epoch 23/40\n",
      "160/160 - 390s - loss: 1.6410 - acc: 0.2789 - 390s/epoch - 2s/step\n",
      "Epoch 24/40\n",
      "160/160 - 394s - loss: 1.6511 - acc: 0.2734 - 394s/epoch - 2s/step\n",
      "Epoch 25/40\n",
      "160/160 - 397s - loss: 1.6787 - acc: 0.2516 - 397s/epoch - 2s/step\n",
      "Epoch 26/40\n",
      "160/160 - 394s - loss: 1.6790 - acc: 0.2734 - 394s/epoch - 2s/step\n",
      "Epoch 27/40\n",
      "160/160 - 380s - loss: 1.7127 - acc: 0.2516 - 380s/epoch - 2s/step\n",
      "Epoch 28/40\n",
      "160/160 - 376s - loss: 1.6492 - acc: 0.2688 - 376s/epoch - 2s/step\n",
      "Epoch 29/40\n",
      "160/160 - 392s - loss: 1.6357 - acc: 0.2805 - 392s/epoch - 2s/step\n",
      "Epoch 30/40\n",
      "160/160 - 390s - loss: 1.6659 - acc: 0.2719 - 390s/epoch - 2s/step\n",
      "Epoch 31/40\n",
      "160/160 - 399s - loss: 1.6572 - acc: 0.2844 - 399s/epoch - 2s/step\n",
      "Epoch 32/40\n",
      "160/160 - 388s - loss: 1.5862 - acc: 0.3016 - 388s/epoch - 2s/step\n",
      "Epoch 33/40\n",
      "160/160 - 387s - loss: 1.6187 - acc: 0.2711 - 387s/epoch - 2s/step\n",
      "Epoch 34/40\n",
      "160/160 - 395s - loss: 1.6153 - acc: 0.2867 - 395s/epoch - 2s/step\n",
      "Epoch 35/40\n",
      "160/160 - 394s - loss: 1.5873 - acc: 0.2773 - 394s/epoch - 2s/step\n",
      "Epoch 36/40\n",
      "160/160 - 399s - loss: 1.5989 - acc: 0.2906 - 399s/epoch - 2s/step\n",
      "Epoch 37/40\n",
      "160/160 - 384s - loss: 1.6246 - acc: 0.2992 - 384s/epoch - 2s/step\n",
      "Epoch 38/40\n",
      "160/160 - 391s - loss: 1.6596 - acc: 0.2688 - 391s/epoch - 2s/step\n",
      "Epoch 39/40\n",
      "160/160 - 403s - loss: 1.6516 - acc: 0.2859 - 403s/epoch - 3s/step\n",
      "Epoch 40/40\n",
      "160/160 - 407s - loss: 1.6395 - acc: 0.2742 - 407s/epoch - 3s/step\n",
      "Total training time = 15688.19968900003\n",
      "--------------Done--------------\n"
     ]
    }
   ],
   "source": [
    "model = createAndTrainResNetB(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "74535028",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 40, 40, 20)  0           []                               \n",
      "                                ]                                                                 \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 40, 40, 128)  23168       ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 40, 40, 128)  0           ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 40, 40, 128)  512        ['activation[0][0]']             \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 40, 40, 128)  16384       ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 40, 40, 128)  0           ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 40, 40, 128)  512        ['activation_1[0][0]']           \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 40, 40, 128)  147584      ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 40, 40, 128)  0           ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 40, 40, 128)  512        ['activation_2[0][0]']           \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 40, 40, 512)  65536       ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 40, 40, 512)  0           ['conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 40, 40, 512)  65536       ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 40, 40, 512)  0           ['activation_3[0][0]',           \n",
      "                                                                  'conv2d_4[0][0]']               \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 40, 40, 512)  0           ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 40, 40, 512)  2048       ['dropout[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 40, 40, 128)  65536       ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 40, 40, 128)  0           ['conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 40, 40, 128)  512        ['activation_4[0][0]']           \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 40, 40, 128)  147584      ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 40, 40, 128)  0           ['conv2d_6[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 40, 40, 128)  512        ['activation_5[0][0]']           \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 40, 40, 512)  65536       ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 40, 40, 512)  0           ['conv2d_7[0][0]']               \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 40, 40, 512)  0           ['activation_6[0][0]',           \n",
      "                                                                  'dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 40, 40, 512)  0           ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 40, 40, 512)  2048       ['dropout_1[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 40, 40, 128)  65536       ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 40, 40, 128)  0           ['conv2d_8[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 40, 40, 128)  512        ['activation_7[0][0]']           \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 40, 40, 128)  147584      ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 40, 40, 128)  0           ['conv2d_9[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 40, 40, 128)  512        ['activation_8[0][0]']           \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 40, 40, 512)  65536       ['batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " activation_9 (Activation)      (None, 40, 40, 512)  0           ['conv2d_10[0][0]']              \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 40, 40, 512)  0           ['activation_9[0][0]',           \n",
      "                                                                  'dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 40, 40, 512)  0           ['add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 40, 40, 512)  2048       ['dropout_2[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 19, 19, 256)  1179904     ['batch_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 19, 19, 256)  0           ['conv2d_11[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 19, 19, 256)  1024       ['dropout_3[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 19, 19, 128)  32768       ['batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " activation_10 (Activation)     (None, 19, 19, 128)  0           ['conv2d_12[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 19, 19, 128)  512        ['activation_10[0][0]']          \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 19, 19, 128)  147584      ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " activation_11 (Activation)     (None, 19, 19, 128)  0           ['conv2d_13[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 19, 19, 128)  512        ['activation_11[0][0]']          \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 19, 19, 512)  65536       ['batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " activation_12 (Activation)     (None, 19, 19, 512)  0           ['conv2d_14[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 19, 19, 512)  131072      ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 19, 19, 512)  0           ['activation_12[0][0]',          \n",
      "                                                                  'conv2d_15[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 19, 19, 512)  0           ['add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 19, 19, 512)  2048       ['dropout_4[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 19, 19, 128)  65536       ['batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " activation_13 (Activation)     (None, 19, 19, 128)  0           ['conv2d_16[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 19, 19, 128)  512        ['activation_13[0][0]']          \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 19, 19, 128)  147584      ['batch_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " activation_14 (Activation)     (None, 19, 19, 128)  0           ['conv2d_17[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 19, 19, 128)  512        ['activation_14[0][0]']          \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 19, 19, 512)  65536       ['batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " activation_15 (Activation)     (None, 19, 19, 512)  0           ['conv2d_18[0][0]']              \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 19, 19, 512)  0           ['activation_15[0][0]',          \n",
      "                                                                  'dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 19, 19, 512)  0           ['add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 19, 19, 512)  2048       ['dropout_5[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 19, 19, 128)  65536       ['batch_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " activation_16 (Activation)     (None, 19, 19, 128)  0           ['conv2d_19[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 19, 19, 128)  512        ['activation_16[0][0]']          \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 19, 19, 128)  147584      ['batch_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " activation_17 (Activation)     (None, 19, 19, 128)  0           ['conv2d_20[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 19, 19, 128)  512        ['activation_17[0][0]']          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)             (None, 19, 19, 512)  65536       ['batch_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " activation_18 (Activation)     (None, 19, 19, 512)  0           ['conv2d_21[0][0]']              \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 19, 19, 512)  0           ['activation_18[0][0]',          \n",
      "                                                                  'dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 19, 19, 512)  0           ['add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 19, 19, 512)  2048       ['dropout_6[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)             (None, 9, 9, 256)    1179904     ['batch_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 9, 9, 256)    0           ['conv2d_22[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 9, 9, 256)   1024        ['dropout_7[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)             (None, 9, 9, 256)    65536       ['batch_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " activation_19 (Activation)     (None, 9, 9, 256)    0           ['conv2d_23[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_21 (BatchN  (None, 9, 9, 256)   1024        ['activation_19[0][0]']          \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)             (None, 9, 9, 256)    590080      ['batch_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " activation_20 (Activation)     (None, 9, 9, 256)    0           ['conv2d_24[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_22 (BatchN  (None, 9, 9, 256)   1024        ['activation_20[0][0]']          \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_25 (Conv2D)             (None, 9, 9, 1024)   262144      ['batch_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " activation_21 (Activation)     (None, 9, 9, 1024)   0           ['conv2d_25[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_26 (Conv2D)             (None, 9, 9, 1024)   262144      ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 9, 9, 1024)   0           ['activation_21[0][0]',          \n",
      "                                                                  'conv2d_26[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 9, 9, 1024)   0           ['add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_23 (BatchN  (None, 9, 9, 1024)  4096        ['dropout_8[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_27 (Conv2D)             (None, 9, 9, 256)    262144      ['batch_normalization_23[0][0]'] \n",
      "                                                                                                  \n",
      " activation_22 (Activation)     (None, 9, 9, 256)    0           ['conv2d_27[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_24 (BatchN  (None, 9, 9, 256)   1024        ['activation_22[0][0]']          \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_28 (Conv2D)             (None, 9, 9, 256)    590080      ['batch_normalization_24[0][0]'] \n",
      "                                                                                                  \n",
      " activation_23 (Activation)     (None, 9, 9, 256)    0           ['conv2d_28[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_25 (BatchN  (None, 9, 9, 256)   1024        ['activation_23[0][0]']          \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_29 (Conv2D)             (None, 9, 9, 1024)   262144      ['batch_normalization_25[0][0]'] \n",
      "                                                                                                  \n",
      " activation_24 (Activation)     (None, 9, 9, 1024)   0           ['conv2d_29[0][0]']              \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 9, 9, 1024)   0           ['activation_24[0][0]',          \n",
      "                                                                  'dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 9, 9, 1024)   0           ['add_7[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_26 (BatchN  (None, 9, 9, 1024)  4096        ['dropout_9[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_30 (Conv2D)             (None, 9, 9, 256)    262144      ['batch_normalization_26[0][0]'] \n",
      "                                                                                                  \n",
      " activation_25 (Activation)     (None, 9, 9, 256)    0           ['conv2d_30[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_27 (BatchN  (None, 9, 9, 256)   1024        ['activation_25[0][0]']          \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_31 (Conv2D)             (None, 9, 9, 256)    590080      ['batch_normalization_27[0][0]'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " activation_26 (Activation)     (None, 9, 9, 256)    0           ['conv2d_31[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_28 (BatchN  (None, 9, 9, 256)   1024        ['activation_26[0][0]']          \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_32 (Conv2D)             (None, 9, 9, 1024)   262144      ['batch_normalization_28[0][0]'] \n",
      "                                                                                                  \n",
      " activation_27 (Activation)     (None, 9, 9, 1024)   0           ['conv2d_32[0][0]']              \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 9, 9, 1024)   0           ['activation_27[0][0]',          \n",
      "                                                                  'dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 9, 9, 1024)   0           ['add_8[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_29 (BatchN  (None, 9, 9, 1024)  4096        ['dropout_10[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_33 (Conv2D)             (None, 9, 9, 256)    262144      ['batch_normalization_29[0][0]'] \n",
      "                                                                                                  \n",
      " activation_28 (Activation)     (None, 9, 9, 256)    0           ['conv2d_33[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_30 (BatchN  (None, 9, 9, 256)   1024        ['activation_28[0][0]']          \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_34 (Conv2D)             (None, 9, 9, 256)    590080      ['batch_normalization_30[0][0]'] \n",
      "                                                                                                  \n",
      " activation_29 (Activation)     (None, 9, 9, 256)    0           ['conv2d_34[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_31 (BatchN  (None, 9, 9, 256)   1024        ['activation_29[0][0]']          \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_35 (Conv2D)             (None, 9, 9, 1024)   262144      ['batch_normalization_31[0][0]'] \n",
      "                                                                                                  \n",
      " activation_30 (Activation)     (None, 9, 9, 1024)   0           ['conv2d_35[0][0]']              \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 9, 9, 1024)   0           ['activation_30[0][0]',          \n",
      "                                                                  'dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 9, 9, 1024)   0           ['add_9[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_32 (BatchN  (None, 9, 9, 1024)  4096        ['dropout_11[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_36 (Conv2D)             (None, 9, 9, 256)    262144      ['batch_normalization_32[0][0]'] \n",
      "                                                                                                  \n",
      " activation_31 (Activation)     (None, 9, 9, 256)    0           ['conv2d_36[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_33 (BatchN  (None, 9, 9, 256)   1024        ['activation_31[0][0]']          \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_37 (Conv2D)             (None, 9, 9, 256)    590080      ['batch_normalization_33[0][0]'] \n",
      "                                                                                                  \n",
      " activation_32 (Activation)     (None, 9, 9, 256)    0           ['conv2d_37[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_34 (BatchN  (None, 9, 9, 256)   1024        ['activation_32[0][0]']          \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_38 (Conv2D)             (None, 9, 9, 1024)   262144      ['batch_normalization_34[0][0]'] \n",
      "                                                                                                  \n",
      " activation_33 (Activation)     (None, 9, 9, 1024)   0           ['conv2d_38[0][0]']              \n",
      "                                                                                                  \n",
      " add_10 (Add)                   (None, 9, 9, 1024)   0           ['activation_33[0][0]',          \n",
      "                                                                  'dropout_11[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)           (None, 9, 9, 1024)   0           ['add_10[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_35 (BatchN  (None, 9, 9, 1024)  4096        ['dropout_12[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_39 (Conv2D)             (None, 4, 4, 512)    4719104     ['batch_normalization_35[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)           (None, 4, 4, 512)    0           ['conv2d_39[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_36 (BatchN  (None, 4, 4, 512)   2048        ['dropout_13[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_40 (Conv2D)             (None, 4, 4, 256)    131072      ['batch_normalization_36[0][0]'] \n",
      "                                                                                                  \n",
      " activation_34 (Activation)     (None, 4, 4, 256)    0           ['conv2d_40[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_37 (BatchN  (None, 4, 4, 256)   1024        ['activation_34[0][0]']          \n",
      " ormalization)                                                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " conv2d_41 (Conv2D)             (None, 4, 4, 256)    590080      ['batch_normalization_37[0][0]'] \n",
      "                                                                                                  \n",
      " activation_35 (Activation)     (None, 4, 4, 256)    0           ['conv2d_41[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_38 (BatchN  (None, 4, 4, 256)   1024        ['activation_35[0][0]']          \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_42 (Conv2D)             (None, 4, 4, 1024)   262144      ['batch_normalization_38[0][0]'] \n",
      "                                                                                                  \n",
      " activation_36 (Activation)     (None, 4, 4, 1024)   0           ['conv2d_42[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_43 (Conv2D)             (None, 4, 4, 1024)   524288      ['dropout_13[0][0]']             \n",
      "                                                                                                  \n",
      " add_11 (Add)                   (None, 4, 4, 1024)   0           ['activation_36[0][0]',          \n",
      "                                                                  'conv2d_43[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)           (None, 4, 4, 1024)   0           ['add_11[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_39 (BatchN  (None, 4, 4, 1024)  4096        ['dropout_14[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_44 (Conv2D)             (None, 4, 4, 256)    262144      ['batch_normalization_39[0][0]'] \n",
      "                                                                                                  \n",
      " activation_37 (Activation)     (None, 4, 4, 256)    0           ['conv2d_44[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_40 (BatchN  (None, 4, 4, 256)   1024        ['activation_37[0][0]']          \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_45 (Conv2D)             (None, 4, 4, 256)    590080      ['batch_normalization_40[0][0]'] \n",
      "                                                                                                  \n",
      " activation_38 (Activation)     (None, 4, 4, 256)    0           ['conv2d_45[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_41 (BatchN  (None, 4, 4, 256)   1024        ['activation_38[0][0]']          \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_46 (Conv2D)             (None, 4, 4, 1024)   262144      ['batch_normalization_41[0][0]'] \n",
      "                                                                                                  \n",
      " activation_39 (Activation)     (None, 4, 4, 1024)   0           ['conv2d_46[0][0]']              \n",
      "                                                                                                  \n",
      " add_12 (Add)                   (None, 4, 4, 1024)   0           ['activation_39[0][0]',          \n",
      "                                                                  'dropout_14[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)           (None, 4, 4, 1024)   0           ['add_12[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_42 (BatchN  (None, 4, 4, 1024)  4096        ['dropout_15[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_47 (Conv2D)             (None, 4, 4, 256)    262144      ['batch_normalization_42[0][0]'] \n",
      "                                                                                                  \n",
      " activation_40 (Activation)     (None, 4, 4, 256)    0           ['conv2d_47[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_43 (BatchN  (None, 4, 4, 256)   1024        ['activation_40[0][0]']          \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_48 (Conv2D)             (None, 4, 4, 256)    590080      ['batch_normalization_43[0][0]'] \n",
      "                                                                                                  \n",
      " activation_41 (Activation)     (None, 4, 4, 256)    0           ['conv2d_48[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_44 (BatchN  (None, 4, 4, 256)   1024        ['activation_41[0][0]']          \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_49 (Conv2D)             (None, 4, 4, 1024)   262144      ['batch_normalization_44[0][0]'] \n",
      "                                                                                                  \n",
      " activation_42 (Activation)     (None, 4, 4, 1024)   0           ['conv2d_49[0][0]']              \n",
      "                                                                                                  \n",
      " add_13 (Add)                   (None, 4, 4, 1024)   0           ['activation_42[0][0]',          \n",
      "                                                                  'dropout_15[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)           (None, 4, 4, 1024)   0           ['add_13[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_45 (BatchN  (None, 4, 4, 1024)  4096        ['dropout_16[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 1024)        0           ['batch_normalization_45[0][0]'] \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      " dropout_17 (Dropout)           (None, 1024)         0           ['global_average_pooling2d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " batch_normalization_46 (BatchN  (None, 1024)        4096        ['dropout_17[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dense (Dense)                  (None, 512)          524800      ['batch_normalization_46[0][0]'] \n",
      "                                                                                                  \n",
      " activation_43 (Activation)     (None, 512)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_47 (BatchN  (None, 512)         2048        ['activation_43[0][0]']          \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 4)            2052        ['batch_normalization_47[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 18,867,588\n",
      "Trainable params: 18,828,420\n",
      "Non-trainable params: 39,168\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "28e60347",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------Make Predictions--------------\n",
      "Testing started\n",
      "40/40 [==============================] - 82s 2s/step\n",
      "Testing time (s) = 83.90742780000437\n",
      "\n",
      "[[0.12750964 0.63459015 0.02466625 0.2132339 ]\n",
      " [0.17508358 0.42142767 0.01831834 0.38517037]\n",
      " [0.18620972 0.43045273 0.04283597 0.34050155]\n",
      " ...\n",
      " [0.1260701  0.56228507 0.0101266  0.30151817]\n",
      " [0.10763425 0.55876917 0.02066662 0.3129299 ]\n",
      " [0.13776065 0.5337003  0.01638258 0.3121565 ]]\n",
      "--------\n",
      "Confusion matrix = \n",
      "[[  0 311   0   9]\n",
      " [  0 312   0   8]\n",
      " [  0 314   0   6]\n",
      " [  0 311   0   9]]\n",
      "------------------------------------------------\n",
      "--------------Make Predictions--------------\n",
      "Testing started\n",
      "10/10 [==============================] - 19s 2s/step\n",
      "Testing time (s) = 19.241014199971687\n",
      "\n",
      "[[0.16669679 0.5220874  0.0185956  0.29262027]\n",
      " [0.1629391  0.64386934 0.04610719 0.14708439]\n",
      " [0.19759353 0.51729906 0.02531968 0.2597878 ]\n",
      " ...\n",
      " [0.1206059  0.63160974 0.0200491  0.22773528]\n",
      " [0.10512616 0.55612165 0.01537493 0.32337734]\n",
      " [0.16039236 0.5402015  0.01769875 0.28170747]]\n",
      "--------\n",
      "Confusion matrix = \n",
      "[[ 0 75  0  5]\n",
      " [ 0 79  0  1]\n",
      " [ 0 76  0  4]\n",
      " [ 0 75  0  5]]\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "predict(model,params[\"normalization_type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5904040f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
